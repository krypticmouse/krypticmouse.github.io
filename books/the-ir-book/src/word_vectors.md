# Words are Vectors and so are Sentences

***

<!-- toc -->

***

> *I must not create a Bad Vector.*
>
> *Bad Vector is the mind-killer.*
> 
> *Bad Vector is the little-death that brings total obliteration.*
> 
> *I will improve my Vectors Representaions.*
> 
> **-Lady Jessica**

I mean I didn't butcher it that bad right? Well I guess everyone now knows I watched Dune 2, What a masterpiece!! Anyways, getting back to the topic, we learned a bit about the history of Information Retrieval in the previous chapter. How far we've come right? In that we talked breifly about Vector Space Models and Semantic Search.

The core idea that drives Semantic Search and most modern day retrievers is Word/Sentence Embeddings. The better we can represent words and sentences in a vector space, the more accurate and relevant our search results become. In this chapter we'll be diving deep into the world of word and sentence embeddings. We'll look at the different kinds of word embeddings, how they're trained, and how they can be used in Information Retrieval.

## Word Embeddings: Words are Vectors

### Sparse Embeddings: Vectors based on Word Frequency

#### Bag of Words: Count Based Embeddings

#### TFIDF: Minority Matters

#### Okapi BM25: Sparse SOTA?

### Dense Embeddings: Vectors based on Word Meaning

#### Initial Attempts: Word2Vec, GloVE and FastText

#### LSTMs and Psuedo Bidirectionality

#### Towards True Bidirectionality

## Sentence Embeddings: Because Sentences are also Vectors

### Sentence as Single Vector

### Sentence as Mutiple Vectors

## Matryoshka Representation Learning