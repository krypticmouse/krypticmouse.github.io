<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Transformers: Attention is all you need | Journal | Herumb Shandilya</title>
<meta name=keywords content>
<meta name=description content="Attention is Transformer‚Äôs breath,
Multi-Head is Transformer‚Äôs release,
RNNs thou wert and art,
May thy model reach to greater accuracy.
L√°tom.
- Enen No Shouboutai
 I&rsquo;m getting tired of butchering anime and video game quotes so I&rsquo;m thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.">
<meta name=author content="Herumb Shandilya">
<link rel=canonical href=https://journal.herumbshandilya.com/posts/transformers/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://journal.herumbshandilya.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://journal.herumbshandilya.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://journal.herumbshandilya.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://journal.herumbshandilya.com/apple-touch-icon.png>
<link rel=mask-icon href=https://journal.herumbshandilya.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.2">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Transformers: Attention is all you need">
<meta property="og:description" content="Attention is Transformer‚Äôs breath,
Multi-Head is Transformer‚Äôs release,
RNNs thou wert and art,
May thy model reach to greater accuracy.
L√°tom.
- Enen No Shouboutai
 I&rsquo;m getting tired of butchering anime and video game quotes so I&rsquo;m thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://journal.herumbshandilya.com/posts/transformers/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-10T00:00:00+00:00">
<meta property="article:modified_time" content="2021-12-10T00:00:00+00:00"><meta property="og:site_name" content="Journal | Herumb Shandilya">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Transformers: Attention is all you need">
<meta name=twitter:description content="Attention is Transformer‚Äôs breath,
Multi-Head is Transformer‚Äôs release,
RNNs thou wert and art,
May thy model reach to greater accuracy.
L√°tom.
- Enen No Shouboutai
 I&rsquo;m getting tired of butchering anime and video game quotes so I&rsquo;m thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://journal.herumbshandilya.com/posts/"},{"@type":"ListItem","position":3,"name":"Transformers: Attention is all you need","item":"https://journal.herumbshandilya.com/posts/transformers/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformers: Attention is all you need","name":"Transformers: Attention is all you need","description":"Attention is Transformer‚Äôs breath,\nMulti-Head is Transformer‚Äôs release,\nRNNs thou wert and art,\nMay thy model reach to greater accuracy.\nL√°tom.\n- Enen No Shouboutai\n I\u0026rsquo;m getting tired of butchering anime and video game quotes so I\u0026rsquo;m thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.","keywords":[],"articleBody":" Attention is Transformer‚Äôs breath,\nMulti-Head is Transformer‚Äôs release,\nRNNs thou wert and art,\nMay thy model reach to greater accuracy.\nL√°tom.\n- Enen No Shouboutai\n I‚Äôm getting tired of butchering anime and video game quotes so I‚Äôm thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.\nI mean I‚Äôve been seeing blogs that go by the name of Paper Dissected so I wanted to try writing one myself. Well this is going to be my first try in this and frankly, I have no idea how it‚Äôll be but let‚Äôs give it a try, shall we?\nI guess it‚Äôs safe to say that Attention Mechanism and Transformers is something that recently took over NLP. Not only did it show improvements over the SOTA models at the time but also overcome the shortcoming of the RNN models like LSTM and GRU.\nSo let‚Äôs go ahead and break down the sections of the paper, Attention is all you need. To give you a gist of what‚Äôll be there, it‚Äôll be an explanation of each section in the paper like abstract, introduction, model architecture, etc.\nAbstract This section pretty much summarizes the whole paper not in terms of working but rather in terms of what it has to offer. It starts with explaining how Seq2Seq models usually use RNN and CNN for encoder and decoder, and how the best ones connect encoder and decoder via attention mechanism.\nNow just to be clear, attention itself is not a new concept but to rely completely on attention is something that this paper introduced. It went ahead explaining the results achieved on the machine translation task which were like the following on WMT-2014 dataset:-\n   MT - Task BLEU Score     English to German 28.4   English to French 41.8    It also took 3.5 days and 8 GPUs to train on, which is certainly out of my budget. But the main thing that shined a bit differently than others in the abstract section was:-\n Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.\n We saw how results were better than traditional models but the thing that separated transformers from LSTM is parallel computation. This can feel a bit tricky to understand at the start so let‚Äôs break it down. By nature, RNN models aren‚Äôt parallel but rather Sequential and this is because they compute results one step at a time.\nHowever, in the Transformer model, there is no such thing as ‚Äútime steps‚Äù but rather the inputs are passed all together and all this is made possible due to multi-headed attention and other stuff which we‚Äôll talk about later.\nIntroduction This section mainly talks about the working of RNNs and it‚Äôs limitations, along with the recent developments to tackle them. Now that we are at it let‚Äôs understand how does Seq2Seq modeling working in the case of RNNs and to make it easier, take a look a look at the picture below:-\nSeq2Seq Models consists of 2 models i.e. an Encoder and a Decoder. The Encoder model takes the text as input and generates a Context Vector as an output which is nothing but the last hidden state of RNN. Encoders and Decoders are nothing but RNNs and in few instances CNN üåö.\nAnother thing you‚Äôll notice is the crosses above RNN cells except the last one that doesn‚Äôt mean they aren‚Äôt produced, they are but we just don‚Äôt use them and all we‚Äôll be doing is to take the context vector from the last time step and feeding it to the decoder as input who‚Äôll output each word at a time giving us the resultant text.\nIn attention, however, you‚Äôll be taking the outputs of all the time step of encoder so you don‚Äôt discard in that case. I mean you can also do mean pooling on all the states and pass that as context vector, so attention isn‚Äôt the only way that uses all the states. But how does the decoder predict the words? Let‚Äôs see, I‚Äôll try to be brief.\nIn the decoder, the first token is the special token signifying the start of the sentence along with the context vector from the encoder, after that you pass the previous output as input to the decoder along with the hidden state to generate new output. However during training, we feed the ground-truth value of targets as input to the decoder, this is known as Teacher Forcing. Without which the model weights will be updated on inferred text making it difficult for model to learn from.\nHowever during inference, you‚Äôll use generated output as input to generate the next token, and it‚Äôs cool except the output is being constructed on the context of inferred output rather than actual ones, which causes discrepancy during training and inference, giving rise to the problem of exposure bias.\nBack to the topic, the nature of RNNs is sequential which makes parallelization difficult for training, making RNN models take more time to train. However, there are few developments that tackle it:-\n Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation, while also improving model performance in the case of the latter. The fundamental constraint of sequential computation, however, remains.\n To understand the factorization trick you can go ahead and read another paper, i.e. Factorization Tricks for LSTM Networks by Oleksii and Boris, which I recommend but I‚Äôll give you the gist of it.\n The major part of LSTMP cell computation is in computing affine transform T because it involves multiplication with 4n √ó 2p matrix W.\n The aim of the factorization trick is to approximate that W by replacing it with the product of 2 matrices W1(2p*r) and W2(r*4n), this way takes fewer parameters (2pr + r4n) as compared to W (2p*4n) given r I know it‚Äôs a lot to grasp so if you don‚Äôt understand it all at the moment it‚Äôs fine, important thing to understand is that many attempts have been made to improve RNNs but the sequential nature isn‚Äôt completely removed. The attention model did help in capturing long-term dependencies and improve results, however, it was used along with RNNs.\nThere is another concept of conditional computation that aims at activating parts of the network to improve computational efficiency, and performance too in this case, but let‚Äôs not dive much into it. But if you want to you can read this short book on the same.\nThe transformer model on the other hand relies entirely on attention mechanism allowing it to be more parallelizable and give better results with the cost of training being 12 hours on 8 P100 GPUs, something that even Colab Pro users can‚Äôt get their hands on all the time anymore ü•≤.\nModel Architecture Finally, we‚Äôve come to the real deal. Now while I wanna try and break down the explanation sub-section wise, I know that may or may not end up leaving you kinda confused, and hence instead of explaining each sub-section of this section we‚Äôll dismantle the architecture step by step and understand how it works.\nInput Embeddings Let‚Äôs start from the beginning, what you have with you is a sentence or bunch of sentences. Now you can‚Äôt exactly feed it to the model as it is so you need a way to convert it to a form more suitable for it. When we used RNNs for text classification or seq2seq we convert our text to word embeddings.\nThese embeddings are nothing but vector representations of words such that words with similar meanings are closer to each other. But what does closer mean? Mathematically, closer means the similarity score is high. This score can be anything but cosine similarity is popular so feel free to think that. Once we have our embeddings to use we are all set! Are we?\nTechnically, yes we are since these embeddings can be used with RNNs and CNNs because they are able to capture the ordering into consideration. But Transformers use only self-attention, which takes all the words at the same time hence it is not able to take the order of words into consideration. To fix this problem we introduce something called positional encodings.\nPositional Encodings So as mentioned before, inherently attention model is not able to take the order of words into consideration. In order to compensate for this, we use something called Positional Encodings. Don‚Äôt be intimidated by the name it‚Äôs actually much more simple. Let‚Äôs dive a bit deeper into this.\nNeed of Positional Encodings When working with text it‚Äôs really important to take words order into consideration. Why? The order of words defines the structure of the sentence and this structure helps in providing the context.\nRNNs by nature are sequential in nature by taking inputs one by one and updating hidden states accordingly, but the same is not the case with attention since it takes the words all at once. Why this is so, is something we‚Äôll understand in the next section but for now, let‚Äôs say that‚Äôs how it is.\nSo we need some extra sources of information in our embeddings to convey this information, something that could tell the position of a word in a sentence. To do so we can add an extra embedding or matrix to our word embeddings to provide info regarding the ordering of the word. But how?\nFinding Positional Encodings Well in the most simple case you can use a one-hot encoded form of [0,1,2,3,4,5,‚Ä¶, pos] as positional encodings. You can train an embedding layer to find the positional embedding matrix, where each row has the positional embedding for word at pos index, for this and add it to input embeddings.\nIn the paper, however, the proposed approach uses a combination of sin and cos function to find the encodings. Take a look at the following picture:-\nAs you can see in the above picture the encodings are found by using a combination of sin and cos function. The function is quite self-explanatory, it says that for the embedding of a word at position pos:-\n  The value at index 2i is given using the sin function, colored red.\n  The value at index 2i +1 is given using the cos function, colored blue.\n  But why are we using this particular function? Well let‚Äôs hear from the authors themselves:-\n We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since, for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\n Well, researchers sure love to make stuff seem more complicated, don‚Äôt they? It‚Äôs fine if you don‚Äôt understand the above sentence, take it as a confirmation that you‚Äôre still sane.\nWhat the sentence means to say is that given the positional encoding of the word at pos you can find the positional encoding of word at pos + k by applying some linear transformation on the positional encoding of pos. Mathematically, it can be written like this:-\n\\[PE_{p+k} = T * PE_p\\]\nWriting pos as p in the above equation since latex is getting messed up for some reason. The important thing to know is that the matrix T can be used to find PE at pos+k but what is this T? Can we find it? Well yes, that T is a rotational matrix that depends on offset k rather than pos. Wait what! How?\nWell, you can derive it, and here is a really good blog by Amirhossein doing the same. There is another blog by Timo Denk that proves the linear relationship between PE at pos and pos+k but the first one explains in a much simpler fashion, but do give both a read.\nImproving a bit more Wow so everything is dope, right? Fortunately or unfortunately, No. Even though everything might seem super nice with these encodings, which it is, what these really are doing is finding values for given pos and i values, these are also called Absolute Positional Encodings. While the altering of sin and cosine might be able to add relative nature, we might actually have something better i.e. Relative Positional Encodings.\nFollowing this paper, there was another paper released i.e. Self-Attention with Relative Position Representations by Shaw et. al. 2019. This paper introduces a modified attention mechanism which is mostly the same except this one has 2 matrices that are added to Key and Value Vector and the aim is that they‚Äôll be able to induce pairwise relationships in the inputs. In the author‚Äôs own words:-\n We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully connected graph.\n The paper is actually a pretty decent read and you should give it a try. Since this paper doesn‚Äôt use these embeddings we‚Äôll not go into the detail of these. There are many papers that try to come up with better Positional Encodings and try to improve them, but what they are used for and proper intuition is something you should aim to understand.\nAttention Attention is something that we‚Äôve been talking about a lot in this blog, after all that‚Äôs what this paper is about. But what does it actually mean? So before diving into the proposed method for attention calculation in this paper, we‚Äôll go ahead and understand what attention is and how it helps our cause.\nSo when we do Seq2Seq with RNNs we have an encoder and decoder network in which the work of the decoder is to generate a sentence given context vector. This context vector is nothing but the last hidden state of the encoder which is RNN based model. So what‚Äôs the issue? Well, you see:-\n  The context vector is a vector of fixed length defined by hidden_dims but the sentences are of variable length and so what we are doing is squeezing info into a vector due to which information might be lost.\n  Another problem is that the generated sentences are based on the context of the whole sentence, however in doing so we might lose out on the local context of parts of the sentence that might‚Äôve been helpful in tasks like translation.\n  So in order to improve on this, we introduce attention and instead of working on the global context of the sentence by taking all the hidden states in the encoder and passes it to the decoder. The decoder, in this case, doesn‚Äôt produce the output directly instead it‚Äôll calculate the attention score for the states and creates the context vector for that particular time step given the attention score, the encoder hidden states, and the input vector, output of prev. step or last input token.\nBut how does this help? Well, what attention does is that it generates a context vector at each time step of the decoder based on attention score and this attention score helps to amplify the more important words to focus on during that time step in order to improve the quality of results. If you calculate attention among the words of the input sequence it‚Äôs called Self Attention and when you calculate attention among the words of the input and output sequence it‚Äôs called General Attention.\nNow based on the architecture you have 2 types of attention-based Seq2Seq models i.e. Bahdanau Attention and Luong Attention. Except the architecture and computation both are basically the same but if you wanna learn more you can go ahead and read the paper, Effective Approaches to Attention-based Neural Machine Translation by Luong et. al. 2015.\nScaled Dot-Product Attention We have a basic idea of how attention works in RNN now, but we don‚Äôt exactly have an RNN in Transformer or a way to sequentially input data. So the proposed method for calculating attention is via something called Scaled Dot-Product Attention, which is basically a bunch of matrix multiplication operations.\nBut before diving in, let‚Äôs talk about QKV or Query-Key-Value vectors and their dimensions. Throughout the model, the dimension of columns of the resultant matrix from each layer is set to 512 which is denoted by \\( d_{model} \\) .\nSo if we have m rows in word embedding we‚Äôll have the resultant matrix of dimension \\( m * d_{model} \\) , this column dimension is consistent for all the layers in the model including the embedding layer.\nThis is done to facilitate the residual connections, which we‚Äôll talk about later. Now that we have output dims cleared let‚Äôs talk about columns of QKV vectors.\nFor now, just remember the dimension of QKV vectors being 64, the reasoning is something we‚Äôll discuss in Multiheaded attention. The dimensions of Query and Key vector are the same and are denoted by \\( d_k \\) , and even though here it is same the dimensions of value vector can be different and is denoted by \\( d_v \\) . So to sum it all up we‚Äôll have the following notation for dimensions:-\n$$d_{model} = 512$$\n$$d_q = d_k = 64$$\n$$d_v = 64$$\nNow that we have the important stuff cleared, let‚Äôs take a look at the following image that sums up the process of calculating attention:-\nIt might seem confusing so let‚Äôs go in step by step on how attention is being calculated and how its equation is constructed:-\n  You start by finding Query, Key, and Value vector by multiplying the input matrix with their corresponding weight vector. The dims of these vectors are something we‚Äôll discuss in Multi Headed Attention Section. For now, let‚Äôs just focus on understanding the equation.\n$$Attention(Q,K,V)$$\n  Apply Matrix Multiplication on Query and Key Vector.\n$$Attention(Q,K,V)=QKT$$\n  Scale the result of above with \\( \\sqrt{d_k} \\) . It was observed that for small values of \\( d_k \\) additive attention and dot product attention perform similarly. But for larger values of \\( d_k \\) additive attention outperforms dot product attention without scaling. As mentioned in paper, that for large value the result may explode and may push softmax to smaller gradients. To counteract this \\( \\frac{1}{\\sqrt{d_k}} \\) was used for scaling. As to why this occurs, paper mentions:-\n   To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product has mean 0 and variance d_k.\n   Applying softmax on the result of the above.\n$$Attention(Q,K,V)=softmax((QK^T)/\\sqrt{d_k})$$\n  Apply Matrix Multiplication on result and Value Vector and get the final result.\n$$Attention(Q,K,V)=softmax((QK^T)/\\sqrt{dk})V$$\n  There you go that‚Äôs how you find attention using Scaled Dot Product Attention. Just for the info, this is not the absolute way to find attention, there are other ways to calculate attention like additive attention, dot-product(unscaled) attention. While the two are similar in complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nNow let‚Äôs take a look at the newly proposed approach where instead of calculating attention one time we calculate it multiple times and then concatenate the results. Yep, it is indeed Multi-Headed Attention.\nMulti-Headed Attention Let‚Äôs talk about the beast that the paper proposed and when I say ‚Äúbeast‚Äù I mean it. The paper suggested that instead of calculating attention one time we can calculate it h times with \\(d_q\\), \\(d_k\\), \\(d_v\\) dimensioned QKV vectors found after passing each of them from a Linear Layer.\nWe then use them to calculate attention values from each attention head depicted by head_i and concatenate these heads forming \\( d_{model} \\) dimensional resultant vector. After that, we‚Äôll pass them through another linear layer to get our Multi-Headed Attention output. Architecture wise it looks like this:-\nOne thing to note is that the value of d_q, d_v, and d_k will be d_model / h, that‚Äôs why after you concatenate, the attention head‚Äôs output it becomes d_model sized vector. In this paper, the value of h for the optimal model was 8. So d_k, d_v, and d_q become 512/8 = 64.\nWhy does it Work? But after hearing all this it‚Äôs only sensible to ask. Why does this even work? To answer this query, we should go and take a look at the paper ‚ÄúAnalyzing Multi-Head Self-Attention‚Äù by Voita et. al. 2019. Where they basically try to find the roles that different attention heads played along with other stuff which we‚Äôll talk about later i.e. Attention Head Pruning.\nTo explain the gist of it the conclusion was, that based on roles attention heads can be divided into 3 parts:-\n  Positional Heads: Heads where the maximum attention weights was assigned to a specific relative position. This was usually +/- 1 in practice signifying attention to adjacent positions\n  Syntactical Heads: Head attends to tokens corresponding to any of the major syntactic relations in a sentence. In the paper, they analyze direct relations like nominal subject, direct object, adjectival modifier, and adverbial modifier.\n  Rare Word Heads: Heads attend to the least frequent tokens in a sentence.\n  So different heads try to perform different task cool, but do we need all of them? We have only 3 types right? Does that mean other heads are useless? Yes and No. Let‚Äôs understand the Yes part first.\nYes, attention heads are important and in fact, that‚Äôs exactly what the blog ‚ÄúAre Sixteen Heads Really Better than One?\" by Paul Michael tries to find. In the blog, he tried to explain how the performance was affected when attention heads were pruned. After the experiment they found the following result:-\nAs you can see the BLEU Score is really affected after pruning the heads however like other metrics the dip is observed after more than 60% heads are pruned. This leads us to the No part. I mean kinda No.\nEven though we just saw that heads are important we can also prune one or a few of them without seeing a major dip in the performance, something that both the mentioned sources, that I highly recommend you to go through, suggested. Phew that was a lot, now that we have the initial stuff cleared up, let‚Äôs get back to the architecture. I promise this point forward is a smooth sailing simple life, probably xD.\nEncoder If we try to understand the architecture of Encoder it can be broken into 2 parts:-\n  Attention Layer\n  Feed Forward Layer\n  Yes that‚Äôs it, you are already familiar with Attention Layer which is nothing but Multi Headed Attention with 8 heads and the Feed Forward Layer is a basic Neural Network with a single hidden layer with 2048 nodes and 512 nodes in input and output layer. Visually it‚Äôll look something like this:-\nIts flow of Infomation is really simple, or not depending on your knowledge of skip connections. If I have to explain in one line skip connection is basically adding input to the output.\nI know it sounds weird, but trust me it makes sense. ResNet model was the one that popularised these skip connections which provides an alternate path for gradients to flow. They helped train deeper models without running into vanishing gradient problem. So if x is the input to a block and f(x) is the output of the block then skip connections basically add them both making x + f(x) the new output.\n$$f'(x) = x + f(x)$$\nNow that we are all on the same page, let‚Äôs step by step go through the flow of info in an encoder block:-\n  Encoder block receives an input X.\n  X is supplied to Multi-Headed Attention Layer. QKV all will be the same i.e. X.\n  Multi Headed Attention does the magic and generates an output M(x).\n  Add X to M(x) because of skip connection.\n  Apply Layer Normalisation to X + M(x), let‚Äôs call this N(x).\n  Pass N(x) to Feed Forward layer to generate an output F(x).\n  Add N(x) in F(x) because of skip connection again.\n  Apply Layer Normalisation to F(x) + N(x), let‚Äôs call this E(x) which is the output of this encoder block.\n  One last thing, the above is flow of info for one encoder block. This encoder block‚Äôs output will be passed on to the next encoder block and the architecture of all encoder blocks is the same. The paper proposed the architecture with 6 encoder blocks. I swear I need to learn animation one of these days, would make stuff much much easier.\nYea I know too much info xD, but hey that‚Äôs all that you need to know about Encoders. The output of the last encoder block is passed to the decoder as KV vector values, which is our next topic of discussion. We are almost near the end, so hang in there üò¨.\nDecoder Finally, we are at the last step of the architecture, to be honest decoder is not really that different from than encoder except the fact that here were have an extra layer for masked multi-headed attention. We‚Äôll get into the details of that in a bit, but let‚Äôs get an overview of how a decoder block looks:-\nBefore going into the flow of information let‚Äôs understand exactly what is the input and output of the decoder block. As far as input is concerned the decoder takes 2 inputs:-\n  Encoder Output: This is will be fed as KV Vectors for multi-headed attention in the 2nd layer, this is usually referred to as Encoder-Decoder Attention. All 6 Decoder Blocks will take the encoder output from the last block as input for encoder-decoder attention.\n  Output Embedding: These are the embeddings of the target sentence. But wait!? We don‚Äôt have a target in testing! So what are we inputting here then? Let‚Äôs see.\n  Well usually the first input to the decoder is a special token signifying the start of the output sentence and the output is the token that comes after this. This output is the output of time step 1.\nAfter that, the input to decoder becomes the tokens until a certain timestep i.e. for the 5th timestep input will be all generated tokens till the 4th timestep. After that, it‚Äôll keep on giving an output until it throws a special token signifying the end of the output sentence.\nLet‚Äôs understand the flow of information in decoder block to get a better understanding:-\n  Decoder block receives an input X.\n  X is supplied to Masked Multi-Headed Attention as QKV vector, to get an output M(x).\n  Add X in M(x) because of skip connection.\n  Apply Layer Normalisation to X + M(x), let‚Äôs call this N(x).\n  N(x) is supplied to Multi-Headed Attention as Query and Encoder Output will be supplied as Key-Value..\n  Multi Headed Attention does the magic and generates an output O(x).\n  Add N(x) in O(x) because of skip connection.\n  Apply Layer Normalisation to N(x) + O(x), let‚Äôs call this P(x).\n  Pass P(x) to Feed Forward layer to generate an output F(x).\n  Add P(x) in F(x) because of skip connection again.\n  Apply Layer Normalisation to P(x) + F(x), let‚Äôs call this D(x) which is the output of this decoder block.\n  But all that is nice but how does the decoder predict the next token? Actually, the output of the last decoder block is fed to a Linear Layer that gives an output vector of the size same as that of vocabulary. So depending on how many words the model knows that will be the size of the output vector of this Linear Layer.\nNow once you have this vector you apply softmax on this to convert and interpret the values of this vector as probabilities. The token at the index of maximum probability is the output of that ‚Äútime-step‚Äù. Take a look at the following infographic to understand it better.\nWell, that‚Äôs basically how the decoder works, but we are still missing one key piece in the puzzle i.e. Masked Multi-Headed Attention. It‚Äôs actually pretty simple and trust me it won‚Äôt take much long to understand.\nMasked Multi-Headed Attention If I have to say the difference between Normal Multi-Head Attention and Masked Multi-Head Attention it‚Äôll be that in Masked Multi-Head Attention we mask the output of attention score after a point for each token by replacing it with -inf and when it applies softmax these -inf become 0.\nWell, that‚Äôs fine but what exactly is going on? Let‚Äôs understand. The reason we need such a thing in the first place is so that we can speed up training by being able to compute the probabilities of output token for each input token parallelly. The only reason we are doing this stuff is to speed up training.\nAnd just to be clear this passing all at once stuff is something we‚Äôll do only in training, during inference we‚Äôll still do it step by step by passing previous outputs to get the new ones.\nThe aim of an autoregressive model is to find the next step given the previous ones but for parallelization, we‚Äôll be passing all the stuff together at once hence we‚Äôll need to hide the output of the attention score for the future time steps of that token to properly train the model. Let‚Äôs understand this step by step to get a better understanding.\nStep-By-Step Guide to Masked MHA So let‚Äôs start by knowing what exactly we need to find:-\n$$P(x_{i+1}|[x_0, x_1, ‚Ä¶, x_i])$$\nWhat the above equation means is that we‚Äôll need to find the probability of the next word given the previous ones in sequence. Now let‚Äôs consider the following sentence and let‚Äôs assume it to be the expected output:-\n$$The \\space cake \\space was \\space sour$$\nIf you pass words one by one you prevent seeing the future steps but for efficiency, you‚Äôll need to pass all the words together. When you do this you expose the decoder to future info, in order to prevent that we mask the weights of all j(i) future steps, i being the current time step.\nSo for the word cake in the above sentence, we‚Äôll mask the weight for words was and sour, the same way for the word was in the above sentence we‚Äôll mask the weight for sour. We can do this by adding a mask matrix to this:-\nImplementation-wise, you apply the mask after doing the scaled matrix multiplication of the QV vector in scaled dot product attention. After this you proceed normally, the only difference is this masking step.\nTraining and Results Done! We are done with the architecture. We are officially out of the deadly stuff and now it‚Äôs just simple stuff. So let‚Äôs wrap it up. Let‚Äôs start by going through some details regarding the training:-\n  Data: The WMT English German Dataset contained 4.7 million sentence pairs, with a vocabulary of about 37k tokens. The WMT English French Dataset contained 36 million sentence pairs, which is so big that my hard drive will die before loading it.\n  Hardware: Richie Rich people xD. Trained on 8 Nvidia P100 GPU something which make me go bankrupt 100 times. Trained for 100k steps for 12hrs. The bigger variants were trained for 300k steps for 3.5 days.\n  Optimizer: They used Adam optimizer with Œ≤1 = 0.9, Œ≤2 = 0.98 and epsilon = 10^‚àí9. Increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. They used warmup_steps = 4000.\n  **Regularization: ** Dropout of 0.1 was applied to the sum of the positional and wor embeddings. Dropout was also applied to the output of the sublayer before the residual step. Label smoothing was also applied with epsilon = 0.1. Label Smoothing is a way to prevent the model from being too confident about it‚Äôs prediction.\n  That‚Äôs all as far as training is concerned. Now the architecture I explained had very specific parameters but that‚Äôs not the only model they tried. They actually tried a bunch of them and noted the result of each in the table below:-\nAs you can see big model performed the best out of all but between big model and base model with a small performance drop, I‚Äôll take my chance with the base one first.\nThat‚Äôs all! We have finished the paper and reached the end, take a moment and clap for yourself.\nFrom Me to You‚Ä¶ Wow, that was a big blog and I won‚Äôt lie it was tough to write. This is probably one of my longest and most experimental blog ever. I was super casual and explained everything as much as possible. At some point I wanted to just stop but every time I just thought, just a bit more.\nTransformers are probably one of those revolutionary concepts in deep learning that changed the way we approached stuff and I hope I was able to dissect the paper that started it all. I hope you had fun reading it.\nForever and Always, Herumb.\n","wordCount":"5520","inLanguage":"en","datePublished":"2021-12-10T00:00:00Z","dateModified":"2021-12-10T00:00:00Z","author":{"@type":"Person","name":"Herumb Shandilya"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://journal.herumbshandilya.com/posts/transformers/"},"publisher":{"@type":"Organization","name":"Journal | Herumb Shandilya","logo":{"@type":"ImageObject","url":"https://journal.herumbshandilya.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://journal.herumbshandilya.com accesskey=h title="Herumb's Journal (Alt + H)">Herumb's Journal</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://journal.herumbshandilya.com/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=https://the-ir-book.herumbshandilya.com/ title="The Small Book of Information Retrieval">
<span>The Small Book of Information Retrieval</span>
</a>
</li>
<li>
<a href=https://www.herumbshandilya.com/ title=Portfolio>
<span>Portfolio</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Transformers: Attention is all you need
</h1>
<div class=post-meta><span title="2021-12-10 00:00:00 +0000 UTC">December 10, 2021</span>&nbsp;¬∑&nbsp;26 min&nbsp;¬∑&nbsp;Herumb Shandilya
</div>
</header>
<div class=post-content><blockquote>
<p><em>Attention is Transformer‚Äôs breath,</em></p>
<p><em>Multi-Head is Transformer‚Äôs release,</em></p>
<p><em>RNNs thou wert and art,</em></p>
<p><em>May thy model reach to greater accuracy.</em></p>
<p><em>L√°tom.</em></p>
<p><strong>- Enen No Shouboutai</strong></p>
</blockquote>
<p>I&rsquo;m getting tired of butchering anime and video game quotes so I&rsquo;m thinking I should butcher some meme quotes next time. Anyways, well I have been writing on a lot of topics but something I always wanted to write about is probably explaining a research paper.</p>
<p>I mean I&rsquo;ve been seeing blogs that go by the name of <strong>Paper Dissected</strong> so I wanted to try writing one myself. Well this is going to be my first try in this and frankly, I have no idea how it&rsquo;ll be but let&rsquo;s give it a try, shall we?</p>
<p>I guess it&rsquo;s safe to say that Attention Mechanism and Transformers is something that recently took over NLP. Not only did it show improvements over the SOTA models at the time but also overcome the shortcoming of the RNN models like LSTM and GRU.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1631918244839/oF2h1CrF8.jpeg alt="1610374526-cheems1 (2).jpg">
</p>
<p>So let&rsquo;s go ahead and break down the sections of the paper, <strong>Attention is all you need</strong>. To give you a gist of what&rsquo;ll be there, it&rsquo;ll be an explanation of each section in the paper like abstract, introduction, model architecture, etc.</p>
<h1 id=abstract>Abstract<a hidden class=anchor aria-hidden=true href=#abstract>#</a></h1>
<p>This section pretty much summarizes the whole paper not in terms of working but rather in terms of what it has to offer. It starts with explaining how Seq2Seq models usually use RNN and CNN for encoder and decoder, and how the best ones connect encoder and decoder via attention mechanism.</p>
<p>Now just to be clear, attention itself is not a new concept but to rely completely on attention is something that this paper introduced. It went ahead explaining the results achieved on the machine translation task which were like the following on WMT-2014 dataset:-</p>
<table>
<thead>
<tr>
<th>MT - Task</th>
<th>BLEU Score</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>English to German</strong></td>
<td>28.4</td>
</tr>
<tr>
<td><strong>English to French</strong></td>
<td>41.8</td>
</tr>
</tbody>
</table>
<p>It also took 3.5 days and 8 GPUs to train on, which is certainly out of my budget. But the main thing that shined a bit differently than others in the abstract section was:-</p>
<blockquote>
<p>Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.</p>
</blockquote>
<p>We saw how results were better than traditional models but the thing that separated transformers from LSTM is parallel computation. This can feel a bit tricky to understand at the start so let&rsquo;s break it down. By nature, RNN models aren&rsquo;t parallel but rather Sequential and this is because they compute results one step at a time.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1631732056936/0U5QBxJEa.png alt=YjlBt.png>
</p>
<p>However, in the Transformer model, there is no such thing as &ldquo;time steps&rdquo; but rather the inputs are passed all together and all this is made possible due to multi-headed attention and other stuff which we&rsquo;ll talk about later.</p>
<h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1>
<p>This section mainly talks about the working of RNNs and it&rsquo;s limitations, along with the recent developments to tackle them. Now that we are at it let&rsquo;s understand how does Seq2Seq modeling working in the case of RNNs and to make it easier, take a look a look at the picture below:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1631910532329/yoH-0RbHk.png alt="Seq2Seq Blog Img #1.png">
</p>
<p>Seq2Seq Models consists of 2 models i.e. an Encoder and a Decoder. The Encoder model takes the text as input and generates a Context Vector as an output which is nothing but the last hidden state of RNN. Encoders and Decoders are nothing but RNNs and in few instances CNN üåö.</p>
<p>Another thing you&rsquo;ll notice is the crosses above RNN cells except the last one that doesn&rsquo;t mean they aren&rsquo;t produced, they are but we just don&rsquo;t use them and all we&rsquo;ll be doing is to take the context vector from the last time step and feeding it to the decoder as input who&rsquo;ll output each word at a time giving us the resultant text.</p>
<p>In attention, however, you&rsquo;ll be taking the outputs of all the time step of encoder so you don&rsquo;t discard in that case. I mean you can also do mean pooling on all the states and pass that as context vector, so attention isn&rsquo;t the only way that uses all the states. But how does the decoder predict the words? Let&rsquo;s see, I&rsquo;ll try to be brief.</p>
<p>In the decoder, the first token is the special token signifying the start of the sentence along with the context vector from the encoder, after that you pass the previous output as input to the decoder along with the hidden state to generate new output. However during training, we feed the ground-truth value of targets as input to the decoder, this is known as <strong>Teacher Forcing</strong>. Without which the model weights will be updated on inferred text making it difficult for model to learn from.</p>
<p>However during inference, you&rsquo;ll use generated output as input to generate the next token, and it&rsquo;s cool except the output is being constructed on the context of inferred output rather than actual ones, which causes discrepancy during training and inference, giving rise to the problem of <strong>exposure bias</strong>.</p>
<p>Back to the topic, the nature of RNNs is sequential which makes parallelization difficult for training, making RNN models take more time to train. However, there are few developments that tackle it:-</p>
<blockquote>
<p>Recent work has achieved significant improvements in computational efficiency through <strong>factorization tricks</strong> and <strong>conditional computation</strong>, while also improving model performance in the case of the latter. The fundamental constraint of sequential computation, however, remains.</p>
</blockquote>
<p>To understand the factorization trick you can go ahead and read another paper, i.e. <strong>Factorization Tricks for LSTM Networks</strong> by Oleksii and Boris, which I recommend but I&rsquo;ll give you the gist of it.</p>
<blockquote>
<p>The major part of LSTMP cell computation is in computing affine transform T because it involves multiplication with 4n √ó 2p matrix W.</p>
</blockquote>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1631916140101/s9rWIKqZA.png alt=image.png>
</p>
<p>The aim of the factorization trick is to approximate that W by replacing it with the product of 2 matrices W1(2p*r) and W2(r*4n), this way takes fewer parameters (2p<em>r + r</em>4n) as compared to W (2p*4n) given r &lt; p &lt;=n. Another factorization trick proposes to think parts of inputs and the hidden states as independent of each other and calculate affine transforms of these groups individually and parallelly.</p>
<p>I know it&rsquo;s a lot to grasp so if you don&rsquo;t understand it all at the moment it&rsquo;s fine, important thing to understand is that <strong>many attempts have been made to improve RNNs but the sequential nature isn&rsquo;t completely removed</strong>. The attention model did help in capturing long-term dependencies and improve results, however, it was used along with RNNs.</p>
<p>There is another concept of conditional computation that aims at activating parts of the network to improve computational efficiency, and performance too in this case, but let&rsquo;s not dive much into it. But if you want to you can read this <a href="https://trace.tennessee.edu/cgi/viewcontent.cgi?article=5323&context=utk_graddiss"><strong>short book</strong></a> on the same.</p>
<p>The transformer model on the other hand relies entirely on attention mechanism allowing it to be more parallelizable and give better results with the cost of training being 12 hours on 8 P100 GPUs, something that even Colab Pro users can&rsquo;t get their hands on all the time anymore ü•≤.</p>
<h1 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h1>
<p>Finally, we&rsquo;ve come to the real deal. Now while I wanna try and break down the explanation sub-section wise, I know that may or may not end up leaving you kinda confused, and hence instead of explaining each sub-section of this section we&rsquo;ll dismantle the architecture step by step and understand how it works.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1632343488045/17Un25edDk.png alt=image.png>
</p>
<h2 id=input-embeddings>Input Embeddings<a hidden class=anchor aria-hidden=true href=#input-embeddings>#</a></h2>
<p>Let&rsquo;s start from the beginning, what you have with you is a sentence or bunch of sentences. Now you can&rsquo;t exactly feed it to the model as it is so you need a way to convert it to a form more suitable for it. When we used RNNs for text classification or seq2seq we convert our text to <strong>word embeddings</strong>.</p>
<p>These embeddings are nothing but vector representations of words such that words with similar meanings are closer to each other. But what does closer mean? Mathematically, closer means the similarity score is high. This score can be anything but cosine similarity is popular so feel free to think that. Once we have our embeddings to use we are all set! Are we?</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1632351034260/Qo6JVLncJ.jpeg alt="4824982 (1).jpg">
</p>
<p>Technically, yes we are since these embeddings can be used with RNNs and CNNs because they are able to capture the ordering into consideration. But Transformers use only self-attention, which takes all the words at the same time hence it is not able to take the order of words into consideration. To fix this problem we introduce something called <strong>positional encodings</strong>.</p>
<h2 id=positional-encodings>Positional Encodings<a hidden class=anchor aria-hidden=true href=#positional-encodings>#</a></h2>
<p>So as mentioned before, inherently attention model is not able to take the order of words into consideration. In order to compensate for this, we use something called Positional Encodings. Don&rsquo;t be intimidated by the name it&rsquo;s actually much more simple. Let&rsquo;s dive a bit deeper into this.</p>
<h3 id=need-of-positional-encodings>Need of Positional Encodings<a hidden class=anchor aria-hidden=true href=#need-of-positional-encodings>#</a></h3>
<p>When working with text it&rsquo;s really important to take words order into consideration. Why? The order of words defines the structure of the sentence and this structure helps in providing the context.</p>
<p>RNNs by nature are sequential in nature by taking inputs one by one and updating hidden states accordingly, but the same is not the case with attention since it takes the words all at once. Why this is so, is something we&rsquo;ll understand in the next section but for now, let&rsquo;s say that&rsquo;s how it is.</p>
<p>So we need some extra sources of information in our embeddings to convey this information, something that could tell the position of a word in a sentence. To do so we can add an extra embedding or matrix to our word embeddings to provide info regarding the ordering of the word. But how?</p>
<h3 id=finding-positional-encodings>Finding Positional Encodings<a hidden class=anchor aria-hidden=true href=#finding-positional-encodings>#</a></h3>
<p>Well in the most simple case you can use a one-hot encoded form of [0,1,2,3,4,5,&mldr;, pos] as positional encodings. You can train an embedding layer to find the <strong>positional embedding</strong> matrix, where each row has the positional embedding for word at pos index, for this and add it to input embeddings.</p>
<p>In the paper, however, the proposed approach uses a combination of sin and cos function to find the encodings. Take a look at the following picture:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1632693800233/lcT9IBipy.png alt="i (3).png">
</p>
<p>As you can see in the above picture the encodings are found by using a combination of sin and cos function. The function is quite self-explanatory, it says that for the embedding of a word at position <strong>pos</strong>:-</p>
<ul>
<li>
<p>The value at index <strong>2i</strong> is given using the <strong>sin</strong> function, colored red.</p>
</li>
<li>
<p>The value at index <strong>2i +1</strong> is given using the <strong>cos</strong> function, colored blue.</p>
</li>
</ul>
<p>But why are we using this particular function? Well let&rsquo;s hear from the authors themselves:-</p>
<blockquote>
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions since, for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.</p>
</blockquote>
<p>Well, researchers sure love to make stuff seem more complicated, don&rsquo;t they? It&rsquo;s fine if you don&rsquo;t understand the above sentence, take it as a confirmation that you&rsquo;re still sane.</p>
<p>What the sentence means to say is that given the positional encoding of the word at <strong>pos</strong> you can find the positional encoding of word at <strong>pos + k</strong> by applying some linear transformation on the positional encoding of <strong>pos</strong>. Mathematically, it can be written like this:-</p>
<p>\[PE_{p+k} = T * PE_p\]</p>
<p>Writing <strong>pos</strong> as <strong>p</strong> in the above equation since latex is getting messed up for some reason. The important thing to know is that the matrix T can be used to find PE at pos+k but what is this T? Can we find it? Well yes, that T is a rotational matrix that depends on offset k rather than pos. Wait what! How?</p>
<p>Well, you can derive it, and here is a really good <a href=https://kazemnejad.com/blog/transformer_architecture_positional_encoding/>blog by Amirhossein</a> doing the same. There is another <a href=https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/>blog by Timo Denk</a> that proves the linear relationship between PE at pos and pos+k but the first one explains in a much simpler fashion, but do give both a read.</p>
<h3 id=improving-a-bit-more>Improving a bit more<a hidden class=anchor aria-hidden=true href=#improving-a-bit-more>#</a></h3>
<p>Wow so everything is dope, right? Fortunately or unfortunately, No. Even though everything might seem super nice with these encodings, which it is, what these really are doing is finding values for given pos and i values, these are also called <strong>Absolute Positional Encodings</strong>. While the altering of sin and cosine might be able to add relative nature, we might actually have something better i.e. <strong>Relative Positional Encodings</strong>.</p>
<p>Following this paper, there was another paper released i.e. <strong>Self-Attention with Relative Position Representations</strong> by <a href=https://arxiv.org/pdf/1803.02155.pdf>Shaw et. al. 2019</a>. This paper introduces a modified attention mechanism which is mostly the same except this one has 2 matrices that are added to Key and Value Vector and the aim is that they&rsquo;ll be able to induce pairwise relationships in the inputs. In the author&rsquo;s own words:-</p>
<blockquote>
<p>We propose an extension to self-attention to consider the pairwise relationships between input elements. In this sense, we model the input as a labeled, directed, fully connected graph.</p>
</blockquote>
<p>The paper is actually a pretty decent read and you should give it a try. Since this paper doesn&rsquo;t use these embeddings we&rsquo;ll not go into the detail of these. There are many papers that try to come up with better Positional Encodings and try to improve them, but what they are used for and proper intuition is something you should aim to understand.</p>
<h2 id=attention>Attention<a hidden class=anchor aria-hidden=true href=#attention>#</a></h2>
<p>Attention is something that we&rsquo;ve been talking about a lot in this blog, after all that&rsquo;s what this paper is about. But what does it actually mean? So before diving into the proposed method for attention calculation in this paper, we&rsquo;ll go ahead and understand what attention is and how it helps our cause.</p>
<p>So when we do Seq2Seq with RNNs we have an encoder and decoder network in which the work of the decoder is to generate a sentence given <strong>context vector</strong>. This context vector is nothing but the last hidden state of the encoder which is RNN based model. So what&rsquo;s the issue? Well, you see:-</p>
<ul>
<li>
<p>The <strong>context vector</strong> is a vector of fixed length defined by hidden_dims but the sentences are of variable length and so what we are doing is squeezing info into a vector due to which information might be lost.</p>
</li>
<li>
<p>Another problem is that the generated sentences are based on the context of the whole sentence, however in doing so we might lose out on the local context of parts of the sentence that might&rsquo;ve been helpful in tasks like translation.</p>
</li>
</ul>
<p>So in order to improve on this, we introduce <strong>attention</strong> and instead of working on the global context of the sentence by taking all the hidden states in the encoder and passes it to the decoder. The decoder, in this case, doesn&rsquo;t produce the output directly instead it&rsquo;ll calculate the attention score for the states and creates the context vector for that particular time step given the attention score, the encoder hidden states, and the input vector, output of prev. step or last input token.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1633217192689/yLRmt7ySf.png alt="Normal Decoder.png">
</p>
<p>But how does this help? Well, what attention does is that it generates a context vector at each time step of the decoder based on attention score and this attention score helps to amplify the more important words to focus on during that time step in order to improve the quality of results. If you calculate attention among the words of the input sequence it&rsquo;s called <strong>Self Attention</strong> and when you calculate attention among the words of the input and output sequence it&rsquo;s called <strong>General Attention</strong>.</p>
<p>Now based on the architecture you have 2 types of attention-based Seq2Seq models i.e. <strong>Bahdanau Attention</strong> and <strong>Luong Attention</strong>. Except the architecture and computation both are basically the same but if you wanna learn more you can go ahead and read the paper, <strong>Effective Approaches to Attention-based Neural Machine Translation</strong> by <a href=https://arxiv.org/abs/1508.04025>Luong et. al. 2015</a>.</p>
<h3 id=scaled-dot-product-attention>Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3>
<p>We have a basic idea of how attention works in RNN now, but we don&rsquo;t exactly have an RNN in Transformer or a way to sequentially input data. So the proposed method for calculating attention is via something called <strong>Scaled Dot-Product Attention</strong>, which is basically a bunch of matrix multiplication operations.</p>
<p>But before diving in, let&rsquo;s talk about QKV or Query-Key-Value vectors and their dimensions. Throughout the model, the dimension of columns of the resultant matrix from each layer is set to <strong>512</strong> which is denoted by \( d_{model} \) .</p>
<p>So if we have <strong>m</strong> rows in word embedding we&rsquo;ll have the resultant matrix of dimension \( m * d_{model} \) , this column dimension is consistent for all the layers in the model including the embedding layer.</p>
<p>This is done to facilitate the residual connections, which we&rsquo;ll talk about later. Now that we have output dims cleared let&rsquo;s talk about columns of QKV vectors.</p>
<p>For now, just remember the dimension of QKV vectors being 64, the reasoning is something we&rsquo;ll discuss in Multiheaded attention. The dimensions of Query and Key vector are the same and are denoted by \( d_k \) , and even though here it is same the dimensions of value vector can be different and is denoted by \( d_v \) . So to sum it all up we&rsquo;ll have the following notation for dimensions:-</p>
<p>$$d_{model} = 512$$</p>
<p>$$d_q = d_k = 64$$</p>
<p>$$d_v = 64$$</p>
<p>Now that we have the important stuff cleared, let&rsquo;s take a look at the following image that sums up the process of calculating attention:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1632788371388/1NFBKy8H8D.png alt="S C A L E.png">
</p>
<p>It might seem confusing so let&rsquo;s go in step by step on how attention is being calculated and how its equation is constructed:-</p>
<ul>
<li>
<p>You start by finding <em>Query</em>, <em>Key</em>, and <em>Value</em> vector by multiplying the input matrix with their corresponding weight vector. The dims of these vectors are something we&rsquo;ll discuss in Multi Headed Attention Section. For now, let&rsquo;s just focus on understanding the equation.</p>
<p>$$Attention(Q,K,V)$$</p>
</li>
<li>
<p>Apply Matrix Multiplication on <em>Query</em> and <em>Key</em> Vector.</p>
<p>$$Attention(Q,K,V)=QKT$$</p>
</li>
<li>
<p>Scale the result of above with \( \sqrt{d_k} \) . It was observed that for small values of \( d_k \) additive attention and dot product attention perform similarly. But for larger values of \( d_k \) additive attention outperforms dot product attention without scaling. As mentioned in paper, that for large value the result may explode and may push softmax to smaller gradients. To counteract this \( \frac{1}{\sqrt{d_k}} \) was used for scaling. As to why this occurs, paper mentions:-</p>
</li>
</ul>
<blockquote>
<p><em>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product has mean 0 and variance d_k.</em></p>
</blockquote>
<ul>
<li>
<p>Applying softmax on the result of the above.</p>
<p>$$Attention(Q,K,V)=softmax((QK^T)/\sqrt{d_k})$$</p>
</li>
<li>
<p>Apply Matrix Multiplication on result and <em>Value</em> Vector and get the final result.</p>
<p>$$Attention(Q,K,V)=softmax((QK^T)/\sqrt{dk})V$$</p>
</li>
</ul>
<p>There you go that&rsquo;s how you find attention using Scaled Dot Product Attention. Just for the info, this is not the absolute way to find attention, there are other ways to calculate attention like additive attention, dot-product(unscaled) attention. While the two are similar in complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.</p>
<p>Now let&rsquo;s take a look at the newly proposed approach where instead of calculating attention one time we calculate it multiple times and then concatenate the results. Yep, it is indeed <strong>Multi-Headed Attention</strong>.</p>
<h3 id=multi-headed-attention>Multi-Headed Attention<a hidden class=anchor aria-hidden=true href=#multi-headed-attention>#</a></h3>
<p>Let&rsquo;s talk about the beast that the paper proposed and when I say &ldquo;beast&rdquo; I mean it. The paper suggested that instead of calculating attention one time we can calculate it <strong>h</strong> times with \(d_q\), \(d_k\), \(d_v\) dimensioned QKV vectors found after passing each of them from a Linear Layer.</p>
<p>We then use them to calculate attention values from each attention head depicted by head_i and concatenate these heads forming \( d_{model} \) dimensional resultant vector. After that, we&rsquo;ll pass them through another linear layer to get our Multi-Headed Attention output. Architecture wise it looks like this:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638644522204/RBUNc_MHH.png alt=Linear.png>
</p>
<p>One thing to note is that the value of d_q, d_v, and d_k will be <strong>d_model / h</strong>, that&rsquo;s why after you concatenate, the attention head&rsquo;s output it becomes d_model sized vector. In this paper, the value of <strong>h</strong> for the optimal model was 8. So d_k, d_v, and d_q become 512/8 = 64.</p>
<h4 id=why-does-it-work>Why does it Work?<a hidden class=anchor aria-hidden=true href=#why-does-it-work>#</a></h4>
<p>But after hearing all this it&rsquo;s only sensible to ask. Why does this even work? To answer this query, we should go and take a look at the paper <strong>&ldquo;Analyzing Multi-Head Self-Attention&rdquo;</strong> by <a href=https://arxiv.org/pdf/1905.09418.pdf>Voita et. al. 2019</a>. Where they basically try to find the roles that different attention heads played along with other stuff which we&rsquo;ll talk about later i.e. Attention Head Pruning.</p>
<p>To explain the gist of it the conclusion was, that based on roles attention heads can be divided into 3 parts:-</p>
<ul>
<li>
<p><strong>Positional Heads:</strong> Heads where the maximum attention weights was assigned to a specific relative position. This was usually +/- 1 in practice signifying attention to adjacent positions</p>
</li>
<li>
<p><strong>Syntactical Heads:</strong> Head attends to tokens corresponding to any of the major syntactic relations in a sentence. In the paper, they analyze direct relations like nominal subject, direct object, adjectival modifier, and adverbial modifier.</p>
</li>
<li>
<p><strong>Rare Word Heads</strong>: Heads attend to the least frequent tokens in a sentence.</p>
</li>
</ul>
<p>So different heads try to perform different task cool, but do we need all of them? We have only 3 types right? Does that mean other heads are useless? Yes and No. Let&rsquo;s understand the <strong>Yes</strong> part first.</p>
<p>Yes, attention heads are important and in fact, that&rsquo;s exactly what the blog <strong>&ldquo;Are Sixteen Heads Really Better than One?"</strong> by <a href=https://blog.ml.cmu.edu/2020/03/20/are-sixteen-heads-really-better-than-one/>Paul Michael</a> tries to find. In the blog, he tried to explain how the performance was affected when attention heads were pruned. After the experiment they found the following result:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638650058322/s9kG6ry4m.png alt=image.png>
</p>
<p>As you can see the BLEU Score is really affected after pruning the heads however like other metrics the dip is observed after more than 60% heads are pruned. This leads us to the <strong>No</strong> part. I mean kinda No.</p>
<p>Even though we just saw that heads are important we can also prune one or a few of them without seeing a major dip in the performance, something that both the mentioned sources, that I highly recommend you to go through, suggested. Phew that was a lot, now that we have the initial stuff cleared up, let&rsquo;s get back to the architecture. I promise this point forward is a smooth sailing simple life, probably xD.</p>
<h2 id=encoder>Encoder<a hidden class=anchor aria-hidden=true href=#encoder>#</a></h2>
<p>If we try to understand the architecture of Encoder it can be broken into 2 parts:-</p>
<ul>
<li>
<p>Attention Layer</p>
</li>
<li>
<p>Feed Forward Layer</p>
</li>
</ul>
<p>Yes that&rsquo;s it, you are already familiar with Attention Layer which is nothing but Multi Headed Attention with 8 heads and the Feed Forward Layer is a basic Neural Network with a single hidden layer with 2048 nodes and 512 nodes in input and output layer. Visually it&rsquo;ll look something like this:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638657930173/RL6ONG63c.png alt="Multi-Headed Attention.png">
</p>
<p>Its flow of Infomation is really simple, or not depending on your knowledge of skip connections. If I have to explain in one line skip connection is basically <strong>adding input to the output</strong>.</p>
<p>I know it sounds weird, but trust me it makes sense. ResNet model was the one that popularised these skip connections which provides an alternate path for gradients to flow. They helped train deeper models without running into vanishing gradient problem. So if x is the input to a block and f(x) is the output of the block then skip connections basically add them both making <strong>x + f(x)</strong> the new output.</p>
<p>$$f'(x) = x + f(x)$$</p>
<p>Now that we are all on the same page, let&rsquo;s step by step go through the flow of info in an encoder block:-</p>
<ol>
<li>
<p>Encoder block receives an input <strong>X</strong>.</p>
</li>
<li>
<p>X is supplied to Multi-Headed Attention Layer. QKV all will be the same i.e. <strong>X</strong>.</p>
</li>
<li>
<p>Multi Headed Attention does the magic and generates an output M(x).</p>
</li>
<li>
<p>Add X to M(x) because of skip connection.</p>
</li>
<li>
<p>Apply Layer Normalisation to X + M(x), let&rsquo;s call this N(x).</p>
</li>
<li>
<p>Pass N(x) to Feed Forward layer to generate an output F(x).</p>
</li>
<li>
<p>Add N(x) in F(x) because of skip connection again.</p>
</li>
<li>
<p>Apply Layer Normalisation to F(x) + N(x), let&rsquo;s call this E(x) which is the output of this encoder block.</p>
</li>
</ol>
<p>One last thing, the above is flow of info for one encoder block. This encoder block&rsquo;s output will be passed on to the next encoder block and the architecture of all encoder blocks is the same. The paper proposed the architecture with <strong>6 encoder blocks</strong>. I swear I need to learn animation one of these days, would make stuff much much easier.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638659972564/YTnCToXMN.png alt=image.png>
</p>
<p>Yea I know too much info xD, but hey that&rsquo;s all that you need to know about Encoders. The output of the last encoder block is passed to the decoder as KV vector values, which is our next topic of discussion. We are almost near the end, so hang in there üò¨.</p>
<h2 id=decoder>Decoder<a hidden class=anchor aria-hidden=true href=#decoder>#</a></h2>
<p>Finally, we are at the last step of the architecture, to be honest decoder is not really that different from than encoder except the fact that here were have an extra layer for masked multi-headed attention. We&rsquo;ll get into the details of that in a bit, but let&rsquo;s get an overview of how a decoder block looks:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638738474914/aVah0DKWI.png alt="Decoder (1).png">
</p>
<p>Before going into the flow of information let&rsquo;s understand exactly what is the input and output of the decoder block. As far as input is concerned the decoder takes 2 inputs:-</p>
<ol>
<li>
<p><strong>Encoder Output:</strong> This is will be fed as KV Vectors for multi-headed attention in the 2nd layer, this is usually referred to as Encoder-Decoder Attention. All <strong>6 Decoder Blocks</strong> will take the encoder output from the last block as input for encoder-decoder attention.</p>
</li>
<li>
<p><strong>Output Embedding:</strong> These are the embeddings of the target sentence. But wait!? We don&rsquo;t have a target in testing! So what are we inputting here then? Let&rsquo;s see.</p>
</li>
</ol>
<p>Well usually the first input to the decoder is a special token signifying the start of the output sentence and the output is the token that comes after this. This output is the output of time step 1.</p>
<p>After that, the input to decoder becomes the tokens until a certain timestep i.e. for the 5th timestep input will be all generated tokens till the 4th timestep. After that, it&rsquo;ll keep on giving an output until it throws a special token signifying the end of the output sentence.</p>
<p>Let&rsquo;s understand the flow of information in decoder block to get a better understanding:-</p>
<ol>
<li>
<p>Decoder block receives an input <strong>X</strong>.</p>
</li>
<li>
<p>X is supplied to Masked Multi-Headed Attention as QKV vector, to get an output M(x).</p>
</li>
<li>
<p>Add X in M(x) because of skip connection.</p>
</li>
<li>
<p>Apply Layer Normalisation to X + M(x), let&rsquo;s call this N(x).</p>
</li>
<li>
<p>N(x) is supplied to Multi-Headed Attention as Query and Encoder Output will be supplied as Key-Value..</p>
</li>
<li>
<p>Multi Headed Attention does the magic and generates an output O(x).</p>
</li>
<li>
<p>Add N(x) in O(x) because of skip connection.</p>
</li>
<li>
<p>Apply Layer Normalisation to N(x) + O(x), let&rsquo;s call this P(x).</p>
</li>
<li>
<p>Pass P(x) to Feed Forward layer to generate an output F(x).</p>
</li>
<li>
<p>Add P(x) in F(x) because of skip connection again.</p>
</li>
<li>
<p>Apply Layer Normalisation to P(x) + F(x), let&rsquo;s call this D(x) which is the output of this decoder block.</p>
</li>
</ol>
<p>But all that is nice but how does the decoder predict the next token? Actually, the output of the last decoder block is fed to a Linear Layer that gives an output vector of the size same as that of vocabulary. So depending on how many words the model knows that will be the size of the output vector of this Linear Layer.</p>
<p>Now once you have this vector you apply softmax on this to convert and interpret the values of this vector as probabilities. The token at the index of maximum probability is the output of that &ldquo;time-step&rdquo;. Take a look at the following infographic to understand it better.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638817538390/vAicWO0-Q.png alt=1.01.png>
</p>
<p>Well, that&rsquo;s basically how the decoder works, but we are still missing one key piece in the puzzle i.e. Masked Multi-Headed Attention. It&rsquo;s actually pretty simple and trust me it won&rsquo;t take much long to understand.</p>
<h3 id=masked-multi-headed-attention>Masked Multi-Headed Attention<a hidden class=anchor aria-hidden=true href=#masked-multi-headed-attention>#</a></h3>
<p>If I have to say the difference between Normal Multi-Head Attention and Masked Multi-Head Attention it&rsquo;ll be that in Masked Multi-Head Attention we mask the output of attention score after a point for each token by replacing it with <strong>-inf</strong> and when it applies softmax these -inf become 0.</p>
<p>Well, that&rsquo;s fine but what exactly is going on? Let&rsquo;s understand. The reason we need such a thing in the first place is so that we can speed up training by being able to compute the probabilities of output token for each input token parallelly. The only reason we are doing this stuff is to speed up training.</p>
<p>And just to be clear this passing all at once stuff is something we&rsquo;ll do only in training, during inference we&rsquo;ll still do it step by step by passing previous outputs to get the new ones.</p>
<p>The aim of an autoregressive model is to find the next step given the previous ones but for parallelization, we&rsquo;ll be passing all the stuff together at once hence we&rsquo;ll need to hide the output of the attention score for the future time steps of that token to properly train the model. Let&rsquo;s understand this step by step to get a better understanding.</p>
<h4 id=step-by-step-guide-to-masked-mha>Step-By-Step Guide to Masked MHA<a hidden class=anchor aria-hidden=true href=#step-by-step-guide-to-masked-mha>#</a></h4>
<p>So let&rsquo;s start by knowing what exactly we need to find:-</p>
<p>$$P(x_{i+1}|[x_0, x_1, &mldr;, x_i])$$</p>
<p>What the above equation means is that we&rsquo;ll need to find the <strong>probability of the next word given the previous ones in sequence</strong>. Now let&rsquo;s consider the following sentence and let&rsquo;s assume it to be the expected output:-</p>
<p>$$The \space cake \space was \space sour$$</p>
<p>If you pass words one by one you prevent seeing the future steps but for efficiency, you&rsquo;ll need to pass all the words together. When you do this you expose the decoder to future info, in order to prevent that we mask the weights of all j(>i) future steps, i being the current time step.</p>
<p>So for the word <strong>cake</strong> in the above sentence, we&rsquo;ll mask the weight for words <strong>was</strong> and <strong>sour</strong>, the same way for the word <strong>was</strong> in the above sentence we&rsquo;ll mask the weight for <strong>sour</strong>. We can do this by adding a mask matrix to this:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638824585791/vkXCmdGyw.png alt="Attention Matrix.png">
</p>
<p>Implementation-wise, you apply the mask after doing the scaled matrix multiplication of the QV vector in scaled dot product attention. After this you proceed normally, the only difference is this masking step.</p>
<h1 id=training-and-results>Training and Results<a hidden class=anchor aria-hidden=true href=#training-and-results>#</a></h1>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638824797235/s42ytjQf4.png alt=image.png>
</p>
<p>Done! We are done with the architecture. We are officially out of the deadly stuff and now it&rsquo;s just simple stuff. So let&rsquo;s wrap it up. Let&rsquo;s start by going through some details regarding the training:-</p>
<ul>
<li>
<p><strong>Data:</strong> The WMT English German Dataset contained 4.7 million sentence pairs, with a vocabulary of about 37k tokens. The WMT English French Dataset contained 36 million sentence pairs, which is so big that my hard drive will die before loading it.</p>
</li>
<li>
<p><strong>Hardware:</strong> Richie Rich people xD. Trained on 8 Nvidia P100 GPU something which make me go bankrupt 100 times. Trained for 100k steps for 12hrs. The bigger variants were trained for 300k steps for 3.5 days.</p>
</li>
<li>
<p><strong>Optimizer:</strong> They used Adam optimizer with Œ≤1 = 0.9, Œ≤2 = 0.98 and epsilon = 10^‚àí9. Increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. They used warmup_steps = 4000.</p>
</li>
<li>
<p>**Regularization: ** Dropout of 0.1 was applied to the sum of the positional and wor embeddings. Dropout was also applied to the output of the sublayer before the residual step. Label smoothing was also applied with epsilon = 0.1. Label Smoothing is a way to prevent the model from being too confident about it&rsquo;s prediction.</p>
</li>
</ul>
<p>That&rsquo;s all as far as training is concerned. Now the architecture I explained had very specific parameters but that&rsquo;s not the only model they tried. They actually tried a bunch of them and noted the result of each in the table below:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638742942369/YXSCN_Hzb.png alt=image.png>
</p>
<p>As you can see big model performed the best out of all but between big model and base model with a small performance drop, I&rsquo;ll take my chance with the base one first.</p>
<p>That&rsquo;s all! We have finished the paper and reached the end, take a moment and clap for yourself.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1638743290287/RzCFUzOdXI.gif alt=PlayfulUnknownDuckbillcat-max-1mb.gif>
</p>
<h1 id=from-me-to-you>From Me to You&mldr;<a hidden class=anchor aria-hidden=true href=#from-me-to-you>#</a></h1>
<p>Wow, that was a big blog and I won&rsquo;t lie it was tough to write. This is probably one of my longest and most experimental blog ever. I was super casual and explained everything as much as possible. At some point I wanted to just stop but every time I just thought, just a bit more.</p>
<p>Transformers are probably one of those revolutionary concepts in deep learning that changed the way we approached stuff and I hope I was able to dissect the paper that started it all. I hope you had fun reading it.</p>
<p>Forever and Always, Herumb.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=https://journal.herumbshandilya.com/posts/lima/>
<span class=title>¬´ Prev Page</span>
<br>
<span>LIMA: Less is More for Alignment</span>
</a>
<a class=next href=https://journal.herumbshandilya.com/posts/pytorch-lightning/>
<span class=title>Next Page ¬ª</span>
<br>
<span>PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on twitter" href="https://twitter.com/intent/tweet/?text=Transformers%3a%20Attention%20is%20all%20you%20need&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f&title=Transformers%3a%20Attention%20is%20all%20you%20need&summary=Transformers%3a%20Attention%20is%20all%20you%20need&source=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f&title=Transformers%3a%20Attention%20is%20all%20you%20need"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on whatsapp" href="https://api.whatsapp.com/send?text=Transformers%3a%20Attention%20is%20all%20you%20need%20-%20https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Transformers: Attention is all you need on telegram" href="https://telegram.me/share/url?text=Transformers%3a%20Attention%20is%20all%20you%20need&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2ftransformers%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=https://journal.herumbshandilya.com>Journal | Herumb Shandilya</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>