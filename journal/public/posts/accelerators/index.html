<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Parallelism in CPUs: | Journal | Herumb Shandilya</title><meta name=keywords content><meta name=description content="
You’re absolutely right. I can’t execute anything fast alone.
Everyone has their flaws & imperfections, but that’s what drives us to work together
To make up for those flaws. Together, we complete the job faster.
- Gintoki Sakata, Gintama

Parallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn&rsquo;t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and faster systems."><meta name=author content="Herumb Shandilya"><link rel=canonical href=https://journal.herumbshandilya.com/posts/accelerators/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://journal.herumbshandilya.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://journal.herumbshandilya.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://journal.herumbshandilya.com/favicon-32x32.png><link rel=apple-touch-icon href=https://journal.herumbshandilya.com/apple-touch-icon.png><link rel=mask-icon href=https://journal.herumbshandilya.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://journal.herumbshandilya.com/posts/accelerators/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://journal.herumbshandilya.com/posts/accelerators/"><meta property="og:site_name" content="Journal | Herumb Shandilya"><meta property="og:title" content="Parallelism in CPUs: "><meta property="og:description" content=" You’re absolutely right. I can’t execute anything fast alone.
Everyone has their flaws & imperfections, but that’s what drives us to work together
To make up for those flaws. Together, we complete the job faster.
- Gintoki Sakata, Gintama
Parallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn’t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and faster systems."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-03T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-03T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Parallelism in CPUs: "><meta name=twitter:description content="
You’re absolutely right. I can’t execute anything fast alone.
Everyone has their flaws & imperfections, but that’s what drives us to work together
To make up for those flaws. Together, we complete the job faster.
- Gintoki Sakata, Gintama

Parallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn&rsquo;t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and faster systems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://journal.herumbshandilya.com/posts/"},{"@type":"ListItem","position":2,"name":"Parallelism in CPUs: ","item":"https://journal.herumbshandilya.com/posts/accelerators/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Parallelism in CPUs: ","name":"Parallelism in CPUs: ","description":" You’re absolutely right. I can’t execute anything fast alone.\nEveryone has their flaws \u0026amp; imperfections, but that’s what drives us to work together\nTo make up for those flaws. Together, we complete the job faster.\n- Gintoki Sakata, Gintama\nParallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn\u0026rsquo;t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and faster systems.\n","keywords":[],"articleBody":" You’re absolutely right. I can’t execute anything fast alone.\nEveryone has their flaws \u0026 imperfections, but that’s what drives us to work together\nTo make up for those flaws. Together, we complete the job faster.\n- Gintoki Sakata, Gintama\nParallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn’t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and faster systems.\nMaking better systems, task metric wise, is a matter of research(build-evaluate-improve) but building a faster system like faster training loops or faster inference pipelines is a matter of engineering. More specifically, for faster systems you can either build a faster algorithm, like Flash Attention, or you can build better hardware that optimizes for speed of execution, like AI Accelerators. This blog focuses on the former, we’ll see how we can make algorithms faster and how each of them works on hardware level i.e. CPUs.\nIn up coming blogs we’ll talk about the latter, more specifically looking at the working of AI accelerator chips like LPUs, GPUs, TPUs, WSE-3 etc. But before all that let’s under how execution/computation happens and how we can make it faster on software level!\nSingle Thread Execution What are we executing? Whenever we talk about execution we usually mean execution of a program. What is a program though? It’s quite literally a set of executable instructions, for example when you write a code in C you compile it to a binary which literally long list of assembly instructions in binary format.\nSo, when you write something innocent like this:\n#include int main() { int a = 5; int b = 7; int c = a + b; printf(\"c = %d\\n\", c); return 0; } your CPU is running a sequence like this:\n0000000100000f50 pushq %rbp 0000000100000f51 movq %rsp, %rbp 0000000100000f54 subq $0x10, %rsp 0000000100000f58 movl $0x0, -0x4(%rbp) 0000000100000f5f movl $0x5, -0x8(%rbp) 0000000100000f66 movl $0x7, -0xc(%rbp) 0000000100000f6d movl -0x8(%rbp), %eax 0000000100000f70 addl -0xc(%rbp), %eax 0000000100000f73 movl %eax, -0x10(%rbp) 0000000100000f76 leaq 0x15(%rip), %rdi 0000000100000f7d movb $0x0, %al 0000000100000f7f callq 0x100000f8c 0000000100000f84 xorl %eax, %eax 0000000100000f86 addq $0x10, %rsp 0000000100000f8a popq %rbp 0000000100000f8b retq The speed at which this program will be executed depends on how fast your CPU works. Usually, this measured by the clock frequency of the CPU. Clock frequency tells you how many cycles occur per second. The number of instructions per second depends on how many instructions the CPU can do per cycle.\n…the Kaby Lake counterparts can achieve 4.9GHz…\nIt basically means Intel’s Kaby Lake makes 4.9e9 cycles per second. Higher the clock frequency faster the execution. During execution the program is loaded on main memory, like RAM, CPU gets this executed and that’s usually the end of story. The code we saw above compiles to roughly 15 assembly instructions.\nFor simplicity, let’s assume each instruction takes ~1 cycle. In reality, modern CPUs can execute multiple instructions per cycle, and some instructions take longer depending on their latency and dependencies. With only ~15 instructions, the execution time is effectively negligible, on the order of a few nanoseconds.\nCISC and RISC Instruction sets ISC hai toh RISC hai\nHow CPU executes program? When you start the execution the instructions are loaded onto the main memory, these instruction need to be executed in an order so from here CPU fetches the instruction the needs to run and it executes it.\nGoing more in depth, in the CPU core there is a dedicated Instruction Processor that handles finding what instruction to execute and decode which component(ALUs, registers, etc.) will handle this. Then that component executes it.\nThis Instruction Processor is more formally known as Control Unit. Fetching and decoding is one job of the Control Unit, it also takes the decoded command and passes it to the right hardware block. If it’s arithmetic, it’ll go to an Arithmetic Logic Unit (ALU). This is where the actual math happens: addition, subtraction, comparisons, bitwise operations. For more complex operations, like floating point multiplication, the CPU calls upon the Floating Point Unit (FPU), which specializes in handling decimal numbers.\nLet’s say the instruction fetched is: addl -0xc(%rbp), %eax. The ALU(our execution engine) fetches the current values from registers: %rbp and %eax from the thread execution state. That’s a single instruction done, often in just one or a few cycles. Meanwhile, the control logic already begins fetching the next instruction, ensuring the core never sits idle.\nThe execution state of the thread is stored in registers which are small but crazy fast ephemeral storage. Aside from this you have cache hierarchies that CPU manages to “save” the data so it can be fetched quickly. Why bother with different cache and memory hierarchy? If you are interested you can read the following blob.\nWorking and Need of Caches This is a summary using the IDEA callout!\nIt starts by fetching the next instruction, pulling it from memory (likely from a small but incredibly quick instruction cache). Once fetched, it decodes it i.e. the CPU’s logic interprets what that instruction means: maybe it’s an addition, a comparison, or a write to memory. Finally, it executes the operation, carrying it out on the data stored in its tiny working memory called registers.\nHow do you pass the signal/data around? Buses\nSo that’s how execution happens, but how do you make it fast? Two words, Clock…Frequency.\nMaking Execution go brrr The most straightforward way to make a program run faster is to make the CPU itself run faster. Since everything inside the CPU is ultimately driven by a cycle, increasing the clock frequency means increasing how many cycles happen per second and therefore, how many instructions the CPU gets to execute.\nIf a CPU runs at 1 GHz clock frequency, it makes 1 billion cycle per second. At 5 GHz, it makes 5 billion cycles per second. If your program takes, say, 1 billion cycles to finish, then:\nOn a 1 GHz CPU -\u003e it takes ~1 second On a 5 GHz CPU -\u003e it takes ~0.2 seconds Execution wise, everything is same in both except one has a faster clock frequency and thus executes faster.\nEach instruction still goes through the same fetch-decode-execute pipeline, but the pipeline itself is moving faster. Every stage completes sooner, so more instructions finish per unit time.\nHistorically, this is exactly how CPUs got faster for decades programmers wrote code and every year their program worked faster without them needing to do anything. Why? Because each generation just cranked the clock frequency higher and programs just got executed faster. Free Lunch FTW!!!\nSadly, that came to an end recently. To understand why it all came to an end we’ll need to learn a bit more on how clock frequency can be increased and what exactly happens in cycle.\nFree Lunch is Over How TF do you increase clock frequency? A CPU core is just a giant collection transistor. NO READ THIS CAREFULLY!!! A CPU CORE IS LITERALLY A GIANT COLLECTION OF TRANSISTORS!!!. To emphasize more and make this more concrete everything in core is a transistor:\n1 DRAM Bit: 1 transistor (and 1 Capacitor) 1 SRAM Bit: 6 transistors 1 64-bit Registers(1 SRAM): 6 * 64 = 384 transistors L1 Cache( \\( l_1 \\) kb): 6 * 1024 * \\( l_1 \\) transistors ALU: Millions of transistors!!! …list goes on These transistors can store two states 0 and 1. When you execute code these transistors change these state from 0 to 1 or 1 to 0. The CPU is driven by a clock which ticks at a fixed rate. This clock period is chosen so that all transistor switching and signal propagation inside the CPU can complete before the next tick. A cycle is one tick of the CPU clock and as we saw before the number of cycles CPU can do in 1 second is clock frequency.\nHow TF does a CPU Clock work? Quartz crystal oscillator, Phase locked loop\nNow the states 0 and 1 is usually decided by voltage in transistors. To change the state we need to change the voltage and this is what limit the speed. So if we decide to increase this clock frequency we’ll need to make this faster. Few ways to do that are:\nIncreasing Voltage: The time for a transistor to switch depends on how fast you can charge or discharge its gate capacitance through the available current. This scale linearly with the voltage, so higher the voltage you supply the faster the transistor will switch and smaller the cycle could be. This is what you call Overclocking, you might have seen this in setting and it just ramps up the supplied voltage.\nMake transistors smaller: The time of propagation inside and outside the transistors is limited by the speed and distance the electron covers. We call this gate delay. While speed of atomic is something we can’t accessibly change we can reduce the distance they cover by make the transistors smaller. Thus make transistors smaller in turn leads to making the signal propagation fast and turn reduces the clock period.\nReducing critical path: A huge chunk of delay is just wires. Signals need to travel across the chip, and those wires have resistance and capacitance. If we place things closer together and make critical paths shorter via optimal layouts, we can reduce signal propagation delays and increase clock frequency which is dependant on critical path.\nSo as we saw while clock frequency can be made faster it’s not as free lunch as we expected it to be. This is also the reason why the speed gains via clock frequency stopped. Let’s see that in more detail!\nEnd of Free Lunch As I mentioned in prior sections typically for execution speedups we just relied on cranking up clock frequency. Year after year, CPUs got faster clocks and the code just ran faster without any code changes. This was nicely captured in The Free Lunch Is Over by Herb Sutter and as you can see the title says it all.\nThe free lunch was built on two pillars: Moore’s Law and Dennard Scaling. Moore’s Law predicted that transistor count would double roughly every two years, which meant chips could pack more functionality. Dennard Scaling promised that as transistors got smaller, they’d use less voltage and switch faster while maintaining constant power density. Together, these meant: make transistors smaller would use same power, switch faster, have higher clock speeds and faster execution. Free. Fucking. Lunch.\nThis worked smoothy from the 1970s through early 2000s. Clock speeds went from MHz to GHz. Intel’s Pentium 4 in 2004 hit 3.8 GHz, and the roadmap pointed to 10 GHz by 2010. Except that never happened. Instead, clock speeds plateaued around 3-5 GHz and have stayed there ever since.\nWhy? Didn’t we just discuss the ways to make clock speed faster?? Well yes, we talked about the ideas but each idea comes with an issue…\nIncreasing Voltage:\nThe Issue: This is not free though…doing this the power consumption goes up quadratically with voltage (P ≈ V²·f) and a lot more heat is dissipated because of which the system is more prone to reaching thermal limit making the chip burn off. Make transistors smaller\nThe Issue: As transistors get smaller their power density stays constant this is what we call Dennard Scaling. So this meant that if we made transistors smaller we can supply lower voltage and the transistors would be cool and fast. This worked…until it didn’t.\nDennard Scaling ignored the existence of leakage current(transistors don’t fully turn off which happens because of Quantum Tunneling) and threshold voltage(minimum voltage, aside from supplied, needed to keep transistor working), both of which scaled with time. So, the power density started going up and didn’t remain constant.\nQuantum Tunneling Effect when electrons pass through thin insulating barriers even when the device should be off.\nIf you wanna know why this happens this short answer explains it quite well.\nSo did we stop? No, We still make transistors smaller But they no longer give free clock speed. Not to mention how costly it is to manufacture smaller transistors and Quantum tunneling in them.\nReducing critical path\nThe Issue: If it was that easy it wouldn’t be an issue lmao. Such Chip layouts unfortunately are usually not optimal and even in todays chips propagation delays like these dominate even the gate delay inside transistors. Hardware folks saw this doom coming way before 2005 and had already started cooking. The core idea was what if we execute multiple instructions at the same time rather than trying to make the single instruction run fast?\nParallelism on Single Thread Let’s understand some ways of how parallelism was induces on hardware end to help felicitate concurrent instruction execution. For reference this is how our normie CPU looks like until now:\nInstruction Level Parallelism We understand that we want to execute multiple instructions at once, but what instructions can be execute in parallel? Let’s take the example of the following program:\nInstruction Address OPs Operands ================================================ 0000000100000f50 pushq %rbp 0000000100000f51 movq %rsp, %rbp 0000000100000f54 subq $0x10, %rsp 0000000100000f58 movl $0x0, -0x4(%rbp) 0000000100000f5f movl $0x5, -0x8(%rbp) 0000000100000f66 movl $0x7, -0xc(%rbp) We have six instructions here we can see the first three depend on each other i.e. 3rd instruction can’t be executed until 2nd instructions is executed and 2nd instruction can’t run until 1st instruction is executed. The last 3 instructions, however, are independant of each other, so these can be executed in parallel.\nWhy are last 3 parallel when they all work on %rbp? Well yes they all use %rbp as a base register but they’re writing to different memory locations because mov $a, b(%rbp) in assembly copy value \"a\" to the address (b + what’s in %rbp):\nmovl $0x0, -0x4(%rbp) writes 0 to offset -0x4 from %rbp movl $0x5, -0x8(%rbp) writes 5 to offset -0x8 from %rbp movl $0x7, -0xc(%rbp) writes 7 to offset -0xc from %rbp Since they write to different slots (different offsets from %rbp), there are no dependencies between them. They only read %rbp, so they can all execute simultaneously without conflicts.\nSo our instruction dependency/execution graph would look like this:\nSo what a CPU does is rather than fetching and decoding 1 instruction at a time it’ll have multiple instructions being fetched and decoded. Hardware wise this means we’ll have multiple control units at play:\nSo now our CPU can fetch and decode 2 instructions simultaneously thanks to multiple control units. But this still would only execute one operation, let’s say arithmetic, at a time because we only have 1 ALU. For this to work, the CPU must actually have the hardware that enable execution of multiple instructions in the same cycle. How do we do that? Add more ALUs!!!!!\nNow our CPU can fetch-decode-execute 2 instructions at the same time!! This ability to make CPU execute multiple instruction at same time is called Instruction level parallelism and any dependancy that stalls this is called a Hazard. If it can execute 2 instructions simultaneously it’s called 2-wide superscalar, if it can execute 3 instructions simultaneously it’s called 3-wide superscalar and so on!\nILP is the reason why 2.4Ghz CPUs with ILP are faster than 3GHz CPUs with no ILP. You might think now we’ve overcome the issue and we are in free lunch land again. Unfortunately, ILP gains plateaued around 2005 as well and Herb Sutter’s article shows that in the graph as well. Honestly, ILP is fairly old as well so it wasn’t like people waited for sequential execution to stagnate to implement ILP.\nSo, is single thread era done for? Well, kinda. Around early 2000, a major shift towards building concurrent systems started happening. People started utilizing thread concurrency in software more and more. But there is still one way that is still widely used to enable stuff like vectorization.\nSIMD: Single Instruction, Multiple Data Look at the following program:\n#include int main() { int a[] = {1,2,3,4,5,6}; int b[] = {1,2,3,4,5,6}; int arr_size = sizeof(a) / sizeof(int); int c[arr_size]; for(int i = 0; i \u003c arr_size; i++) c[i] = a[i] + b[i]; return 0; } Above is an example of basic element-wise addition of two array. We are iterating over the array and adding them i.e. the add operation instruction for each element is fetched-decoded-executed. But is that the optimal way? If you think the ILP way, all the addition operations are independant of each other and the ILP graph would be:\nIf you look at the graph, you’ll notice all the element wise operations in the for loop are independent of each other and do the same operation…addition, but since we are using a for loop this’ll be done 1 at a time. Even with loop unrolling and ILP, we are still fundamentally executing one arithmetic operation per data element. The CPU may issue a few of them in parallel, but we are still limited by the quantity of compute units i.e. ALUs here. What we really want is to apply the same operation to many data elements inside the execution unit itself.\nThis is where SIMD comes in place which executes one operation on a batch of data rather than one by one! ILP exploits parallelism across instructions. SIMD exploits parallelism inside the data. SIMD exists because it’s much better to apply one operation to many data elements than to apply the same operation many times to one element each. But how do we enable it say hardware wise?\nWe have special SIMD units which when prompted execute one instruction that operates on multiple values at once and we use wider registers(XMM[128 bits], YMM[256 bits], ZMM[512 bits]), alongside the typical 64 bit ones, capable of storing multiple packed values instead on single ones! So if we have one such wide register of 256 bits it would be able to store: 8 values of size 32 bits, 4 values of size 64 bits, etc.\nTo invoke a SIMD unit operation you’ll need special instructions that tell the SIMD units what to do. Different processor architectures have different SIMD instruction sets:\nx86/x64 (Intel/AMD): SSE (Streaming SIMD Extensions), AVX (Advanced Vector Extensions), AVX2, AVX-512 ARM: NEON, SVE (Scalable Vector Extension) RISC-V: V extension Let’s see how our array addition example would work with SIMD:\n#include // AVX intrinsics #include int main() { int a[] = {1, 2, 3, 4, 5, 6, 7, 8}; int b[] = {1, 2, 3, 4, 5, 6, 7, 8}; int arr_size = sizeof(a) / sizeof(int); int c[arr_size]; // Load 8 integers (256 bits) at once into SIMD registers __m256i vec_a = _mm256_loadu_si256((__m256i*)a); __m256i vec_b = _mm256_loadu_si256((__m256i*)b); // Add all 8 pairs in one instruction __m256i vec_c = _mm256_add_epi32(vec_a, vec_b); // Store result back _mm256_storeu_si256((__m256i*)c, vec_c); return 0; } For now let’s briefly skim what this program is doing, SIMD Programming can be a different blog on it’s own later. In this SIMD program, instead of 8 separate addition instructions, we execute one instruction that adds 8 pairs of integers simultaneously. This is crazy efficient!!\nWithout SIMD: 8 iterations, 8 separate add instructions With SIMD (256-bit): 1 iteration, 1 vectorized add instruction operating on 8 values Of course is not all perfect, you can’t using conditional without getting a performance hit due to Divergent Execution, lanes are limited, etc. So at this point we need to face the truth folks…Single Thread Free Lunch is done for. Good news is we got used to a new era of Parallel Computing on Multi Core Processor.\nOn hardware end, we scaled the number of threads a core can run and number of cores that are there. Software developers would also need to write concurrent programs that can utilize multiple cores effectively. This is where multi-threading, parallel algorithms, and concurrent programming models become essential. We’ll\nMulti Threading Multi Processing The Modern CPU ","wordCount":"3340","inLanguage":"en","datePublished":"2025-01-03T00:00:00Z","dateModified":"2025-01-03T00:00:00Z","author":{"@type":"Person","name":"Herumb Shandilya"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://journal.herumbshandilya.com/posts/accelerators/"},"publisher":{"@type":"Organization","name":"Journal | Herumb Shandilya","logo":{"@type":"ImageObject","url":"https://journal.herumbshandilya.com/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://journal.herumbshandilya.com/ accesskey=h title="Herumb's Journal (Alt + H)">Herumb's Journal</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://journal.herumbshandilya.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://the-ir-book.herumbshandilya.com/ title="The Small Book of Information Retrieval"><span>The Small Book of Information Retrieval</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.herumbshandilya.com/ title=Portfolio><span>Portfolio</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Parallelism in CPUs:</h1><div class=post-meta><span title='2025-01-03 00:00:00 +0000 UTC'>January 3, 2025</span>&nbsp;·&nbsp;<span>16 min</span>&nbsp;·&nbsp;<span>3340 words</span>&nbsp;·&nbsp;<span>Herumb Shandilya</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#single-thread-execution>Single Thread Execution</a><ul><li><a href=#what-are-we-executing>What are we executing?</a></li><li><a href=#how-cpu-executes-program>How CPU executes program?</a></li><li><a href=#making-execution-go-brrr>Making Execution go brrr</a></li></ul></li><li><a href=#free-lunch-is-over>Free Lunch is Over</a><ul><li><a href=#how-tf-do-you-increase-clock-frequency>How TF do you increase clock frequency?</a></li><li><a href=#end-of-free-lunch>End of Free Lunch</a></li></ul></li><li><a href=#parallelism-on-single-thread>Parallelism on Single Thread</a><ul><li><a href=#instruction-level-parallelism>Instruction Level Parallelism</a></li><li><a href=#simd-single-instruction-multiple-data>SIMD: Single Instruction, Multiple Data</a></li></ul></li><li><a href=#multi-threading>Multi Threading</a></li><li><a href=#multi-processing>Multi Processing</a></li><li><a href=#the-modern-cpu>The Modern CPU</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p><em>You’re absolutely right. I can’t execute anything fast alone.</em></p><p><em>Everyone has their flaws & imperfections, but that’s what drives us to work together</em></p><p><em>To make up for those flaws. Together, we complete the job faster.</em></p><p>- <strong>Gintoki Sakata, Gintama</strong></p></blockquote><p>Parallelism is kinda like that too, bunch of worker doing there work in chunks and if needed accumulating the results to get the final answer. Wasn&rsquo;t always like this though. Anyways, AI is everywhere these days and there are people who build AI and people who build with AI. At the heart of it both aim for the same thing, building better and <strong>faster</strong> systems.</p><p>Making better systems, task metric wise, is a matter of research(build-evaluate-improve) but building a faster system like faster training loops or faster inference pipelines is a matter of engineering. More specifically, for faster systems you can either build a faster algorithm, like Flash Attention, or you can build better hardware that optimizes for speed of execution, like AI Accelerators. This blog focuses on the former, we&rsquo;ll see how we can make algorithms faster and how each of them works on hardware level i.e. CPUs.</p><p>In up coming blogs we&rsquo;ll talk about the latter, more specifically looking at the working of AI accelerator chips like LPUs, GPUs, TPUs, WSE-3 etc. But before all that let&rsquo;s under how execution/computation happens and how we can make it faster on software level!</p><h2 id=single-thread-execution>Single Thread Execution<a hidden class=anchor aria-hidden=true href=#single-thread-execution>#</a></h2><h3 id=what-are-we-executing>What are we executing?<a hidden class=anchor aria-hidden=true href=#what-are-we-executing>#</a></h3><p>Whenever we talk about execution we usually mean execution of a <strong>program</strong>. What is a program though? It&rsquo;s quite literally a <em>set of executable instructions</em>, for example when you write a code in C you compile it to a binary which literally long list of assembly instructions in binary format.</p><p>So, when you write something innocent like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> a <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> b <span style=color:#f92672>=</span> <span style=color:#ae81ff>7</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> c <span style=color:#f92672>=</span> a <span style=color:#f92672>+</span> b;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>printf</span>(<span style=color:#e6db74>&#34;c = %d</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span>, c);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>your CPU is running a sequence like this:</p><pre tabindex=0><code>0000000100000f50        pushq   %rbp
0000000100000f51        movq    %rsp, %rbp
0000000100000f54        subq    $0x10, %rsp
0000000100000f58        movl    $0x0, -0x4(%rbp)
0000000100000f5f        movl    $0x5, -0x8(%rbp)
0000000100000f66        movl    $0x7, -0xc(%rbp)
0000000100000f6d        movl    -0x8(%rbp), %eax
0000000100000f70        addl    -0xc(%rbp), %eax
0000000100000f73        movl    %eax, -0x10(%rbp)
0000000100000f76        leaq    0x15(%rip), %rdi
0000000100000f7d        movb    $0x0, %al
0000000100000f7f        callq   0x100000f8c
0000000100000f84        xorl    %eax, %eax
0000000100000f86        addq    $0x10, %rsp
0000000100000f8a        popq    %rbp
0000000100000f8b        retq
</code></pre><p>The speed at which this program will be executed depends on how fast your CPU works. Usually, this measured by the clock frequency of the CPU. Clock frequency tells you how many cycles occur per second. The number of instructions per second depends on how many instructions the CPU can do per cycle.</p><blockquote><p><em>&mldr;the Kaby Lake counterparts can achieve 4.9GHz&mldr;</em></p></blockquote><p>It basically means Intel&rsquo;s Kaby Lake makes <code>4.9e9</code> cycles per second. Higher the clock frequency faster the execution. During execution the program is loaded on main memory, like RAM, CPU gets this executed and that&rsquo;s usually the end of story. The code we saw above compiles to roughly 15 assembly instructions.</p><p>For simplicity, let&rsquo;s assume each instruction takes ~1 cycle. In reality, modern CPUs can execute multiple instructions per cycle, and some instructions take longer depending on their latency and dependencies. With only ~15 instructions, the execution time is effectively negligible, on the order of a few nanoseconds.</p><link rel=stylesheet href=/css/vendors/admonitions.a5128328e65b14e9af1bfc5d96ffbcc40978b6d81dbeecb7f70959b501ee715c.css integrity="sha256-pRKDKOZbFOmvG/xdlv+8xAl4ttgdvuy39wlZtQHucVw=" crossorigin=anonymous><div class="admonition idea"><div class=admonition-header><svg viewBox="0 0 384 512"><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2.0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zM192 512c44.2.0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8.0 16 7.2 16 16s-7.2 16-16 16c-44.2.0-80 35.8-80 80z"/></svg>
<span>CISC and RISC Instruction sets</span></div><div class=admonition-content><p>ISC hai toh RISC hai</p></div></div><h3 id=how-cpu-executes-program>How CPU executes program?<a hidden class=anchor aria-hidden=true href=#how-cpu-executes-program>#</a></h3><p>When you start the execution the instructions are loaded onto the main memory, these instruction need to be executed in an order so from here CPU fetches the instruction the needs to run and it executes it.</p><p><img alt="CPU Execution - Level 1" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Level%201.png></p><p>Going more in depth, in the CPU core there is a dedicated <strong>Instruction Processor</strong> that handles finding what instruction to execute and decode which component(ALUs, registers, etc.) will handle this. Then that component executes it.</p><p><img alt="CPU Execution - Level 2" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Level%202.png></p><p>This <strong>Instruction Processor</strong> is more formally known as <strong>Control Unit</strong>. Fetching and decoding is one job of the Control Unit, it also takes the decoded command and passes it to the right hardware block. If it’s arithmetic, it’ll go to an <strong>Arithmetic Logic Unit (ALU)</strong>. This is where the actual math happens: addition, subtraction, comparisons, bitwise operations. For more complex operations, like floating point multiplication, the CPU calls upon the <strong>Floating Point Unit (FPU)</strong>, which specializes in handling decimal numbers.</p><p><img alt="CPU Execution - Level 3" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Level%203.png></p><p>Let&rsquo;s say the instruction fetched is: <code>addl -0xc(%rbp), %eax</code>. The ALU(our execution engine) fetches the current values from registers: <code>%rbp</code> and <code>%eax</code> from the thread <em>execution state</em>. That’s a single instruction done, often in just one or a few cycles. Meanwhile, the <strong>control logic</strong> already begins fetching the next instruction, ensuring the core never sits idle.</p><p>The <em>execution state</em> of the thread is stored in registers which are small but crazy fast ephemeral storage. Aside from this you have cache hierarchies that CPU manages to &ldquo;save&rdquo; the data so it can be fetched quickly. Why bother with different cache and memory hierarchy? If you are interested you can read the following blob.</p><div class="admonition idea"><div class=admonition-header><svg viewBox="0 0 384 512"><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2.0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zM192 512c44.2.0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8.0 16 7.2 16 16s-7.2 16-16 16c-44.2.0-80 35.8-80 80z"/></svg>
<span>Working and Need of Caches</span></div><div class=admonition-content><p>This is a summary using the <code>IDEA</code> callout!</p></div></div><p><img alt="CPU Execution - Level 4" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Level%204.png></p><p>It starts by <strong>fetching</strong> the next instruction, pulling it from memory (likely from a small but incredibly quick instruction cache). Once fetched, it <strong>decodes</strong> it i.e. the CPU’s logic interprets what that instruction means: maybe it’s an addition, a comparison, or a write to memory. Finally, it <strong>executes</strong> the operation, carrying it out on the data stored in its tiny working memory called <em>registers</em>.</p><div class="admonition idea"><div class=admonition-header><svg viewBox="0 0 384 512"><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2.0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zM192 512c44.2.0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8.0 16 7.2 16 16s-7.2 16-16 16c-44.2.0-80 35.8-80 80z"/></svg>
<span>How do you pass the signal/data around?</span></div><div class=admonition-content><p>Buses</p></div></div><p>So that&rsquo;s how execution happens, but how do you make it fast? Two words, Clock&mldr;Frequency.</p><h3 id=making-execution-go-brrr>Making Execution go brrr<a hidden class=anchor aria-hidden=true href=#making-execution-go-brrr>#</a></h3><p>The most straightforward way to make a program run faster is to make the CPU itself run faster. Since everything inside the CPU is ultimately driven by a cycle, increasing the clock frequency means increasing how many cycles happen per second and therefore, how many instructions the CPU gets to execute.</p><p>If a CPU runs at 1 GHz clock frequency, it makes 1 billion cycle per second. At 5 GHz, it makes 5 billion cycles per second. If your program takes, say, 1 billion cycles to finish, then:</p><ul><li>On a 1 GHz CPU -> it takes ~1 second</li><li>On a 5 GHz CPU -> it takes ~0.2 seconds</li></ul><p>Execution wise, everything is same in both except one has a faster clock frequency and thus executes faster.</p><p><img alt="Clock Speed Analogy" loading=lazy src=/posts/accelerators/imgs/Clock%20Frequency%20Comparison.png></p><p>Each instruction still goes through the same fetch-decode-execute pipeline, but the pipeline itself is moving faster. Every stage completes sooner, so more instructions finish per unit time.</p><p>Historically, this is exactly how CPUs got faster for decades programmers wrote code and every year their program worked faster without them needing to do anything. Why? Because each generation just cranked the clock frequency higher and programs just got executed faster. Free Lunch FTW!!!</p><p>Sadly, that came to an end recently. To understand why it all came to an end we&rsquo;ll need to learn a bit more on how clock frequency can be increased and what exactly happens in cycle.</p><h2 id=free-lunch-is-over>Free Lunch is Over<a hidden class=anchor aria-hidden=true href=#free-lunch-is-over>#</a></h2><h3 id=how-tf-do-you-increase-clock-frequency>How TF do you increase clock frequency?<a hidden class=anchor aria-hidden=true href=#how-tf-do-you-increase-clock-frequency>#</a></h3><p>A CPU core is just a giant collection transistor. NO READ THIS CAREFULLY!!! <strong>A CPU CORE IS LITERALLY A GIANT COLLECTION OF TRANSISTORS!!!</strong>. To emphasize more and make this more concrete everything in core is a transistor:</p><ul><li><strong>1 DRAM Bit:</strong> 1 transistor (and 1 Capacitor)</li><li><strong>1 SRAM Bit:</strong> 6 transistors</li><li><strong>1 64-bit Registers(1 SRAM):</strong> 6 * 64 = 384 transistors</li><li><strong>L1 Cache( \( l_1 \) kb):</strong> 6 * 1024 * \( l_1 \) transistors</li><li><strong>ALU:</strong> Millions of transistors!!!</li><li>&mldr;list goes on</li></ul><p>These transistors can store two states 0 and 1. When you execute code these transistors change these state from 0 to 1 or 1 to 0. The CPU is driven by a clock which ticks at a fixed rate. This clock period is chosen so that all transistor switching and signal propagation inside the CPU can complete before the next tick. A cycle is one tick of the CPU clock and as we saw before the number of cycles CPU can do in 1 second is clock frequency.</p><div class="admonition idea"><div class=admonition-header><svg viewBox="0 0 384 512"><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2.0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zM192 512c44.2.0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8.0 16 7.2 16 16s-7.2 16-16 16c-44.2.0-80 35.8-80 80z"/></svg>
<span>How TF does a CPU Clock work?</span></div><div class=admonition-content><p>Quartz crystal oscillator, Phase locked loop</p></div></div><p>Now the states 0 and 1 is usually decided by voltage in transistors. To change the state we need to change the voltage and this is what limit the speed. So if we decide to increase this clock frequency we&rsquo;ll need to make this faster. Few ways to do that are:</p><ul><li><p><strong>Increasing Voltage:</strong> The time for a transistor to switch depends on how fast you can charge or discharge its gate capacitance through the available current. This scale linearly with the voltage, so higher the voltage you supply the faster the transistor will switch and smaller the cycle could be. This is what you call <strong>Overclocking</strong>, you might have seen this in setting and it just ramps up the supplied voltage.</p></li><li><p><strong>Make transistors smaller:</strong> The time of propagation inside and outside the transistors is limited by the speed and distance the electron covers. We call this <em>gate delay</em>. While speed of atomic is something we can&rsquo;t accessibly change we can reduce the distance they cover by make the transistors smaller. Thus make transistors smaller in turn leads to making the signal propagation fast and turn reduces the clock period.</p></li><li><p><strong>Reducing critical path:</strong> A huge chunk of delay is just wires. Signals need to travel across the chip, and those wires have resistance and capacitance. If we place things closer together and make critical paths shorter via optimal layouts, we can reduce signal propagation delays and increase clock frequency which is dependant on critical path.</p></li></ul><p>So as we saw while clock frequency can be made faster it&rsquo;s not as free lunch as we expected it to be. This is also the reason why the speed gains via clock frequency stopped. Let&rsquo;s see that in more detail!</p><h3 id=end-of-free-lunch>End of Free Lunch<a hidden class=anchor aria-hidden=true href=#end-of-free-lunch>#</a></h3><p>As I mentioned in prior sections typically for execution speedups we just relied on cranking up clock frequency. Year after year, CPUs got faster clocks and the code just ran faster without any code changes. This was nicely captured in <a href=https://www.cs.utexas.edu/~lin/cs380p/Free_Lunch.pdf>The Free Lunch Is Over by Herb Sutter</a> and as you can see the title says it all.</p><p>The free lunch was built on two pillars: <strong>Moore&rsquo;s Law</strong> and <strong>Dennard Scaling</strong>. Moore&rsquo;s Law predicted that transistor count would double roughly every two years, which meant chips could pack more functionality. Dennard Scaling promised that as transistors got smaller, they&rsquo;d use less voltage and switch faster while maintaining constant power density. Together, these meant: make transistors smaller would use same power, switch faster, have higher clock speeds and faster execution. Free. Fucking. Lunch.</p><p><img alt="Free Lunch" loading=lazy src=/posts/accelerators/imgs/FFL.png></p><p>This worked smoothy from the 1970s through early 2000s. Clock speeds went from MHz to GHz. Intel&rsquo;s Pentium 4 in 2004 hit 3.8 GHz, and the roadmap pointed to 10 GHz by 2010. Except that never happened. Instead, clock speeds plateaued around 3-5 GHz and have stayed there ever since.</p><p><img alt="End of Free Lunch" loading=lazy src=/posts/accelerators/imgs/konosuba.png></p><p>Why? Didn&rsquo;t we just discuss the ways to make clock speed faster?? Well yes, we talked about the ideas but each idea comes with an issue&mldr;</p><ul><li><p><strong>Increasing Voltage:</strong></p><ul><li><strong>The Issue:</strong> This is not free though&mldr;doing this the power consumption goes up quadratically with voltage (P ≈ V²·f) and a lot more heat is dissipated because of which the system is more prone to reaching thermal limit making the chip burn off.</li></ul></li><li><p><strong>Make transistors smaller</strong></p><ul><li><strong>The Issue:</strong><ul><li><p>As transistors get smaller their power density stays constant this is what we call <strong>Dennard Scaling</strong>. So this meant that if we made transistors smaller we can supply lower voltage and the transistors would be cool and fast. This worked&mldr;until it didn’t.</p></li><li><p>Dennard Scaling ignored the existence of leakage current(transistors don&rsquo;t fully turn off which happens because of Quantum Tunneling) and threshold voltage(minimum voltage, aside from supplied, needed to keep transistor working), both of which scaled with time. So, the power density started going up and didn&rsquo;t remain constant.</p><div class="admonition info"><div class=admonition-header><svg viewBox="0 0 512 512"><path d="M256 512A256 256 0 10256 0a256 256 0 100 512zM216 336h24v-64h-24c-13.3.0-24-10.7-24-24s10.7-24 24-24h48c13.3.0 24 10.7 24 24v88h8c13.3.0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3.0-24-10.7-24-24s10.7-24 24-24zm40-208a32 32 0 110 64 32 32 0 110-64z"/></svg>
<span>Quantum Tunneling</span></div><div class=admonition-content><p>Effect when electrons pass through thin insulating barriers even when the device should be off.</p><h6></h6><p>If you wanna know why this happens this <a href="https://www.reddit.com/r/explainlikeimfive/comments/1kzaurr/comment/mv400rz/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button"><strong>short answer</strong></a> explains it quite well.</p></div></div></li><li><p>So did we stop? No, We still make transistors smaller But they no longer give free clock speed. Not to mention how costly it is to manufacture smaller transistors and Quantum tunneling in them.</p></li></ul></li></ul></li><li><p><strong>Reducing critical path</strong></p><ul><li><strong>The Issue:</strong> If it was that easy it wouldn&rsquo;t be an issue lmao. Such Chip layouts unfortunately are usually not optimal and even in todays chips propagation delays like these dominate even the gate delay inside transistors.</li></ul></li></ul><p>Hardware folks saw this doom coming way before 2005 and had already started cooking. The core idea was <em>what if we execute multiple instructions at the same time rather than trying to make the single instruction run fast?</em></p><h2 id=parallelism-on-single-thread>Parallelism on Single Thread<a hidden class=anchor aria-hidden=true href=#parallelism-on-single-thread>#</a></h2><p>Let&rsquo;s understand some ways of how parallelism was induces on hardware end to help felicitate concurrent instruction execution. For reference this is how our normie CPU looks like until now:</p><p><img alt="CPU Execution - Level 4" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Level%204.png></p><h3 id=instruction-level-parallelism>Instruction Level Parallelism<a hidden class=anchor aria-hidden=true href=#instruction-level-parallelism>#</a></h3><p>We understand that we want to execute multiple instructions at once, but what instructions can be execute in parallel? Let&rsquo;s take the example of the following program:</p><pre tabindex=0><code>Instruction Address     OPs     Operands
================================================
0000000100000f50        pushq   %rbp
0000000100000f51        movq    %rsp, %rbp
0000000100000f54        subq    $0x10, %rsp
0000000100000f58        movl    $0x0, -0x4(%rbp)
0000000100000f5f        movl    $0x5, -0x8(%rbp)
0000000100000f66        movl    $0x7, -0xc(%rbp)
</code></pre><p>We have six instructions here we can see the first three depend on each other i.e. 3rd instruction can&rsquo;t be executed until 2nd instructions is executed and 2nd instruction can&rsquo;t run until 1st instruction is executed. The last 3 instructions, however, are independant of each other, so these can be executed in parallel.</p><div class="admonition info"><div class=admonition-header><svg viewBox="0 0 512 512"><path d="M256 512A256 256 0 10256 0a256 256 0 100 512zM216 336h24v-64h-24c-13.3.0-24-10.7-24-24s10.7-24 24-24h48c13.3.0 24 10.7 24 24v88h8c13.3.0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3.0-24-10.7-24-24s10.7-24 24-24zm40-208a32 32 0 110 64 32 32 0 110-64z"/></svg>
<span>Why are last 3 parallel when they all work on %rbp?</span></div><div class=admonition-content><p>Well yes they all use <code>%rbp</code> as a base register but they&rsquo;re writing to <strong>different memory locations</strong> because <code>mov $a, b(%rbp)</code> in assembly <code>copy value "a" to the address (b + what’s in %rbp)</code>:</p><ul><li><code>movl $0x0, -0x4(%rbp)</code> writes 0 to offset -0x4 from %rbp</li><li><code>movl $0x5, -0x8(%rbp)</code> writes 5 to offset -0x8 from %rbp</li><li><code>movl $0x7, -0xc(%rbp)</code> writes 7 to offset -0xc from %rbp</li></ul><p>Since they write to different slots (different offsets from <code>%rbp</code>), there are no dependencies between them. They only read <code>%rbp</code>, so they can all execute simultaneously without conflicts.</p></div></div><p>So our instruction dependency/execution graph would look like this:</p><p><img alt="Instruction Dependancy" loading=lazy src=/posts/accelerators/imgs/Inst%20Dep%20Graph.png></p><p>So what a CPU does is rather than fetching and decoding 1 instruction at a time it&rsquo;ll have multiple instructions being fetched and decoded. Hardware wise this means we&rsquo;ll have multiple control units at play:</p><p><img alt=ILP loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20ILP.png></p><p>So now our CPU can fetch and decode 2 instructions simultaneously thanks to multiple control units. But this still would only execute one operation, let&rsquo;s say arithmetic, at a time because we only have 1 ALU. For this to work, the CPU must actually have the hardware that enable execution of multiple instructions in the same cycle. How do we do that? Add more ALUs!!!!!</p><p><img alt="Multiple ALU" loading=lazy src=/posts/accelerators/imgs/CPU%20Execution%20-%20Superscalar.png></p><p>Now our CPU can fetch-decode-execute 2 instructions at the same time!! This ability to make CPU execute multiple instruction at same time is called <strong>Instruction level parallelism</strong> and any dependancy that stalls this is called a <strong>Hazard</strong>. If it can execute 2 instructions simultaneously it&rsquo;s called 2-wide superscalar, if it can execute 3 instructions simultaneously it&rsquo;s called 3-wide superscalar and so on!</p><p>ILP is the reason why 2.4Ghz CPUs with ILP are faster than 3GHz CPUs with no ILP. You might think now we&rsquo;ve overcome the issue and we are in free lunch land again. Unfortunately, ILP gains plateaued around 2005 as well and Herb Sutter&rsquo;s article shows that in the graph as well. Honestly, ILP is fairly old as well so it wasn&rsquo;t like people waited for sequential execution to stagnate to implement ILP.</p><p>So, is single thread era done for? Well, kinda. Around early 2000, a major shift towards building concurrent systems started happening. People started utilizing thread concurrency in software more and more. But there is still one way that is still widely used to enable stuff like vectorization.</p><h3 id=simd-single-instruction-multiple-data>SIMD: Single Instruction, Multiple Data<a hidden class=anchor aria-hidden=true href=#simd-single-instruction-multiple-data>#</a></h3><p>Look at the following program:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> a[] <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>};
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> b[] <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> arr_size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(a) <span style=color:#f92672>/</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>int</span>);
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>int</span> c[arr_size];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span>(<span style=color:#66d9ef>int</span> i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> arr_size; i<span style=color:#f92672>++</span>)
</span></span><span style=display:flex><span>		c[i] <span style=color:#f92672>=</span> a[i] <span style=color:#f92672>+</span> b[i];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Above is an example of basic element-wise addition of two array. We are iterating over the array and adding them i.e. the add operation instruction for each element is fetched-decoded-executed. But is that the optimal way? If you think the ILP way, all the addition operations are independant of each other and the ILP graph would be:</p><p><img alt="Elementwise ILP" loading=lazy src=/posts/accelerators/imgs/Array%20Elementwise%20Add.png></p><p>If you look at the graph, you&rsquo;ll notice all the element wise operations in the for loop are independent of each other and do the same operation&mldr;addition, but since we are using a for loop this&rsquo;ll be done 1 at a time. Even with loop unrolling and ILP, we are still fundamentally executing one arithmetic operation per data element. The CPU may issue a few of them in parallel, but we are still limited by the quantity of compute units i.e. ALUs here. What we really want is to apply the same operation to many data elements inside the execution unit itself.</p><p>This is where SIMD comes in place which executes one operation on a batch of data rather than one by one! ILP exploits parallelism across instructions. SIMD exploits parallelism inside the data. SIMD exists because it&rsquo;s much better to apply one operation to many data elements than to apply the same operation many times to one element each. But how do we enable it say hardware wise?</p><p><img alt="SIMD Unit" loading=lazy src=/posts/accelerators/imgs/SIMD%20Unit.png></p><p>We have special SIMD units which when prompted execute one instruction that operates on multiple values at once and we use wider registers(XMM[128 bits], YMM[256 bits], ZMM[512 bits]), alongside the typical 64 bit ones, capable of storing multiple packed values instead on single ones! So if we have one such wide register of 256 bits it would be able to store: 8 values of size 32 bits, 4 values of size 64 bits, etc.</p><p>To invoke a SIMD unit operation you&rsquo;ll need special instructions that tell the SIMD units what to do. Different processor architectures have different SIMD instruction sets:</p><ul><li><strong>x86/x64 (Intel/AMD):</strong> SSE (Streaming SIMD Extensions), AVX (Advanced Vector Extensions), AVX2, AVX-512</li><li><strong>ARM:</strong> NEON, SVE (Scalable Vector Extension)</li><li><strong>RISC-V:</strong> V extension</li></ul><p>Let&rsquo;s see how our array addition example would work with SIMD:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;immintrin.h&gt; // AVX intrinsics</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;stdio.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>int</span> <span style=color:#a6e22e>main</span>() {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> a[] <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>};
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> b[] <span style=color:#f92672>=</span> {<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> arr_size <span style=color:#f92672>=</span> <span style=color:#66d9ef>sizeof</span>(a) <span style=color:#f92672>/</span> <span style=color:#66d9ef>sizeof</span>(<span style=color:#66d9ef>int</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>int</span> c[arr_size];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Load 8 integers (256 bits) at once into SIMD registers
</span></span></span><span style=display:flex><span>    __m256i vec_a <span style=color:#f92672>=</span> <span style=color:#a6e22e>_mm256_loadu_si256</span>((__m256i<span style=color:#f92672>*</span>)a);
</span></span><span style=display:flex><span>    __m256i vec_b <span style=color:#f92672>=</span> <span style=color:#a6e22e>_mm256_loadu_si256</span>((__m256i<span style=color:#f92672>*</span>)b);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Add all 8 pairs in one instruction
</span></span></span><span style=display:flex><span>    __m256i vec_c <span style=color:#f92672>=</span> <span style=color:#a6e22e>_mm256_add_epi32</span>(vec_a, vec_b);
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>// Store result back
</span></span></span><span style=display:flex><span>    <span style=color:#a6e22e>_mm256_storeu_si256</span>((__m256i<span style=color:#f92672>*</span>)c, vec_c);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>For now let&rsquo;s briefly skim what this program is doing, SIMD Programming can be a different blog on it&rsquo;s own later. In this SIMD program, instead of 8 separate addition instructions, we execute one instruction that adds 8 pairs of integers simultaneously. This is crazy efficient!!</p><ul><li><strong>Without SIMD:</strong> 8 iterations, 8 separate add instructions</li><li><strong>With SIMD (256-bit):</strong> 1 iteration, 1 vectorized add instruction operating on 8 values</li></ul><p>Of course is not all perfect, you can&rsquo;t using conditional without getting a performance hit due to Divergent Execution, lanes are limited, etc. So at this point we need to face the truth folks&mldr;Single Thread Free Lunch is done for. Good news is we got used to a new era of <strong>Parallel Computing on Multi Core Processor</strong>.</p><p>On hardware end, we scaled the number of threads a core can run and number of cores that are there. Software developers would also need to write concurrent programs that can utilize multiple cores effectively. This is where multi-threading, parallel algorithms, and concurrent programming models become essential. We&rsquo;ll</p><h2 id=multi-threading>Multi Threading<a hidden class=anchor aria-hidden=true href=#multi-threading>#</a></h2><h2 id=multi-processing>Multi Processing<a hidden class=anchor aria-hidden=true href=#multi-processing>#</a></h2><h2 id=the-modern-cpu>The Modern CPU<a hidden class=anchor aria-hidden=true href=#the-modern-cpu>#</a></h2></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://journal.herumbshandilya.com/posts/the-aya-project-trials/><span class=title>Next »</span><br><span>The Aya Project: My Experience and Learnings</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on x" href="https://x.com/intent/tweet/?text=Parallelism%20in%20CPUs%3a%20&amp;url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f&amp;title=Parallelism%20in%20CPUs%3a%20&amp;summary=Parallelism%20in%20CPUs%3a%20&amp;source=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f&title=Parallelism%20in%20CPUs%3a%20"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on whatsapp" href="https://api.whatsapp.com/send?text=Parallelism%20in%20CPUs%3a%20%20-%20https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on telegram" href="https://telegram.me/share/url?text=Parallelism%20in%20CPUs%3a%20&amp;url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Parallelism in CPUs:  on ycombinator" href="https://news.ycombinator.com/submitlink?t=Parallelism%20in%20CPUs%3a%20&u=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2faccelerators%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://journal.herumbshandilya.com/>Journal | Herumb Shandilya</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>