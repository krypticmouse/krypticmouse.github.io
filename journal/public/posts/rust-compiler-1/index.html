<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Writing a Compiler in Rust #1: Lexical Analysis | Journal | Herumb Shandilya</title><meta name=keywords content><meta name=description content="
Compilers are not a game of luck.
If you want it to work, code hard.
- Sora (No Game No Life)

Another blog, another butchered quote but what matters is that they never created a season 2 for this series and I&rsquo;m max pissed about it!! Anyways, as someone who has a Computer Science Degree, it&rsquo;s a bit shameful for me to admit that I never formally studied compilers, well in my defense it&rsquo;s because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I&rsquo;ve always enjoyed hearing a lot about."><meta name=author content="Herumb Shandilya"><link rel=canonical href=http://localhost:1313/posts/rust-compiler-1/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/rust-compiler-1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="http://localhost:1313/posts/rust-compiler-1/"><meta property="og:site_name" content="Journal | Herumb Shandilya"><meta property="og:title" content="Writing a Compiler in Rust #1: Lexical Analysis"><meta property="og:description" content=" Compilers are not a game of luck.
If you want it to work, code hard.
- Sora (No Game No Life)
Another blog, another butchered quote but what matters is that they never created a season 2 for this series and I‚Äôm max pissed about it!! Anyways, as someone who has a Computer Science Degree, it‚Äôs a bit shameful for me to admit that I never formally studied compilers, well in my defense it‚Äôs because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I‚Äôve always enjoyed hearing a lot about."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Writing a Compiler in Rust #1: Lexical Analysis"><meta name=twitter:description content="
Compilers are not a game of luck.
If you want it to work, code hard.
- Sora (No Game No Life)

Another blog, another butchered quote but what matters is that they never created a season 2 for this series and I&rsquo;m max pissed about it!! Anyways, as someone who has a Computer Science Degree, it&rsquo;s a bit shameful for me to admit that I never formally studied compilers, well in my defense it&rsquo;s because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I&rsquo;ve always enjoyed hearing a lot about."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Writing a Compiler in Rust #1: Lexical Analysis","item":"http://localhost:1313/posts/rust-compiler-1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Writing a Compiler in Rust #1: Lexical Analysis","name":"Writing a Compiler in Rust #1: Lexical Analysis","description":" Compilers are not a game of luck.\nIf you want it to work, code hard.\n- Sora (No Game No Life)\nAnother blog, another butchered quote but what matters is that they never created a season 2 for this series and I\u0026rsquo;m max pissed about it!! Anyways, as someone who has a Computer Science Degree, it\u0026rsquo;s a bit shameful for me to admit that I never formally studied compilers, well in my defense it\u0026rsquo;s because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I\u0026rsquo;ve always enjoyed hearing a lot about.\n","keywords":[],"articleBody":" Compilers are not a game of luck.\nIf you want it to work, code hard.\n- Sora (No Game No Life)\nAnother blog, another butchered quote but what matters is that they never created a season 2 for this series and I‚Äôm max pissed about it!! Anyways, as someone who has a Computer Science Degree, it‚Äôs a bit shameful for me to admit that I never formally studied compilers, well in my defense it‚Äôs because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I‚Äôve always enjoyed hearing a lot about.\nRecently this shame took over me and I decided to learn it again and I was surprised how simple and elegant this was!! Not just that I feel that LLMs and compilers are kinda same in terms of how they break down their end-to-end pipeline. But that‚Äôs enough rambling for now and let‚Äôs start understanding compilers!!!\nWTF is a Compiler? I‚Äôm gonna assume most of you have some experience in writing code in some programming language, if you don‚Äôt you should probably learn one first üòÖ. In programming what we do is that we write a syntax to define a logic. This logic could be as simple as adding two numbers or as complicated as writing the backend for a product.\nRegardless of the logic or program you are building, you are working around the same syntax for whatever programming language you choose. For the most part, most programming languages share the same semantics but differ in syntax. What that means is the syntax for defining a for loop might differ across different languages but the core meaning and use case of for loop remains the same.\nA Compiler helps the machine understand the semantics of the program from the given syntax. What it does is take the high-level source code that is written in a language like C, C++, Rust, etc., and convert it into low-level machine code that the computer processor can understand and execute. How does a compiler do this anyway?\nThe compiler takes the code you wrote as input, to a compiler your code is just a big string. So to make the machine understand it the compiler transforms it multiple times across multiple steps and converts it to a machine language like assembly that the computer can understand and execute. To elaborate more these steps are:\nLexical Analysis: The source code is broken down into tokens - things like keywords, identifiers, operators, separators, etc.\nParsing or Syntax Analysis: The tokens are analyzed to see if they follow the rules of the language. This ensures the code is syntactically correct.\nSemantic Analysis: The meaning of the code is analyzed to catch any logical errors. Things like type-checking happen here.\nIntermediate Code Generation: The code is converted into an intermediate representation, often in the form of assembly code or bytecode.\nCode Optimization: The intermediate code is optimized to improve efficiency - common optimizations include removing redundant or unused code.\nCode Generation: The optimized intermediate code is translated into the target machine code for the specific CPU architecture.\nIn this article, we‚Äôll be covering step one which is Lexical Analysis, and implement a very basic lexer. To implement this lexer I‚Äôll be choosing Rust as my language of choice because I‚Äôm trying to build my expertise in it so‚Ä¶sorry in advance.\nBut by the end of this blog, we‚Äôll know how to build our own lexer from scratch!! But wait, what‚Äôs a lexer? Let‚Äôs learn.\nWhat is Lexical Analysis? Lexical Analysis is the first step in almost all compilers, it involves taking your code as input and receiving a set of tokens that defines the meaning of each word in the code. These words are called lexemes and we‚Äôll dive more into what makes a lexeme, a lexeme.\nBut for now, we should understand that in the lexical analysis, we are taking the program as input and getting a token per lexeme as the output. All this process is done via a lexer which is basically just a set of code.\nLet‚Äôs take an example code to understand exactly what lexer processes and returns. Take the following Python code as example:\na = 10 if 10\u003c12 else 12 If we pass the above code as input to the lexer it‚Äôll return something like this:\n[IDENTIFIER, 'a'] [ASSIGN_OP, '='] [INTEGER, '10'] [IF] [INTEGER, '10'] [LESS_THAN] [INTEGER, '12'] [ELSE] [INTEGER, '12'] So the lexer looks at each lexeme (word/symbol) in the code and categorizes it into a token type. Identifiers or variables become IDENTIFIER tokens, integer literals become INT_LITERAL tokens, operators and delimiters get their token types, etc.\nThis tokenization is important because it standardizes the vocabulary of the code so the later compiler steps can reliably recognize constructs regardless of identifier names or literal values chosen by the programmer.\nThe lexer often ‚Äúignores‚Äù characters like whitespace and comments and just focuses on the necessary lexemes. To be more clear, the lexer knows the presence of whitespace but it decides to discard it. But in indentation-sensitive languages like Python, things can be different and whitespace might not be discarded.\nBut how do we do this? How do we take a set of lexemes and identify their tokens? Let‚Äôs learn.\nRegular Language The syntax of a language is represented via something called Regular Language. You can think of Regular Language as the rules of a programming language that defines the syntax of the language.\nFor example, in English we can make many words but not all those words are classified as language, so we know ‚Äúbook‚Äù is a valid word in the language but ‚Äúdsfdnf‚Äù is not even though both use alphabets. Same way in programming languages, we can write anything but not every program can be said to be a part of that language. Look at the code below:\nif a===b{ return a; } The above code can be valid for Javascript but not C++ because in JS === can be treated as EQUAL_TO token but in C++ it‚Äôll be EQUAL_TO ASSIGN token which will be an invalid operation.\nTo define these rules or Regular Language we use something called Regular Expressions or RegEx. NLP folks might‚Äôve had some experience with regex in one way or the other. But if you haven‚Äôt no worries, your boy has got you covered.\nRegular Expression A regular expression (or regex) is a way of describing a pattern of characters that can be used to match against a piece of text. They can be used to define the rules for a regular language, and then a compiler can use those rules to analyze a piece of source code and determine whether it is a valid instance of the language.\nThink like this the syntax for variable declaration, keywords, etc. All have a pattern for keywords you have a finite set of them that you need to match, for variables or identifiers you have a rule that the name of the variable must start with a letter which is followed by a letter or a digit.\nNow writing a Regex string has some rules those are:\nStart and End Anchors: ^ and $ are used to denote the beginning and end of a line, respectively. ^ matches the position before the first character in the string, and $ matches the position after the last character in the string.\nCharacter Classes: [ ] are used to match any one of the characters enclosed within the brackets. For example, [abc] will match any single ‚Äòa‚Äô, ‚Äòb‚Äô, or ‚Äòc‚Äô. Ranges can also be specified, like [a-z] for any lowercase letter and [0-9] for any digit.\nPredefined Character Classes: These are shortcuts for common character classes. For instance, \\d matches any digit (equivalent to [0-9]), \\w matches any word character (equivalent to [a-zA-Z0-9_]), and \\s matches any whitespace character (like space, tab, newline).\nNegated Character Classes: Placing a ^ inside square brackets negates the class. For example, [^abc] matches any character except ‚Äòa‚Äô, ‚Äòb‚Äô, or ‚Äòc‚Äô.\nQuantifiers: These specify how many instances of a character or group are needed for a match. * matches zero or more, + matches one or more, ? matches zero or one, and {n}, {n,}, {n,m} are used for specific quantities.\nThis syntax is usually the same for regex across any language, so this will apply to Rust, Python, or any other language. But theory can only take you so far, so let‚Äôs start implementing the lexer and building regex for every keyword, identifier, operation etc.\nImplementing the Lexer So it‚Äôs time to take a deep dive into Building the Lexer for our Compiler, I think we‚Äôll call our compiler BitterByte that sounds cool af!! Let‚Äôs call it that from now on!! So our goal is to compile the following code:\nint x = 5; int y = 6; int z = x + y; if (z \u003e 10) { print(\"Hello, world!\"); } else { print(\"Goodbye, world!\"); } Based on that we need a lexer that can capture: keywords (int, if, else), identifiers (x, y, z), numbers (5, 6, 10), operators (=, +, \u003e), punctuation (;, (, ), {, }), and strings (\"Hello, world!\", \"Goodbye, world!\").\nOhhh this is gonna be fun!! I know many of you won‚Äôt have experience in E\\Rust but if you do that‚Äôll be great but I‚Äôll be giving you some explanation on the code regardless. All the code can be found in the repository here. Let‚Äôs gooo!!\nInitializing our Project So we‚Äôll need to start creating our project directory with Rust stuff all set. For this, we‚Äôll be using cargo which is the package manager, and build system for Rust. Unlike Python, Rust is a compiled language which means you compile the Rust code first before you execute it.\nCargo takes of the compilation, execution, and package management in Rust. We can also use it to initialize our rust project too, kinda like create-next-app. Let‚Äôs initialize our project rust-compiler:\ncargo new rust-compiler cd rust-compiler Pretty simple right? In this directory, you‚Äôll find a file with a weird extension and a src directory which is a pretty big deal. Overall the initial repo structure would look like this:\n. ‚îú‚îÄ‚îÄ Cargo.toml ‚îú‚îÄ‚îÄ src ‚îÇ ‚îî‚îÄ‚îÄ main.rs What are these files and directories you wonder? Let me explain:\nCargo.toml: This is the configuration file for your project. It includes metadata about your project like the name, version, authors, dependencies, and more. It‚Äôs kinda like package.json React.js or pyproject.toml in Python Poetry. If you need to include a package simply drop it here!\nsrc/main.rs: This is the default entry point of your Rust program. When you run or build your Rust code, Cargo looks for this file to compile your project as a binary.\nWhile this initial structure gives us a good starting point to start our project, we‚Äôll be elegant and populate it with more files in src directory to organize it. And trust me when I say it‚Ä¶it‚Äôs hard to be elegant in Rust without losing your mind.\nUnderstanding Repo Structure Now that we have an initial repo structure we can go ahead and populate it with files we‚Äôll be working with. In this tutorial, we are building a lexer so we‚Äôll create a subfolder in src by the name lexing:\ncd src mkdir lexing In lexing we‚Äôll add 3 files: token.rs, lexer.rs and mod.rs. We‚Äôll understand what each of these files will contain but for now, this is what your repo structure should look like:\n. ‚îú‚îÄ‚îÄ Cargo.toml ‚îú‚îÄ‚îÄ src ‚îÇ ‚îú‚îÄ‚îÄ lexing ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ lexer.rs ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ mod.rs ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ token.rs ‚îÇ ‚îî‚îÄ‚îÄ main.rs Before diving into writing code we‚Äôll need to let Cargo know that we‚Äôre using regex library crate too!! To do that we edit the Cargo.toml and add regex library crate in it:\n[package] name = \"rust-compiler\" version = \"0.1.0\" edition = \"2021\" # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html [dependencies] regex=\"1\" # Added this The ‚Äú1‚Äù tells cargo to use the latest package of regex but you can be more specific and write the exact version of the package too. I‚Äôm lazy so I won‚Äôt do that, deal with it.\nNow that we have our files populated it‚Äôs time to fill them up with code!!\nWriting token.rs So, let‚Äôs start by implementing the tokens for our lexer, for this, we‚Äôll be using enums in Rust which are best suited for this task. We‚Äôll be creating an enum Token which will hold the token type for lexemes:\n#[derive(Debug)] pub enum Token{ // Keywords Print(String), If(String), Else(String), Int(String), // Literals IntegerLiteral(i32), StringLiteral(String), // Identifiers Identifier(String), // Operators Plus(String), Assign(String), // Punctuation Semicolon(String), LeftParen(String), RightParen(String), LeftBrace(String), RightBrace(String), // Logical Operators GreaterThan(String), LessThan(String), } We‚Äôve added the type for every possible token which seems to be fairly limited, in reality, this list can be much bigger! As you might have noticed most elements are of type String, this might not be the most optimal way to do this or something people call Idiomatic Rust.\nBut I used it to keep things simple and sane without needing to dwell on the issues of Generic Lifetimes. Trust me I‚Äôm not ready to explain and if you are new to Rust then there is a good chance you aren‚Äôt ready to understand either.\nSo yeah, there goes that!! This #[derive(Debug)] is not a comment, in Rust comments start with //. Rather it is an attribute that implements a trait called Debug which basically lets us print the enum elements println! without any extra effort. I know man, Rust is weird af. Moving on, we need to implement 2 functions:\nGiven the token_type string and a value, return the token object. For example, given token_type string \"IntegerLiteral\" and value 300 it‚Äôll return us Token::IntegerLiteral(300) where :: is used to specify a particular variant of an enum which in this case is IntegerLiteral.\nGiven the token_type string, return the regex to match the lexeme for that token type.\nLike we implement class methods in Python we can do something similar in Rust by using impl. So the above two functions can be implemented inside impl for Token, kinda it‚Äôs their class method iykwim.\nimpl Token { pub fn get_token(token_type: \u0026str, value: Option\u003c\u0026str\u003e) -\u003e Token { match token_type { \"Print\" =\u003e Token::Print(\"print\".to_string()), \"If\" =\u003e Token::If(\"if\".to_string()), \"Else\" =\u003e Token::Else(\"else\".to_string()), \"Int\" =\u003e Token::Int(\"int\".to_string()), \"IntegerLiteral\" =\u003e Token::IntegerLiteral(value.unwrap().parse::().unwrap()), \"StringLiteral\" =\u003e Token::StringLiteral(value.unwrap().to_string()), \"Identifier\" =\u003e Token::Identifier(value.unwrap().to_string()), \"Plus\" =\u003e Token::Plus(\"+\".to_string()), \"Assign\" =\u003e Token::Assign(\"=\".to_string()), \"Semicolon\" =\u003e Token::Semicolon(\";\".to_string()), \"LeftParen\" =\u003e Token::LeftParen(\"(\".to_string()), \"RightParen\" =\u003e Token::RightParen(\")\".to_string()), \"LeftBrace\" =\u003e Token::LeftBrace(\"{\".to_string()), \"RightBrace\" =\u003e Token::RightBrace(\"}\".to_string()), \"GreaterThan\" =\u003e Token::GreaterThan(\"\u003e\".to_string()), \"LessThan\" =\u003e Token::LessThan(\"\u003c\".to_string()), _ =\u003e panic!(\"Invalid token type: {}\", token_type), } } pub fn get_token_regex(token_type: \u0026str) -\u003e String { match token_type { \"Print\" =\u003e r\"print\", \"If\" =\u003e r\"if\", \"Else\" =\u003e r\"else\", \"Int\" =\u003e r\"int\\s+\", \"IntegerLiteral\" =\u003e r\"\\d+\", \"StringLiteral\" =\u003e r#\"\\\".*\\\"\"#, \"Identifier\" =\u003e r\"[a-zA-Z_][a-zA-Z0-9_]* =\", \"Plus\" =\u003e r\"\\+\", \"Assign\" =\u003e r\"=\", \"Semicolon\" =\u003e r\";\", \"LeftParen\" =\u003e r\"\\(\", \"RightParen\" =\u003e r\"\\)\", \"LeftBrace\" =\u003e r\"\\{\", \"RightBrace\" =\u003e r\"}\", \"GreaterThan\" =\u003e r\"\u003e\", \"LessThan\" =\u003e r\"\u003c\", _ =\u003e panic!(\"Invalid token type: {}\", token_type), }.to_string() } } As you can see we are basically doing a basic string matching here and returning the appropriate result based on whatever matches. match in Rust is pretty much similar to switch-case statements in C++ and Java.\nAnother thing you‚Äôll notice is \u0026str and String being used, well they are different in Rust and to convert a \u0026str to String we used to_string. For the sake of simplicity, you can think of \u0026str as an immutable string and String as a mutable string. I regret choosing Rust so bad!!\nLastly, you also see that the function panics if it doesn‚Äôt match any regex this is what we‚Äôll call a Syntax Error. This is something we should never let happen, aside from keywords that if can be mistaken for an identifier too!! But since keywords are before identifier in the match statement or as people say in higher priority this issue would be avoided.\nBut what if we have an overlap b/w two 2 tokens, for example, if we have int ifer = 10 here if is a keyword now but ifer is actually an identifier? In cases like this, we choose the regex which is bigger in size, this rule is called maximal munch. Which we‚Äôll implement as well in lexer.rs.\nNow that we have the functions it‚Äôs time to address the elephant in the room, ‚ÄúWTH are those weird strings in get_token_regex method???‚Äù. Well, they are regex!!\nI know I know it seems like an alien language but let‚Äôs learn more about it and how we built it!!\nUnderstanding Regex Regexes are essentially a set of rules or patterns defined by a string of characters, which can be used to search for specific text within a larger body of text, replace certain text patterns, or extract portions of a text that match a given pattern.\nLet‚Äôs understand with an example of an email validator. Email addresses typically follow a standard format: they start with a series of characters that can include letters, numbers, dots, underscores, or hyphens, followed by the @ symbol, then more characters including letters and possibly dots, hyphens, or underscores, and finally a domain extension like .com, .org, etc.\nNow you can either write a 100-line code that validates this by manually verifying every case possible like @ should be present, the part before @ should start with a letter, should contain letters or digits and not any invalid character, etc. An easy way would be to use regex like [a-zA-Z][a-zA-Z0-9]*@[a-zA-Z0-9]+.[com|org|net].\nThat string might look really weird right now so let‚Äôs break that down a bit:\n[a-zA-Z]: This bit is like saying, ‚ÄúStart by finding a letter.‚Äù It doesn‚Äôt matter if it‚Äôs uppercase (A-Z) or lowercase (a-z), any letter will do. But, it‚Äôs important that it‚Äôs a letter, not a number or a symbol.\n[a-zA-Z0-9]*: After finding that first letter, this part says ‚ÄúNow, keep going and find any mix of letters (both upper or lowercase) and numbers (0-9).‚Äù The asterisk * at the end is a special ‚Äúmetacharacter‚Äù, allowing for 0 or more of these characters.\n@: Next, we must find the @ symbol. This is necessary for email addresses as it separates the user‚Äôs name from their email domain.\n[a-zA-Z0-9]+: After the @ symbol, we‚Äôre looking for a string of letters and numbers again. This represents the domain name (like ‚Äògmail‚Äô in ‚Äòuser@gmail.com‚Äô). + at the end is another special ‚Äúmetacharacter‚Äù, allowing for 1 or more of these characters.\n.: Then comes a literal dot. In email addresses, this dot separates the domain name from the domain extension.\n(com|org|net): Finally, we‚Äôre specifying that the email address must end with either ‚Äò.com‚Äô, ‚Äò.org‚Äô, or ‚Äò.net‚Äô. The parentheses group ‚Äòcom‚Äô, ‚Äòorg‚Äô, and ‚Äônet‚Äô together as a single unit, and the | symbol acts as an ‚Äúor.‚Äù So, this part says, ‚ÄúFind either ‚Äò.com‚Äô, ‚Äò.org‚Äô, or ‚Äò.net‚Äô right after the dot.‚Äù\nYou see how we simplified the whole thing to a regex string. Regex can help you validate a pattern but it can also help you find it!! If we pass if to regex it can return us a vector of starting and ending indexes of every occurrence of if in that string.\nWriting Regex for Tokens Based on whatever we learned now we can understand why we wrote the regex of each token the way we did. Let‚Äôs see:\nKeywords and Operators\nr\"print\", r\"if\", r\"else\", r\"int\\s+\": These are straightforward. They match the specific keyword strings. For int, \\s+ is added to ensure there‚Äôs at least one whitespace character after the keyword, differentiating it from identifiers that might start with ‚Äúint‚Äù (like integerValue). Literals\nr\"\\d+\": This pattern matches integer literals. \\d represents any digit, and + means one or more of the preceding elements, so this matches a sequence of digits.\nr#\"\\\".*\\\"\": This pattern matches string literals. The .* matches any character (except for line terminators) any number of times, and \\\" matches quotation marks.\nIdentifiers\nr\"[a-zA-Z_][a-zA-Z0-9_]* =\": This matches identifiers. Identifiers start with a letter or underscore, followed by any number of letters, digits, or underscores. The space and = at the end ensure we‚Äôre matching an assignment operation. Operators and Punctuation\nr\"\\+\", r\"=\", r\";\", r\"\\(\", r\"\\)\", r\"\\{\", r\"}\", r\"\u003e\", r\"\u003c\": These patterns match operators and punctuation characters. In regex, some characters like +, (, ), {, } are special characters. To match them literally, they are escaped with a backslash \\. Boy was that a lot of information to consume!! But we‚Äôre past the hard stuff now, or not given how much you like rust lol. But let‚Äôs bring it all together and write the logic for our lexer in lexer.rs.\nWriting lexer.rs Now that we have implemented the tools to discover the tokens it‚Äôs time to use them and find the token. But how do we even use them to identify the token? The brute forced way is to:\nFind the first regex match for each token regex.\nFind the matches that have the smallest starting index.\nIf many matches have the same smallest starting index then choose the match that is largest in length or has the maximum ending_index-starting_index. Remove the lexeme of the token that we selected from the program via regex.\nRepeat the Process.\nThis is good but there is too much redundancy in nested looping going on here!! I think instead what we can do is get all the regex matches at once and process them together, this way has many issues but is perfect for the program we want to compile. Here is the process:\nIterate over an array of token string tokens in the descending order of their priority. That means indices of keyword token_strings would be lower than others. Aside from this initialize a current_input that holds the same value as the program and match_vec that‚Äôs a vector of tuples with elements (lexeme, starting_index, ending_index).\nlet current_input = program; let tokens = [ \"Print\", // Highest Priority \"If\", \"Else\", \"Int\", \"Plus\", \"Assign\", \"Semicolon\", \"LeftParen\", \"RightParen\", \"LeftBrace\", \"RightBrace\", \"GreaterThan\", \"LessThan\", \"IntegerLiteral\", \"StringLiteral\", \"Identifier\", // Lowest Priority ]; let mut match_vec: Vec\u003c(\u0026str, usize, usize)\u003e = Vec::new(); Start iteration over each token in tokens array:\nfor token in tokens.iter() { Get the regex pattern string token_regex for the token token string using the get_regex method implemented under Token and initialize the Regex object re for the regex pattern we got.\nlet token_regex = Token::get_token_regex(token); let re = Regex::new(token_regex.as_str()).unwrap(); Use re to find all matches in current_input. This is done using the find_iter method, which returns an iterator over all non-overlapping matches of the pattern in the string. Each match found is a range indicating the match‚Äôs starting index and ending index in the input string.\nlet matched = re.find_iter(current_input); For each match found, create a tuple consisting of the token string, the starting index, and the ending index of the match in current_input. Append each of these tuples to match_vec. This step gathers all the matches for all token types in the input program. If there are no matches found for a token string skip the further steps in loop.\nlet all_matches = matched.collect::","wordCount":"4378","inLanguage":"en","datePublished":"2024-02-26T00:00:00Z","dateModified":"2024-02-26T00:00:00Z","author":{"@type":"Person","name":"Herumb Shandilya"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/rust-compiler-1/"},"publisher":{"@type":"Organization","name":"Journal | Herumb Shandilya","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Herumb's Journal (Alt + H)">Herumb's Journal</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://the-ir-book.herumbshandilya.com/ title="The Small Book of Information Retrieval"><span>The Small Book of Information Retrieval</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://www.herumbshandilya.com/ title=Portfolio><span>Portfolio</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Writing a Compiler in Rust #1: Lexical Analysis</h1><div class=post-meta><span title='2024-02-26 00:00:00 +0000 UTC'>February 26, 2024</span>&nbsp;¬∑&nbsp;<span>21 min</span>&nbsp;¬∑&nbsp;<span>4378 words</span>&nbsp;¬∑&nbsp;<span>Herumb Shandilya</span></div></header><div class=post-content><blockquote><p><em>Compilers are not a game of luck.</em></p><p><em>If you want it to work, code hard.</em></p><p><strong>- Sora (No Game No Life)</strong></p></blockquote><p>Another blog, another butchered quote but what matters is that they never created a season 2 for this series and I&rsquo;m max pissed about it!! Anyways, as someone who has a Computer Science Degree, it&rsquo;s a bit shameful for me to admit that I never formally studied compilers, well in my defense it&rsquo;s because it was an elective and I chose an ML elective instead of Compiler Design. Although it is something that I&rsquo;ve always enjoyed hearing a lot about.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706220084249/579080b9-c333-492f-acb1-1b33e10462a4.gif></p><p>Recently this shame took over me and I decided to learn it again and I was surprised how simple and elegant this was!! Not just that I feel that LLMs and compilers are kinda same in terms of how they break down their end-to-end pipeline. But that&rsquo;s enough rambling for now and let&rsquo;s start understanding compilers!!!</p><h2 id=wtf-is-a-compiler>WTF is a Compiler?<a hidden class=anchor aria-hidden=true href=#wtf-is-a-compiler>#</a></h2><p>I&rsquo;m gonna assume most of you have some experience in writing code in some programming language, if you don&rsquo;t you should probably learn one first <strong>üòÖ</strong>. In programming what we do is that we write a <strong>syntax to define a logic</strong>. This logic could be as simple as adding two numbers or as complicated as writing the backend for a product.</p><p>Regardless of the logic or program you are building, you are working around the same syntax for whatever programming language you choose. For the most part, most programming languages share the <strong>same semantics but differ in syntax</strong>. What that means is the syntax for defining a <em>for loop</em> might differ across different languages but the core meaning and use case of <em>for loop</em> remains the same.</p><p>A Compiler helps the machine understand the semantics of the program from the given syntax. What it does is take the high-level source code that is written in a language like C, C++, Rust, etc., and convert it into low-level machine code that the computer processor can understand and execute. How does a compiler do this anyway?</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706216644697/2a6ae6c4-8a65-45e5-a85c-a65e24d0d0e7.png></p><p>The compiler takes the code you wrote as input, to a compiler your code is just a big string. So to make the machine understand it the compiler transforms it multiple times across multiple steps and converts it to a machine language like assembly that the computer can understand and execute. To elaborate more these steps are:</p><ul><li><p><strong>Lexical Analysis:</strong> The source code is broken down into tokens - things like keywords, identifiers, operators, separators, etc.</p></li><li><p><strong>Parsing or Syntax Analysis:</strong> The tokens are analyzed to see if they follow the rules of the language. This ensures the code is syntactically correct.</p></li><li><p><strong>Semantic Analysis:</strong> The meaning of the code is analyzed to catch any logical errors. Things like type-checking happen here.</p></li><li><p><strong>Intermediate Code Generation:</strong> The code is converted into an intermediate representation, often in the form of assembly code or bytecode.</p></li><li><p><strong>Code Optimization:</strong> The intermediate code is optimized to improve efficiency - common optimizations include removing redundant or unused code.</p></li><li><p><strong>Code Generation:</strong> The optimized intermediate code is translated into the target machine code for the specific CPU architecture.</p></li></ul><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706217098166/ff2a5d45-97cf-4c52-88fc-859fe3a8872b.png></p><p>In this article, we&rsquo;ll be covering step one which is <strong>Lexical Analysis,</strong> and implement a very basic lexer. To implement this lexer I&rsquo;ll be choosing Rust as my language of choice because I&rsquo;m trying to build my expertise in it so&mldr;sorry in advance.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706212688903/580eb300-af39-4c83-b3ae-7e0708df3e64.png></p><p>But by the end of this blog, we&rsquo;ll know how to build our own lexer from scratch!! But wait, what&rsquo;s a lexer? Let&rsquo;s learn.</p><h2 id=what-is-lexical-analysis>What is Lexical Analysis?<a hidden class=anchor aria-hidden=true href=#what-is-lexical-analysis>#</a></h2><p><em>Lexical Analysis</em> is the first step in almost all compilers, it involves taking your code as input and receiving a set of tokens that defines the meaning of each word in the code. These words are called <strong>lexemes</strong> and we&rsquo;ll dive more into what makes a lexeme, a lexeme.</p><p>But for now, we should understand that in the lexical analysis, we are taking the program as input and getting a token per lexeme as the output. All this process is done via a lexer which is basically just a set of code.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706292949508/d3d014a6-84d3-4129-ab67-cf521917ab87.png></p><p>Let&rsquo;s take an example code to understand exactly what lexer processes and returns. Take the following Python code as example:</p><pre tabindex=0><code>a = 10 if 10&lt;12 else 12
</code></pre><p>If we pass the above code as input to the lexer it&rsquo;ll return something like this:</p><pre tabindex=0><code>[IDENTIFIER, &#39;a&#39;]  [ASSIGN_OP, &#39;=&#39;]  [INTEGER, &#39;10&#39;]  [IF]  [INTEGER, &#39;10&#39;]  [LESS_THAN]  [INTEGER, &#39;12&#39;]  [ELSE]  [INTEGER, &#39;12&#39;]
</code></pre><p>So the lexer looks at each lexeme (word/symbol) in the code and categorizes it into a token type. Identifiers or variables become <code>IDENTIFIER</code> tokens, integer literals become <code>INT_LITERAL</code> tokens, operators and delimiters get their token types, etc.</p><p>This tokenization is important because it standardizes the vocabulary of the code so the later compiler steps can reliably recognize constructs regardless of identifier names or literal values chosen by the programmer.</p><p>The lexer often &ldquo;ignores&rdquo; characters like whitespace and comments and just focuses on the necessary lexemes. To be more clear, the lexer knows the presence of whitespace but it decides to discard it. But in indentation-sensitive languages like Python, things can be different and whitespace might not be discarded.</p><p>But how do we do this? How do we take a set of lexemes and identify their tokens? Let&rsquo;s learn.</p><h2 id=regular-language>Regular Language<a hidden class=anchor aria-hidden=true href=#regular-language>#</a></h2><p>The syntax of a language is represented via something called Regular Language. You can think of Regular Language as the rules of a programming language that defines the syntax of the language.</p><p>For example, in English we can make many words but not all those words are classified as <strong>language</strong>, so we know &ldquo;book&rdquo; is a valid word in the language but &ldquo;dsfdnf&rdquo; is not even though both use alphabets. Same way in programming languages, we can write anything but not every program can be said to be a part of that language. Look at the code below:</p><pre tabindex=0><code>if a===b{
    return a;
}
</code></pre><p>The above code can be valid for Javascript but not C++ because in JS <code>===</code> can be treated as <code>EQUAL_TO</code> token but in C++ it&rsquo;ll be <code>EQUAL_TO ASSIGN</code> token which will be an invalid operation.</p><p>To define these rules or Regular Language we use something called Regular Expressions or RegEx. NLP folks might&rsquo;ve had some experience with regex in one way or the other. But if you haven&rsquo;t no worries, your boy has got you covered.</p><h2 id=regular-expression>Regular Expression<a hidden class=anchor aria-hidden=true href=#regular-expression>#</a></h2><p>A regular expression (or regex) is a way of describing a pattern of characters that can be used to match against a piece of text. They can be used to define the rules for a regular language, and then a compiler can use those rules to analyze a piece of source code and determine whether it is a valid instance of the language.</p><p>Think like this the syntax for variable declaration, keywords, etc. All have a pattern for keywords you have a finite set of them that you need to match, for variables or identifiers you have a rule that the name of the variable must start with a letter which is followed by a letter or a digit.</p><p>Now writing a Regex string has some rules those are:</p><ul><li><p><strong>Start and End Anchors</strong>: <code>^</code> and <code>$</code> are used to denote the beginning and end of a line, respectively. <code>^</code> matches the position before the first character in the string, and <code>$</code> matches the position after the last character in the string.</p></li><li><p><strong>Character Classes</strong>: <code>[ ]</code> are used to match any one of the characters enclosed within the brackets. For example, <code>[abc]</code> will match any single &lsquo;a&rsquo;, &lsquo;b&rsquo;, or &lsquo;c&rsquo;. Ranges can also be specified, like <code>[a-z]</code> for any lowercase letter and [0-9] for any digit.</p></li><li><p><strong>Predefined Character Classes</strong>: These are shortcuts for common character classes. For instance, <code>\d</code> matches any digit (equivalent to <code>[0-9]</code>), <code>\w</code> matches any word character (equivalent to <code>[a-zA-Z0-9_]</code>), and <code>\s</code> matches any whitespace character (like space, tab, newline).</p></li><li><p><strong>Negated Character Classes</strong>: Placing a <code>^</code> inside square brackets negates the class. For example, <code>[^abc]</code> matches any character except &lsquo;a&rsquo;, &lsquo;b&rsquo;, or &lsquo;c&rsquo;.</p></li><li><p><strong>Quantifiers</strong>: These specify how many instances of a character or group are needed for a match. <code>*</code> matches zero or more, <code>+</code> matches one or more, <code>?</code> matches zero or one, and <code>{n}</code>, <code>{n,}</code>, <code>{n,m}</code> are used for specific quantities.</p></li></ul><p>This syntax is usually the same for regex across any language, so this will apply to Rust, Python, or any other language. But theory can only take you so far, so let&rsquo;s start implementing the lexer and building regex for every keyword, identifier, operation etc.</p><h2 id=implementing-the-lexer>Implementing the Lexer<a hidden class=anchor aria-hidden=true href=#implementing-the-lexer>#</a></h2><p>So it&rsquo;s time to take a deep dive into Building the Lexer for our Compiler, I think we&rsquo;ll call our compiler <strong>BitterByte</strong> that sounds cool af!! Let&rsquo;s call it that from now on!! So our goal is to compile the following code:</p><pre tabindex=0><code>int x = 5;
int y = 6;
int z = x + y;

if (z &gt; 10) {
    print(&#34;Hello, world!&#34;);
} 

else {
    print(&#34;Goodbye, world!&#34;);
}
</code></pre><p>Based on that we need a lexer that can capture: keywords (<code>int</code>, <code>if</code>, <code>else</code>), identifiers (<code>x</code>, <code>y</code>, <code>z</code>), numbers (<code>5</code>, <code>6</code>, <code>10</code>), operators (<code>=</code>, <code>+</code>, <code>></code>), punctuation (<code>;</code>, <code>(</code>, <code>)</code>, <code>{</code>, <code>}</code>), and strings (<code>"Hello, world!"</code>, <code>"Goodbye, world!"</code>).</p><p>Ohhh this is gonna be fun!! I know many of you won&rsquo;t have experience in E\Rust but if you do that&rsquo;ll be great but I&rsquo;ll be giving you some explanation on the code regardless. All the code can be found in the repository <a href=https://github.com/krypticmouse/rust-compiler><strong>here</strong></a>. Let&rsquo;s gooo!!</p><h3 id=initializing-our-project>Initializing our Project<a hidden class=anchor aria-hidden=true href=#initializing-our-project>#</a></h3><p>So we&rsquo;ll need to start creating our project directory with Rust stuff all set. For this, we&rsquo;ll be using cargo which is the package manager, and build system for Rust. Unlike Python, Rust is a compiled language which means you compile the Rust code first before you execute it.</p><p>Cargo takes of the compilation, execution, and package management in Rust. We can also use it to initialize our rust project too, kinda like <code>create-next-app</code>. Let&rsquo;s initialize our project <code>rust-compiler</code>:</p><pre tabindex=0><code>cargo new rust-compiler
cd rust-compiler
</code></pre><p>Pretty simple right? In this directory, you&rsquo;ll find a file with a weird extension and a <code>src</code> directory which is a pretty big deal. Overall the initial repo structure would look like this:</p><pre tabindex=0><code>.
‚îú‚îÄ‚îÄ Cargo.toml
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ main.rs
</code></pre><p>What are these files and directories you wonder? Let me explain:</p><ol><li><p><strong>Cargo.toml</strong>: This is the configuration file for your project. It includes metadata about your project like the name, version, authors, dependencies, and more. It&rsquo;s kinda like <code>package.json</code> React.js or <code>pyproject.toml</code> in Python Poetry. If you need to include a package simply drop it here!</p></li><li><p><strong>src/</strong><a href=http://main.rs><strong>main.rs</strong></a>: This is the default entry point of your Rust program. When you run or build your Rust code, Cargo looks for this file to compile your project as a binary.</p></li></ol><p>While this initial structure gives us a good starting point to start our project, we&rsquo;ll be elegant and populate it with more files in <code>src</code> directory to organize it. And trust me when I say it&mldr;it&rsquo;s hard to be elegant in Rust without losing your mind.</p><p><img alt="Good for you Crab : r/ProgrammerHumor" loading=lazy src="https://preview.redd.it/good-for-you-crab-v0-5v9ygeh9r1c91.jpg?width=640&crop=smart&auto=webp&s=13e79c30adb04181782c462f07a95bc343d73eaf"></p><h3 id=understanding-repo-structure>Understanding Repo Structure<a hidden class=anchor aria-hidden=true href=#understanding-repo-structure>#</a></h3><p>Now that we have an initial repo structure we can go ahead and populate it with files we&rsquo;ll be working with. In this tutorial, we are building a lexer so we&rsquo;ll create a subfolder in <code>src</code> by the name <code>lexing</code>:</p><pre tabindex=0><code>cd src
mkdir lexing
</code></pre><p>In <code>lexing</code> we&rsquo;ll add 3 files: <code>token.rs</code>, <code>lexer.rs</code> and <code>mod.rs</code>. We&rsquo;ll understand what each of these files will contain but for now, this is what your repo structure should look like:</p><pre tabindex=0><code>.
‚îú‚îÄ‚îÄ Cargo.toml
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ lexing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lexer.rs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mod.rs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ token.rs
‚îÇ   ‚îî‚îÄ‚îÄ main.rs
</code></pre><p>Before diving into writing code we&rsquo;ll need to let Cargo know that we&rsquo;re using <strong>regex</strong> library crate too!! To do that we edit the <code>Cargo.toml</code> and add regex library crate in it:</p><pre tabindex=0><code>[package]
name = &#34;rust-compiler&#34;
version = &#34;0.1.0&#34;
edition = &#34;2021&#34;

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
regex=&#34;1&#34; # Added this
</code></pre><p>The &ldquo;1&rdquo; tells cargo to use the latest package of <code>regex</code> but you can be more specific and write the exact version of the package too. I&rsquo;m lazy so I won&rsquo;t do that, deal with it.</p><p><img alt="Deal with it - Meme by arthurkaly :) Memedroid" loading=lazy src=https://images7.memedroid.com/images/UPLOADED8/545fdf490d891.jpeg></p><p>Now that we have our files populated it&rsquo;s time to fill them up with code!!</p><h3 id=writing-tokenrs>Writing token.rs<a hidden class=anchor aria-hidden=true href=#writing-tokenrs>#</a></h3><p>So, let&rsquo;s start by implementing the tokens for our lexer, for this, we&rsquo;ll be using enums in Rust which are best suited for this task. We&rsquo;ll be creating an enum <code>Token</code> which will hold the token type for lexemes:</p><pre tabindex=0><code>#[derive(Debug)]
pub enum Token{
    // Keywords
    Print(String),
    If(String),
    Else(String),
    Int(String),

    // Literals
    IntegerLiteral(i32),
    StringLiteral(String),

    // Identifiers
    Identifier(String),

    // Operators
    Plus(String),
    Assign(String),

    // Punctuation
    Semicolon(String),
    LeftParen(String),
    RightParen(String),
    LeftBrace(String),
    RightBrace(String),

    // Logical Operators
    GreaterThan(String),
    LessThan(String),
}
</code></pre><p>We&rsquo;ve added the type for every possible token which seems to be fairly limited, in reality, this list can be much bigger! As you might have noticed most elements are of type <code>String</code>, this might not be the most optimal way to do this or something people call <strong>Idiomatic Rust</strong>.</p><p>But I used it to keep things simple and sane without needing to dwell on the issues of Generic Lifetimes. Trust me I&rsquo;m not ready to explain and if you are new to Rust then there is a good chance you aren&rsquo;t ready to understand either.</p><p><img loading=lazy src=https://i.ytimg.com/vi/oD6U_2s0qdY/maxresdefault.jpg></p><p>So yeah, there goes that!! This <code>#[derive(Debug)]</code> is not a comment, in Rust comments start with <code>//</code>. Rather it is an attribute that implements a trait called <code>Debug</code> which basically lets us print the enum elements <code>println!</code> without any extra effort. I know man, Rust is weird af. Moving on, we need to implement 2 functions:</p><ul><li><p>Given the <strong>token_type</strong> <strong>string</strong> and a <strong>value</strong>, return the token object. For example, given <strong>token_type string</strong> <code>"IntegerLiteral"</code> and <strong>value</strong> <code>300</code> it&rsquo;ll return us <code>Token::IntegerLiteral(300)</code> where <code>::</code> is used to specify a particular variant of an enum which in this case is <code>IntegerLiteral</code>.</p></li><li><p>Given the <strong>token_type</strong> string, return the regex to match the lexeme for that token type.</p></li></ul><p>Like we implement class methods in Python we can do something similar in Rust by using <code>impl</code>. So the above two functions can be implemented inside <code>impl</code> for <code>Token</code>, kinda it&rsquo;s their class method iykwim.</p><pre tabindex=0><code>impl Token {
    pub fn get_token(token_type: &amp;str, value: Option&lt;&amp;str&gt;) -&gt; Token {
        match token_type {
            &#34;Print&#34; =&gt; Token::Print(&#34;print&#34;.to_string()),
            &#34;If&#34; =&gt; Token::If(&#34;if&#34;.to_string()),
            &#34;Else&#34; =&gt; Token::Else(&#34;else&#34;.to_string()),
            &#34;Int&#34; =&gt; Token::Int(&#34;int&#34;.to_string()),
            &#34;IntegerLiteral&#34; =&gt; Token::IntegerLiteral(value.unwrap().parse::&lt;i32&gt;().unwrap()),
            &#34;StringLiteral&#34; =&gt; Token::StringLiteral(value.unwrap().to_string()),
            &#34;Identifier&#34; =&gt; Token::Identifier(value.unwrap().to_string()),
            &#34;Plus&#34; =&gt; Token::Plus(&#34;+&#34;.to_string()),
            &#34;Assign&#34; =&gt; Token::Assign(&#34;=&#34;.to_string()),
            &#34;Semicolon&#34; =&gt; Token::Semicolon(&#34;;&#34;.to_string()),
            &#34;LeftParen&#34; =&gt; Token::LeftParen(&#34;(&#34;.to_string()),
            &#34;RightParen&#34; =&gt; Token::RightParen(&#34;)&#34;.to_string()),
            &#34;LeftBrace&#34; =&gt; Token::LeftBrace(&#34;{&#34;.to_string()),
            &#34;RightBrace&#34; =&gt; Token::RightBrace(&#34;}&#34;.to_string()),
            &#34;GreaterThan&#34; =&gt; Token::GreaterThan(&#34;&gt;&#34;.to_string()),
            &#34;LessThan&#34; =&gt; Token::LessThan(&#34;&lt;&#34;.to_string()),
            _ =&gt; panic!(&#34;Invalid token type: {}&#34;, token_type),
        }
    }

    pub fn get_token_regex(token_type: &amp;str) -&gt; String {
        match token_type {
            &#34;Print&#34; =&gt; r&#34;print&#34;,
            &#34;If&#34; =&gt; r&#34;if&#34;,
            &#34;Else&#34; =&gt; r&#34;else&#34;,
            &#34;Int&#34; =&gt; r&#34;int\s+&#34;,
            &#34;IntegerLiteral&#34; =&gt; r&#34;\d+&#34;,
            &#34;StringLiteral&#34; =&gt; r#&#34;\&#34;.*\&#34;&#34;#,
            &#34;Identifier&#34; =&gt; r&#34;[a-zA-Z_][a-zA-Z0-9_]* =&#34;,
            &#34;Plus&#34; =&gt; r&#34;\+&#34;,
            &#34;Assign&#34; =&gt; r&#34;=&#34;,
            &#34;Semicolon&#34; =&gt; r&#34;;&#34;,
            &#34;LeftParen&#34; =&gt; r&#34;\(&#34;,
            &#34;RightParen&#34; =&gt; r&#34;\)&#34;,
            &#34;LeftBrace&#34; =&gt; r&#34;\{&#34;,
            &#34;RightBrace&#34; =&gt; r&#34;}&#34;,
            &#34;GreaterThan&#34; =&gt; r&#34;&gt;&#34;,
            &#34;LessThan&#34; =&gt; r&#34;&lt;&#34;,
            _ =&gt; panic!(&#34;Invalid token type: {}&#34;, token_type),
        }.to_string()
    }
}
</code></pre><p>As you can see we are basically doing a basic string matching here and returning the appropriate result based on whatever matches. <code>match</code> in Rust is pretty much similar to <code>switch-case</code> statements in C++ and Java.</p><p>Another thing you&rsquo;ll notice is <code>&amp;str</code> and <code>String</code> being used, well they are different in Rust and to convert a <code>&amp;str</code> to <code>String</code> we used <code>to_string</code>. For the sake of simplicity, you can think of <code>&amp;str</code> as an immutable string and <code>String</code> as a mutable string. I regret choosing Rust so bad!!</p><p>Lastly, you also see that the function panics if it doesn&rsquo;t match any regex this is what we&rsquo;ll call a <strong>Syntax Error</strong>. This is something we should never let happen, aside from keywords that <code>if</code> can be mistaken for an identifier too!! But since keywords are before identifier in the <code>match</code> statement or as people say in <strong>higher priority</strong> this issue would be avoided.</p><p>But what if we have an overlap b/w two 2 tokens, for example, if we have <code>int ifer = 10</code> here <code>if</code> is a keyword now but <code>ifer</code> is actually an identifier? In cases like this, we choose the regex which is bigger in size, this rule is called <strong>maximal munch</strong>. Which we&rsquo;ll implement as well in <code>lexer.rs</code>.</p><p>Now that we have the functions it&rsquo;s time to address the elephant in the room, &ldquo;WTH are those weird strings in <code>get_token_regex</code> method???&rdquo;. Well, they are regex!!</p><p><img alt="Yeah No Shit William Zabka GIF - Yeah No Shit William Zabka Johnny Lawrence  - Discover & Share GIFs" loading=lazy src=https://media.tenor.com/oak_Ivz3By8AAAAe/yeah-no-shit-william-zabka.png></p><p>I know I know it seems like an alien language but let&rsquo;s learn more about it and how we built it!!</p><h3 id=understanding-regex>Understanding Regex<a hidden class=anchor aria-hidden=true href=#understanding-regex>#</a></h3><p>Regexes are essentially a set of rules or patterns defined by a string of characters, which can be used to search for specific text within a larger body of text, replace certain text patterns, or extract portions of a text that match a given pattern.</p><p>Let&rsquo;s understand with an example of an email validator. Email addresses typically follow a standard format: they start with a series of characters that can include letters, numbers, dots, underscores, or hyphens, followed by the <code>@</code> symbol, then more characters including letters and possibly dots, hyphens, or underscores, and finally a domain extension like <code>.com</code>, <code>.org</code>, etc.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706792046384/c8538e0e-076a-4cf2-bdf1-2779fd17f822.jpeg></p><p>Now you can either write a 100-line code that validates this by manually verifying every case possible like <strong>@</strong> should be present, the part before <strong>@</strong> should start with a letter, should contain letters or digits and not any invalid character, etc. An easy way would be to use regex like <code>[a-zA-Z][a-zA-Z0-9]*@[a-zA-Z0-9]+.[com|org|net]</code>.</p><p>That string might look really weird right now so let&rsquo;s break that down a bit:</p><ul><li><p><code>[a-zA-Z]</code>: This bit is like saying, &ldquo;Start by finding a letter.&rdquo; It doesn&rsquo;t matter if it&rsquo;s uppercase (A-Z) or lowercase (a-z), any letter will do. But, it&rsquo;s important that it&rsquo;s a letter, not a number or a symbol.</p></li><li><p><code>[a-zA-Z0-9]*</code>: After finding that first letter, this part says &ldquo;Now, keep going and find any mix of letters (both upper or lowercase) and numbers (0-9).&rdquo; The asterisk <code>*</code> at the end is a special &ldquo;metacharacter&rdquo;, allowing for 0 or more of these characters.</p></li><li><p><code>@</code>: Next, we must find the <code>@</code> symbol. This is necessary for email addresses as it separates the user&rsquo;s name from their email domain.</p></li><li><p><code>[a-zA-Z0-9]+</code>: After the <code>@</code> symbol, we&rsquo;re looking for a string of letters and numbers again. This represents the domain name (like &lsquo;gmail&rsquo; in &lsquo;<a href=mailto:user@gmail.com>user@gmail.com</a>&rsquo;). <code>+</code> at the end is another special &ldquo;metacharacter&rdquo;, allowing for 1 or more of these characters.</p></li><li><p><code>.</code>: Then comes a literal dot. In email addresses, this dot separates the domain name from the domain extension.</p></li><li><p><code>(com|org|net)</code>: Finally, we&rsquo;re specifying that the email address must end with either &lsquo;.com&rsquo;, &lsquo;.org&rsquo;, or &lsquo;.net&rsquo;. The parentheses group &lsquo;com&rsquo;, &lsquo;org&rsquo;, and &rsquo;net&rsquo; together as a single unit, and the <code>|</code> symbol acts as an &ldquo;or.&rdquo; So, this part says, &ldquo;Find either &lsquo;.com&rsquo;, &lsquo;.org&rsquo;, or &lsquo;.net&rsquo; right after the dot.&rdquo;</p></li></ul><p>You see how we simplified the whole thing to a regex string. Regex can help you validate a pattern but it can also help you find it!! If we pass <code>if</code> to regex it can return us a vector of starting and ending indexes of every occurrence of if in that string.</p><p><img alt="Good-stuff-from-hacf GIFs - Get the best GIF on GIPHY" loading=lazy src="https://media0.giphy.com/media/7pLv68ItwBaHS/giphy.gif?cid=6c09b952gjhqnj7eusw19m2tkgf0t84i60wsffhstd1laqp9&ep=v1_gifs_search&rid=giphy.gif&ct=g"></p><h3 id=writing-regex-for-tokens>Writing Regex for Tokens<a hidden class=anchor aria-hidden=true href=#writing-regex-for-tokens>#</a></h3><p>Based on whatever we learned now we can understand why we wrote the regex of each token the way we did. Let&rsquo;s see:</p><ol><li><p><strong>Keywords and Operators</strong></p><ul><li><code>r"print"</code>, <code>r"if"</code>, <code>r"else"</code>, <code>r"int\s+"</code>: These are straightforward. They match the specific keyword strings. For <code>int</code>, <code>\s+</code> is added to ensure there&rsquo;s at least one whitespace character after the keyword, differentiating it from identifiers that might start with &ldquo;int&rdquo; (like <code>integerValue</code>).</li></ul></li><li><p><strong>Literals</strong></p><ul><li><p><code>r"\d+"</code>: This pattern matches integer literals. <code>\d</code> represents any digit, and <code>+</code> means one or more of the preceding elements, so this matches a sequence of digits.</p></li><li><p><code>r#"\".*\""</code>: This pattern matches string literals. The <code>.*</code> matches any character (except for line terminators) any number of times, and <code>\"</code> matches quotation marks.</p></li></ul></li><li><p><strong>Identifiers</strong></p><ul><li><code>r"[a-zA-Z_][a-zA-Z0-9_]* ="</code>: This matches identifiers. Identifiers start with a letter or underscore, followed by any number of letters, digits, or underscores. The space and <code>=</code> at the end ensure we&rsquo;re matching an assignment operation.</li></ul></li><li><p><strong>Operators and Punctuation</strong></p><ul><li><code>r"\+"</code>, <code>r"="</code>, <code>r";"</code>, <code>r"\("</code>, <code>r"\)"</code>, <code>r"\{"</code>, <code>r"}"</code>, <code>r">"</code>, <code>r"&lt;"</code>: These patterns match operators and punctuation characters. In regex, some characters like <code>+</code>, <code>(</code>, <code>)</code>, <code>{</code>, <code>}</code> are special characters. To match them literally, they are escaped with a backslash <code>\</code>.</li></ul></li></ol><p>Boy was that a lot of information to consume!! But we&rsquo;re past the hard stuff now, or not given how much you like rust lol. But let&rsquo;s bring it all together and write the logic for our lexer in <code>lexer.rs</code>.</p><h3 id=writing-lexerrs>Writing lexer.rs<a hidden class=anchor aria-hidden=true href=#writing-lexerrs>#</a></h3><p>Now that we have implemented the tools to discover the tokens it&rsquo;s time to use them and find the token. But how do we even use them to identify the token? The brute forced way is to:</p><ol><li><p>Find the first regex match for each token regex.</p></li><li><p>Find the matches that have the smallest starting index.</p><ul><li>If many matches have the same smallest starting index then choose the match that is largest in length or has the maximum <code>ending_index-starting_index</code>.</li></ul></li><li><p>Remove the lexeme of the token that we selected from the program via regex.</p></li><li><p>Repeat the Process.</p></li></ol><p>This is good but there is too much redundancy in nested looping going on here!! I think instead what we can do is get all the regex matches at once and process them together, this way has many issues but is perfect for the program we want to compile. Here is the process:</p><ol><li><p>Iterate over an array of token string <code>tokens</code> in the descending order of their priority. That means indices of keyword token_strings would be lower than others. Aside from this initialize a <code>current_input</code> that holds the same value as the program and <code>match_vec</code> that&rsquo;s a vector of tuples with elements <code>(lexeme, starting_index, ending_index)</code>.</p><pre tabindex=0><code>    let current_input = program;
    let tokens = [
        &#34;Print&#34;, // Highest Priority
        &#34;If&#34;,
        &#34;Else&#34;,
        &#34;Int&#34;,
        &#34;Plus&#34;,
        &#34;Assign&#34;,
        &#34;Semicolon&#34;,
        &#34;LeftParen&#34;,
        &#34;RightParen&#34;,
        &#34;LeftBrace&#34;,
        &#34;RightBrace&#34;,
        &#34;GreaterThan&#34;,
        &#34;LessThan&#34;,
        &#34;IntegerLiteral&#34;,
        &#34;StringLiteral&#34;,
        &#34;Identifier&#34;, // Lowest Priority
    ];
    let mut match_vec: Vec&lt;(&amp;str, usize, usize)&gt; = Vec::new();
</code></pre></li><li><p>Start iteration over each <code>token</code> in <code>tokens</code> array:</p><pre tabindex=0><code>    for token in tokens.iter() {
</code></pre></li><li><p>Get the regex pattern string <code>token_regex</code> for the <code>token</code> token string using the <code>get_regex</code> method implemented under <code>Token</code> and initialize the <code>Regex</code> object <code>re</code> for the regex pattern we got.</p><pre tabindex=0><code>        let token_regex = Token::get_token_regex(token);
        let re = Regex::new(token_regex.as_str()).unwrap();
</code></pre></li><li><p>Use <code>re</code> to find all matches in <code>current_input</code>. This is done using the <code>find_iter</code> method, which returns an iterator over all non-overlapping matches of the pattern in the string. Each match found is a range indicating the match&rsquo;s starting index and ending index in the input string.</p><pre tabindex=0><code>        let matched = re.find_iter(current_input);
</code></pre></li><li><p>For each match found, create a tuple consisting of the token string, the starting index, and the ending index of the match in <code>current_input</code>. Append each of these tuples to <code>match_vec</code>. This step gathers all the matches for all token types in the input program. If there are no matches found for a token string skip the further steps in loop.</p><pre tabindex=0><code>        let all_matches = matched.collect::&lt;Vec&lt;_&gt;&gt;();

        if all_matches.len() == 0 {
            continue;
        }

        for m in all_matches.iter() {
            match_vec.push((token, m.start(), m.end()));
        }
    }
</code></pre></li><li><p>Once all matches for all tokens have been found and stored in <code>match_vec</code>, sort this vector. The sorting should primarily be by the starting index of the match (to process the input program from start to end) and secondarily by the length of the match (to prefer longer matches over shorter ones when they start at the same position). This respects both the priority of tokens and the rule of maximal munch (longest match).</p><pre tabindex=0><code>    match_vec.sort_by(|a, b| a.1.cmp(&amp;b.1).then_with(|| (b.2 - b.1).cmp(&amp;(a.2 - a.1))));
</code></pre></li><li><p>Iterate over the sorted <code>match_vec</code>, and for each tuple, create a <code>Token</code> instance corresponding to the token type with the lexeme extracted from <code>current_input</code> using the starting and ending indices using the <code>get_token</code> method.</p><pre tabindex=0><code>    let mut token_vec: Vec&lt;Token&gt; = Vec::new();
    for m in match_vec.iter() {
        token_vec.push(Token::get_token(m.0, Some(&amp;current_input[m.1..m.2])));
    }
</code></pre></li><li><p>Return the token vector.</p><pre tabindex=0><code>    token_vec
}
</code></pre></li></ol><p>And that&rsquo;s it!! You have just created your own memory-safe lexer in Rust in the most non-idiomatic Rust way possible. Well if you go by the idiomatic rust route you&rsquo;ll be stuck in syntax loops, but hey this way everyone understands it.</p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706802941204/12c358e6-2e08-4430-aa8f-138a162816e2.jpeg></p><p>Now that we have the lexer it&rsquo;s time to test it!</p><h3 id=testing-our-lexer>Testing our Lexer<a hidden class=anchor aria-hidden=true href=#testing-our-lexer>#</a></h3><p>In order to test our lexer we&rsquo;ll be writing the test code in <code>main.rs</code> we just need to define the code we need to compile as a string and get the results from the <code>lex_program</code> method we wrote above. That&rsquo;s all!!</p><pre tabindex=0><code>mod lexing;

use lexing::lexer::lex_program;

const PROGRAM: &amp;str = &#34;
int x = 5;
int y = 6;
int z = x + y;

if (z &gt; 10) {
    print(\&#34;Hello, world!\&#34;);
} else {
    print(\&#34;Goodbye, world!\&#34;);
}
&#34;;

fn main() {
    let tokens = lex_program(PROGRAM);

    for token in tokens.iter() {
        println!(&#34;{:?}&#34;, token);
    }
}
</code></pre><p>Everything is pretty standard except that <code>mod lexing;</code> part. Let me explain. This line declares a module named <code>lexing</code>. Rust will look for the definition of this module in a file named <code>lexing.rs</code> or in a file named <code>mod.rs</code> inside the <strong>lexing</strong> directory. We defined the <code>mod.rs</code> file, through which it&rsquo;ll bring the <code>lex_program</code> into <code>main.rs</code> file&rsquo;s namespace.</p><p>And that&rsquo;s it!! Take this moment and clap for yourself, cuz you&rsquo;ve just built your first lexer. LET&rsquo;S GOOOOO!!!!</p><p><img alt="time to chill - Pampered Cat Meme Meme Generator" loading=lazy src=https://media.makeameme.org/created/time-to-chill-f24b05814f.jpg></p><p>The next thing you can do is you can understand the theory behind regular how regex is implemented. But well it&rsquo;s up to you because that&rsquo;s more of a conventional college curriculum stuff.</p><p>As for the next steps for the series, we&rsquo;ll be using the tokens we got from the lexer we coded and build a parse tree or Abstract Syntax Tree from it! Stay tuned for <strong>Writing a Compiler in Rust #2: Parsing.</strong></p><p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1706789935903/1d657ea1-f193-4034-947f-be9b87a66770.png></p><h2 id=from-me-to-you>From Me to You<a hidden class=anchor aria-hidden=true href=#from-me-to-you>#</a></h2><p>In this blog, we covered how Lexers work and how we can code one in Rust. Well, Rust wasn&rsquo;t the smoothest part about what we did but hey there is a first time for everything and this was probably the first time you got angry that a programming language exists. Jokes aside compilers be a tricky topic to understand but coding one makes everything clear. Hope you learned something new!</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/the-aya-project-trials/><span class=title>¬´ Prev</span><br><span>The Aya Project: My Experience and Learnings</span>
</a><a class=next href=http://localhost:1313/posts/lima/><span class=title>Next ¬ª</span><br><span>LIMA: Less is More for Alignment</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on x" href="https://x.com/intent/tweet/?text=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f&amp;title=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis&amp;summary=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f&title=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on whatsapp" href="https://api.whatsapp.com/send?text=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on telegram" href="https://telegram.me/share/url?text=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Writing a Compiler in Rust #1: Lexical Analysis on ycombinator" href="https://news.ycombinator.com/submitlink?t=Writing%20a%20Compiler%20in%20Rust%20%231%3a%20Lexical%20Analysis&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frust-compiler-1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=http://localhost:1313/>Journal | Herumb Shandilya</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>