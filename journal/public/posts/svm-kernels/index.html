<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Training SVM over Custom Kernels | Journal | Herumb Shandilya</title>
<meta name=keywords content>
<meta name=description content="One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.
Most of us might be familiar with training models.">
<meta name=author content="Herumb Shandilya">
<link rel=canonical href=https://journal.herumbshandilya.com/posts/svm-kernels/>
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<link rel=preload href=/apple-touch-icon.png as=image>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://journal.herumbshandilya.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://journal.herumbshandilya.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://journal.herumbshandilya.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://journal.herumbshandilya.com/apple-touch-icon.png>
<link rel=mask-icon href=https://journal.herumbshandilya.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.2">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="Training SVM over Custom Kernels">
<meta property="og:description" content="One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.
Most of us might be familiar with training models.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://journal.herumbshandilya.com/posts/svm-kernels/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-04-29T00:00:00+00:00">
<meta property="article:modified_time" content="2021-04-29T00:00:00+00:00"><meta property="og:site_name" content="Journal | Herumb Shandilya">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Training SVM over Custom Kernels">
<meta name=twitter:description content="One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.
Most of us might be familiar with training models.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://journal.herumbshandilya.com/posts/"},{"@type":"ListItem","position":3,"name":"Training SVM over Custom Kernels","item":"https://journal.herumbshandilya.com/posts/svm-kernels/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training SVM over Custom Kernels","name":"Training SVM over Custom Kernels","description":"One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.\nMost of us might be familiar with training models.","keywords":[],"articleBody":"One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.\nMost of us might be familiar with training models. Few of us might be familiar with when to use which model. But when it comes to the details of these models we might fail to utilize them. In SVM, most of us might use the default RBF, a few of us might play with other kernels to find a better model and chosen ones might understand the working and purpose of these kernels. But can you create a kernel of your own?\nLet’s go ahead and try to understand a bit about how you can create your own kernel and train a simple SVC model over it.\nGetting Our Data Every ML Pipeline is basically non-existent without data. So let’s start by getting our data. I’m a simple man so for the sake of simplicity I’ll just make a dataset using the make_classification utility in sklearn.datasets.\nfrom sklearn.datasets import make_classification x,y = make_classification(n_samples = 1000) print(x.shape, y.shape) Now that we have the data all sorted out let’s go ahead and understand about working and use of kernels in SVM.\nUnderstanding Kernels To those who forgot how SVM works, which is nothing to worry about, let’s take a walk down the memory. So basically what SVM aims to do is to introduce a hyperplane that divides the data such that the margin by which they are separated is maximized but if you just what to maximize the margin then you might run into cases where its impossible to maximize the margin or even worse outliers affecting the margin size.\nSo to tackle this we ignore some points and allow the error in order to achieve optimal margin, we call these points support vectors. And now the margin is called a soft margin. So your aim is to maximize the soft margin. How much misclassification to allow is determined using cross-validation. This is a nice approach until you run into something I call a not so simple distribution. Take a look at the following not so simple example:-\nSo as you can see you can’t really separate the above distribution decently with a single hyperplane. But this is where things become interesting, if you map this distribution to a higher dimension you’ll be able to do the same. This is where kernels come into play these are mathematical functions that map your distribution to a higher dimension. But these kernel functions only compute the relationship of pair in the distribution as if they were in a higher dimension without actually transforming it and this trick to compute relationships in distribution at higher dimension without transforming it is what we call a kernel trick.\nPhew, that was a lot of complicated stuff. But how does RBF work? RBF actually maps the data into infinite dimensions, which makes it difficult to visualize. So let’s keep that for another article.\nSo now that we have a bit of an understanding of the use of kernels in SVM let’s learn about how we can use a custom one in training them.\nMy Kernel, My Rules Every time you create an SVC() instance it has a kernel associated with it that handles the mapping part, if you don’t specify it explicitly then it takes the kernel as RBF with looks like the following:-\n$$ K(x_i,x_j) = e^{-{\\gamma}||x_i-x_j||^2} ,{\\forall}{\\gamma}  0 $$\n$$ ||x_i-x_j||{\\space}is{\\space}the{\\space}euclidean{\\space}distance{\\space}between{\\space}x_i{\\space}and{\\space}x_j $$\nIf you don’t understand what’s written above then it’s fine you can forget it. The important thing to understand is how it works it takes 2 points and computes the RBF for them and stores them in their location in a gram matrix. Gram Matrix is what we’ll use to define relationships among pairs for the given kernel. Now 2 ways to train SVM over custom kernel is to:-\n Passing the kernel function Passing Gram Matrix  For the innocent souls who are unaware of Gram Matrix, it is basically how your kernel functions are represented, simple as that. If you wanna go into the mathematical details for it feel free to Google.\nBy passing the kernel function as an argument Let’s now implement a simple Linear Kernel Function and train our model over it. Linear Kernel looks like the following:-\n$$ K(x_i,x_j) = x_i \\cdot x_j $$\nSimple right? All you have to do is just perform a dot product between the pairs. Let’s create a function to do the same.\ndef linear_kernel(x_i, x_j): return x_i.dot(x_j.T) That was simple, wasn’t it? Let’s go ahead and create 2 classifiers one that uses the linear kernel defined in sklearn and the other that we created and then compare their performance.\nfrom sklearn.svm import SVC from sklearn.metrics import accuracy_score clf1 = SVC(kernel = linear_kernel) clf1.fit(x,y) print(f'Accuracy on Custom Kernel: {accuracy_score(y, clf1.predict(x))}') clf2 = SVC(kernel = 'linear') clf2.fit(x,y) print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y, clf2.predict(x))}') Output:-\nAccuracy on Custom Kernel: 0.961 Accuracy on Inbuilt Kernel: 0.961 Well, the results are the same. That was pretty awesome, wasn’t it? Now let’s try doing the same using the 2nd method.\nBy passing Gram Matrix You can define your own kernels by either giving the kernel as a function, as we saw in the above example, or by precomputing the Gram matrix. We’ll first make a function that makes gram matrix given data and function and then make function to compute RBF.\nimport numpy as np def get_gram(x1, x2, kernel): return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1]) def RBF(x1, x2, gamma = 1): return np.exp(-gamma*np.linalg.norm(x1-x2)) Now that we have pre-requisites all set let’s train our models and compare. Two things that’ll change is:-\n We’ll pass kernel = ‘precomputed’ We’ll pass the data as gram matrix while passing data in fit() or predict() function  Now things are a bit different if you have a testing set. For example, if we have x_train and x_test then gram matrix to pass in fit() is computed between x_train and x_train but for predicting on x_test it is computed between x_test and x_train.\nimport numpy as np from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y) def get_gram(x1, x2, kernel): return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1]) def RBF(x1, x2, gamma = 1): return np.exp(-gamma * np.linalg.norm(x1-x2)) clf1 = SVC(kernel = 'precomputed') clf1.fit(get_gram(x_train, x_train, RBF), y_train) print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(get_gram(x_test, x_train, RBF)))}') clf2 = SVC(kernel = 'rbf') clf2.fit(x_train,y_train) print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}') Output:-\nAccuracy on Custom Kernel: 0.912 Accuracy on Inbuilt Kernel: 0.904 Code import numpy as np from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split x,y = make_classification(n_samples = 1000) x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y) # Method 1 def linear_kernel(x_i, x_j): return x_i.dot(x_j.T) clf1 = SVC(kernel = linear_kernel) clf1.fit(x_train,y_train) print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(x_test))}') clf2 = SVC(kernel = 'linear') clf2.fit(x_train, y_train) print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}') # Method 2 def get_gram(x1, x2, kernel): return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1]) def RBF(x1, x2, gamma = 1): return np.exp(-gamma * np.linalg.norm(x1-x2)) clf1 = SVC(kernel = 'precomputed') clf1.fit(get_gram(x_train, x_train, RBF), y_train) print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(get_gram(x_test, x_train, RBF)))}') clf2 = SVC(kernel = 'rbf') clf2.fit(x_train,y_train) print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}') Parting Words Well, how about that our kernel actually performed well. Noice! Now you know how you can train SVM over a custom kernel. You can try and implement other kernels like Thin-Plate, Cauchy, and other kernels with intimidating names. Using custom kernels isn’t usually done practically and in my experience, it’s probably because custom ones take a lot of time to train and predict especially if you go with the gram matrix approach. But there is no harm in learning about it right? With this, I congratulate you on learning something new and now you can go ahead and try one yourself. The next article is gonna be especially interesting it’s about probably the most popular algo but with a twist of our own. See you soon.\n","wordCount":"1392","inLanguage":"en","datePublished":"2021-04-29T00:00:00Z","dateModified":"2021-04-29T00:00:00Z","author":{"@type":"Person","name":"Herumb Shandilya"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://journal.herumbshandilya.com/posts/svm-kernels/"},"publisher":{"@type":"Organization","name":"Journal | Herumb Shandilya","logo":{"@type":"ImageObject","url":"https://journal.herumbshandilya.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://journal.herumbshandilya.com accesskey=h title="Herumb's Journal (Alt + H)">
<img src=https://journal.herumbshandilya.com/apple-touch-icon.png alt=logo aria-label=logo height=35>Herumb's Journal</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://journal.herumbshandilya.com/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=https://the-ir-book.herumbshandilya.com/ title="The Small Book of Information Retrieval">
<span>The Small Book of Information Retrieval</span>
</a>
</li>
<li>
<a href=https://www.herumbshandilya.com/ title=Portfolio>
<span>Portfolio</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class=post-title>
Training SVM over Custom Kernels
</h1>
<div class=post-meta><span title="2021-04-29 00:00:00 +0000 UTC">April 29, 2021</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Herumb Shandilya
</div>
</header>
<div class=post-content><p>One thing that always intrigued me about ML is that the more you learn about it the more you realize how little you know. One such case that happened to me a few months ago when a person asked me if I could help him in SVM and me being me I was like sure not a big deal. It was a big deal.</p>
<p>Most of us might be familiar with training models. Few of us might be familiar with when to use which model. But when it comes to the details of these models we might fail to utilize them. In SVM, most of us might use the default RBF, a few of us might play with other kernels to find a better model and chosen ones might understand the working and purpose of these kernels. But can you create a kernel of your own?</p>
<p>Let&rsquo;s go ahead and try to understand a bit about how you can create your own kernel and train a simple SVC model over it.</p>
<h2 id=getting-our-data>Getting Our Data<a hidden class=anchor aria-hidden=true href=#getting-our-data>#</a></h2>
<p>Every ML Pipeline is basically non-existent without data. So let&rsquo;s start by getting our data. I&rsquo;m a simple man so for the sake of simplicity I&rsquo;ll just make a dataset using the <strong>make_classification</strong> utility in <strong>sklearn.datasets</strong>.</p>
<pre tabindex=0><code>from sklearn.datasets import make_classification

x,y = make_classification(n_samples = 1000)
print(x.shape, y.shape)
</code></pre><p>Now that we have the data all sorted out let&rsquo;s go ahead and understand about working and use of kernels in SVM.</p>
<h2 id=understanding-kernels>Understanding Kernels<a hidden class=anchor aria-hidden=true href=#understanding-kernels>#</a></h2>
<p>To those who forgot how SVM works, which is nothing to worry about, let&rsquo;s take a walk down the memory. So basically what SVM aims to do is to introduce a hyperplane that divides the data such that the margin by which they are separated is maximized but if you just what to maximize the margin then you might run into cases where its impossible to maximize the margin or even worse outliers affecting the margin size.</p>
<p>So to tackle this we ignore some points and allow the error in order to achieve optimal margin, we call these points <strong>support vectors</strong>. And now the margin is called a soft margin. So your aim is to maximize the soft margin. How much misclassification to allow is determined using cross-validation. This is a nice approach until you run into something I call a not so simple distribution. Take a look at the following not so simple example:-</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1619549309926/PhcWAMCbK.png alt=image.png>
</p>
<p>So as you can see you can&rsquo;t really separate the above distribution decently with a single hyperplane. But this is where things become interesting, if you map this distribution to a higher dimension you&rsquo;ll be able to do the same. This is where kernels come into play these are mathematical functions that map your distribution to a higher dimension. But these kernel functions only compute the relationship of pair in the distribution as if they were in a higher dimension without actually transforming it and this trick to compute relationships in distribution at higher dimension without transforming it is what we call a <strong>kernel trick</strong>.</p>
<p>Phew, that was a lot of complicated stuff. But how does RBF work? RBF actually maps the data into infinite dimensions, which makes it difficult to visualize. So let&rsquo;s keep that for another article.</p>
<p>So now that we have a bit of an understanding of the use of kernels in SVM let&rsquo;s learn about how we can use a custom one in training them.</p>
<h2 id=my-kernel-my-rules>My Kernel, My Rules<a hidden class=anchor aria-hidden=true href=#my-kernel-my-rules>#</a></h2>
<p>Every time you create an SVC() instance it has a kernel associated with it that handles the mapping part, if you don&rsquo;t specify it explicitly then it takes the kernel as RBF with looks like the following:-</p>
<p>$$
K(x_i,x_j) = e^{-{\gamma}||x_i-x_j||^2} ,{\forall}{\gamma} > 0
$$</p>
<p>$$
||x_i-x_j||{\space}is{\space}the{\space}euclidean{\space}distance{\space}between{\space}x_i{\space}and{\space}x_j
$$</p>
<p>If you don&rsquo;t understand what&rsquo;s written above then it&rsquo;s fine you can forget it. The important thing to understand is how it works it takes 2 points and computes the RBF for them and stores them in their location in a gram matrix. Gram Matrix is what we&rsquo;ll use to define relationships among pairs for the given kernel. Now 2 ways to train SVM over custom kernel is to:-</p>
<ul>
<li>Passing the kernel function</li>
<li>Passing Gram Matrix</li>
</ul>
<p>For the innocent souls who are unaware of Gram Matrix, it is basically how your kernel functions are represented, simple as that. If you wanna go into the mathematical details for it feel free to Google.</p>
<h3 id=by-passing-the-kernel-function-as-an-argument>By passing the kernel function as an argument<a hidden class=anchor aria-hidden=true href=#by-passing-the-kernel-function-as-an-argument>#</a></h3>
<p>Let&rsquo;s now implement a simple Linear Kernel Function and train our model over it. Linear Kernel looks like the following:-</p>
<p>$$
K(x_i,x_j) = x_i \cdot x_j
$$</p>
<p>Simple right? All you have to do is just perform a dot product between the pairs. Let&rsquo;s create a function to do the same.</p>
<pre tabindex=0><code>def linear_kernel(x_i, x_j):
    return x_i.dot(x_j.T)
</code></pre><p>That was simple, wasn&rsquo;t it? Let&rsquo;s go ahead and create 2 classifiers one that uses the linear kernel defined in sklearn and the other that we created and then compare their performance.</p>
<pre tabindex=0><code>from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

clf1 = SVC(kernel = linear_kernel)
clf1.fit(x,y)
print(f'Accuracy on Custom Kernel: {accuracy_score(y, clf1.predict(x))}')

clf2 = SVC(kernel = 'linear')
clf2.fit(x,y)
print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y, clf2.predict(x))}')
</code></pre><p><strong>Output:-</strong></p>
<pre tabindex=0><code>Accuracy on Custom Kernel: 0.961
Accuracy on Inbuilt Kernel: 0.961
</code></pre><p>Well, the results are the same. That was pretty awesome, wasn&rsquo;t it? Now let&rsquo;s try doing the same using the 2nd method.</p>
<h3 id=by-passing-gram-matrix>By passing Gram Matrix<a hidden class=anchor aria-hidden=true href=#by-passing-gram-matrix>#</a></h3>
<p>You can define your own kernels by either giving the kernel as a function, as we saw in the above example, or by precomputing the Gram matrix. We&rsquo;ll first make a function that makes gram matrix given data and function and then make function to compute RBF.</p>
<pre tabindex=0><code>import numpy as np
def get_gram(x1, x2, kernel):
    return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1])

def RBF(x1, x2, gamma  = 1):
    return np.exp(-gamma*np.linalg.norm(x1-x2))
</code></pre><p>Now that we have pre-requisites all set let&rsquo;s train our models and compare. Two things that&rsquo;ll change is:-</p>
<ul>
<li>We&rsquo;ll pass <strong>kernel = &lsquo;precomputed&rsquo;</strong></li>
<li>We&rsquo;ll pass the data as gram matrix while passing data in <strong>fit()</strong> or <strong>predict()</strong> function</li>
</ul>
<p>Now things are a bit different if you have a testing set. For example, if we have x_train and x_test then gram matrix to pass in fit() is computed between x_train and x_train but for predicting on x_test it is computed between x_test and x_train.</p>
<pre tabindex=0><code>import numpy as np
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y)

def get_gram(x1, x2, kernel):
    return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1])

def RBF(x1, x2, gamma  = 1):
    return np.exp(-gamma * np.linalg.norm(x1-x2))

clf1 = SVC(kernel = 'precomputed')
clf1.fit(get_gram(x_train, x_train, RBF), y_train)
print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(get_gram(x_test, x_train, RBF)))}')

clf2 = SVC(kernel = 'rbf')
clf2.fit(x_train,y_train)
print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}')
</code></pre><p><strong>Output:-</strong></p>
<pre tabindex=0><code>Accuracy on Custom Kernel: 0.912
Accuracy on Inbuilt Kernel: 0.904
</code></pre><h1 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h1>
<pre tabindex=0><code>import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

x,y = make_classification(n_samples = 1000)
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y)

# Method 1
def linear_kernel(x_i, x_j):
    return x_i.dot(x_j.T)

clf1 = SVC(kernel = linear_kernel)
clf1.fit(x_train,y_train)
print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(x_test))}')

clf2 = SVC(kernel = 'linear')
clf2.fit(x_train, y_train)
print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}')


# Method 2
def get_gram(x1, x2, kernel):
    return np.array([[kernel(_x1, _x2) for _x2 in x2] for _x1 in x1])

def RBF(x1, x2, gamma  = 1):
    return np.exp(-gamma * np.linalg.norm(x1-x2))

clf1 = SVC(kernel = 'precomputed')
clf1.fit(get_gram(x_train, x_train, RBF), y_train)
print(f'Accuracy on Custom Kernel: {accuracy_score(y_test, clf1.predict(get_gram(x_test, x_train, RBF)))}')

clf2 = SVC(kernel = 'rbf')
clf2.fit(x_train,y_train)
print(f'Accuracy on Inbuilt Kernel: {accuracy_score(y_test, clf2.predict(x_test))}')

</code></pre><h2 id=parting-words>Parting Words<a hidden class=anchor aria-hidden=true href=#parting-words>#</a></h2>
<p>Well, how about that our kernel actually performed well. Noice! Now you know how you can train SVM over a custom kernel. You can try and implement other kernels like Thin-Plate, Cauchy, and other kernels with intimidating names. Using custom kernels isn&rsquo;t usually done practically and in my experience, it&rsquo;s probably because custom ones take a lot of time to train and predict especially if you go with the gram matrix approach. But there is no harm in learning about it right? With this, I congratulate you on learning something new and now you can go ahead and try one yourself. The next article is gonna be especially interesting it&rsquo;s about probably the most popular algo but with a twist of our own. See you soon.</p>
</div>
<footer class=post-footer>
<nav class=paginav>
<a class=prev href=https://journal.herumbshandilya.com/posts/class-imbalance/>
<span class=title>« Prev Page</span>
<br>
<span>Class Imbalance comes in Like a Lion</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on twitter" href="https://twitter.com/intent/tweet/?text=Training%20SVM%20over%20Custom%20Kernels&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f&title=Training%20SVM%20over%20Custom%20Kernels&summary=Training%20SVM%20over%20Custom%20Kernels&source=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f&title=Training%20SVM%20over%20Custom%20Kernels"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on whatsapp" href="https://api.whatsapp.com/send?text=Training%20SVM%20over%20Custom%20Kernels%20-%20https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Training SVM over Custom Kernels on telegram" href="https://telegram.me/share/url?text=Training%20SVM%20over%20Custom%20Kernels&url=https%3a%2f%2fjournal.herumbshandilya.com%2fposts%2fsvm-kernels%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=https://journal.herumbshandilya.com>Journal | Herumb Shandilya</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>