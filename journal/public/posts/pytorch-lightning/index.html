<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers | Journal | Herumb Shandilya</title>
<meta name=keywords content>
<meta name=description content="When I was a young man,
I had liberty but I didn’t see it,
I had time but I didn’t know it,
And I had PyTorch Lightning but I didn&rsquo;t use it.
- Newbie PyTorch User
 Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance.">
<meta name=author content="Herumb Shandilya">
<link rel=canonical href=https://www.herumbshandilya.com/posts/pytorch-lightning/>
<link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style>
<link rel=icon href=https://www.herumbshandilya.com/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://www.herumbshandilya.com/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://www.herumbshandilya.com/favicon-32x32.png>
<link rel=apple-touch-icon href=https://www.herumbshandilya.com/apple-touch-icon.png>
<link rel=mask-icon href=https://www.herumbshandilya.com/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><meta property="og:title" content="PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers">
<meta property="og:description" content="When I was a young man,
I had liberty but I didn’t see it,
I had time but I didn’t know it,
And I had PyTorch Lightning but I didn&rsquo;t use it.
- Newbie PyTorch User
 Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://www.herumbshandilya.com/posts/pytorch-lightning/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-06-08T00:00:00+00:00">
<meta property="article:modified_time" content="2021-06-08T00:00:00+00:00"><meta property="og:site_name" content="Journal | Herumb Shandilya">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers">
<meta name=twitter:description content="When I was a young man,
I had liberty but I didn’t see it,
I had time but I didn’t know it,
And I had PyTorch Lightning but I didn&rsquo;t use it.
- Newbie PyTorch User
 Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.herumbshandilya.com/posts/"},{"@type":"ListItem","position":2,"name":"PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers","item":"https://www.herumbshandilya.com/posts/pytorch-lightning/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers","name":"PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers","description":"When I was a young man,\nI had liberty but I didn’t see it,\nI had time but I didn’t know it,\nAnd I had PyTorch Lightning but I didn\u0026rsquo;t use it.\n- Newbie PyTorch User\n Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance.","keywords":[],"articleBody":" When I was a young man,\nI had liberty but I didn’t see it,\nI had time but I didn’t know it,\nAnd I had PyTorch Lightning but I didn’t use it.\n- Newbie PyTorch User\n Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance. I mean I have to write a training loop with redundant steps while Tensorflow beginners were just passing and chilling.\nSo I did what most PyTorch newbies did, learned and wrote the training loop code until it became muscle memory and that is something you shouldn’t do. Few tips for that is just understanding why each step works and once you do that every line of code starts to make sense, trust me once I did that I never forgot it again.\nBack to the topic, one thing that I love about PyTorch is the extent to which I can customize it and how easy it is to debug. But still, it would be better if somehow I was able to keep those features and reduce the redundancy.\nWell there you have it, the solution is PyTorch Lightning I mean anyone with such a cool name is already destined for greatness but if that reason doesn’t convince you then I hope that by the end of this article you will be.\nP.S. it’s actually better if you have some basic idea about Lightning. If you wanna learn its basics try this article I wrote a while back on Training Neural Networks using PyTorch Lighting.\nDatasets, DataLoaders and DataModule There are two things that give me immense satisfaction in life, first is watching a newbie Vim user trying to exit Vim, and the second is watching a newbie PyTorch user trying to create Dataloader for custom data. So let’s take a look at how you can create DataLoaders in PyTorch using an awesome utility called Dataset.\nThe Dataset Class One of the things that is essential if you are learning PyTorch is how to create DataLoaders there are many ways to go for it I mean for Image you have ImageFolder utility in torchvision and for Data we have BucketIterator which I won’t lie are quite handy but still what if the data you load isn’t in the desired format? In that case, you can use the Dataset class, and not just that the thing that I love about Dataset class is that they are customizable to an extent you can’t imagine.\nIn Dataset class you have 3 main functions that you must define, let’s take a look at them:-\n  __init__(self, ): The constructor function. there are infinite possibilities of things you can do here. Think of it as the staging area of our data. Usually, you pass either a path, a dataframe, or an array as the argument to this class’s instance. Here you usually define class attributes like feature matrix and target vector. If you are working with an image you can assign transform variable, if you are working with the you can assign a tokenizer variable etc.\n  __len__(self): The length function. this is where you’ll return the length of the dataset, you basically return the length of the feature matrix or the dataset in whatever format you have.\n  __getitem__(self, idx): The fetching function. This is the function where you define how your data will be returned. You can apply various preprocessing to be applied on the data of at idx index here and then return its tensor along with other variables like mask or target by packing them in a list, tuple, dict, etc. I used to pack them in a list but when I saw Abhishek Thakur’s video on this I started using dicts and I never looked back since that day.\n  The guy is a PyTorch madlad go watch his videos if you haven’t already brilliant and to-the-point hands-on videos on many interesting topics. Let’s keep all that aside for now and take an example, shall we? I’ll be using make_classification to create the dataset to keep things simple.\nimport torch from sklearn.datasets import make_classification from torch.utils.data import Dataset class TabularData(Dataset): def __init__(self, X, Y, train = True): self.X = X self.Y = Y def __len__(self): return len(self.X) def __getitem__(self, idx): features = self.X[idx] if self.Y is not None: target = self.Y[idx] return { 'X' : torch.tensor(features, dtype = torch.float32), 'Y' : torch.tensor(target, dtype = torch.long) } else: return { 'X' : torch.tensor(features, dtype = torch.float32) } X, Y = make_classification() train_data = TabularData(X, Y) train_data[0] Output:-\n{'X': tensor([ 1.1018, -0.0042, 2.1382, -0.7926, -0.6016, 1.5499, -0.4010, 0.3327, 0.1973, -1.3655, 0.4870, 0.7568, -0.7460, -0.8977, 0.1395, 0.0814, -1.4849, -0.2000, 1.2643, 0.4178]), 'Y': tensor(1)} Subarashi! That seems to be working correctly you can try and experiment with what happens if you change the output of the classes a bit and check the results. But for this tutorial, we’ll be working on Fashion MNIST data and thankfully in torchvision there already is a dataset for that so let’s load that.\nfrom torchvision import datasets, transforms transform = transforms.Compose([ transforms.ToTensor() ]) train = datasets.FashionMNIST('',train = True, download = True, transform=transform) test = datasets.FashionMNIST('',train = False, download = True, transform=transform) Now that we have the data it’s time to move onto DataLoaders.\nDataLoaders DataLoaders are responsible to take input a dataset and then pack the data in them into batches and create an iterator to iterate over these batches. They really make the whole batching process easier while keeps the customizability to the fullest. I mean you can define how to batch your data by writing your own collate_fn, what more do you want?\nWe saw how we can create a dataset class, to create its DataLoader you just pass that Dataset instance to the DataLoader and you are done. Let’s see how to do it for the MNIST dataset that we created above and check it’s output.\nimport matplotlib.pyplot as plt from torch.utils.data import DataLoader trainloader = DataLoader(train, batch_size= 32, shuffle=True) testloader = DataLoader(test, batch_size= 32, shuffle=True) #Plotting a Batch of DataLoader images, labels = iter(trainloader).next() plt.figure(figsize = (12,16)) for e,(img, lbl) in enumerate(zip(images, labels)): plt.subplot(8,4,e+1) plt.imshow(img[0]) plt.title(f'Class: {lbl.item()}') plt.subplots_adjust(hspace=0.6) Output:-\nDamn, that looks beautiful and correct, well there you go seems like our DataLoaders work as expected. But is it just me or does this seem too messy? I mean it’s great we batched the data but the variables seem to be everywhere, it doesn’t seem much organized. Well, that’s basically where DataModules come in handy 😎.\nDataModules DataModule is a reusable and shareable class that encapsulates the DataLoaders along with the steps required to process data. Creating DataLoaders can get messy that’s why it’s better to club the dataset in the form of DataModule. DataModule has few methods that must define the format of DataModule is as follows:-\nimport pytorch-lightning as pl class DataModuleClass(pl.LightningDataModule): def __init__(self): # Define class attributs here def prepare_data(self): # Define steps that should be done # on only one GPU, like getting data. def setup(self, stage=None): # Define steps that should be done on # every GPU, like splitting data, applying # transform etc. def train_dataloader(self): # Stage DataLoader for Training Data def val_dataloader(self): # Stage DataLoader for Validation Data def test_dataloader(self): # Stage DataLoader for Testing Data That seems great so let’s go ahead and create DataModule for our Fashion MNIST Data.\nimport pytorch_lightning as pl class DataModuleFashionMNIST(pl.LightningDataModule): def __init__(self): super().__init__() self.dir = '' self.batch_size = 32 self.transform = transforms.Compose([ transforms.ToTensor() ]) def prepare_data(self): datasets.FashionMNIST(self.dir, train = True, download = True) datasets.FashionMNIST(self.dir, train = False, download = True) def setup(self, stage=None): data = datasets.FashionMNIST(self.dir, train = True, transform = self.transform) self.train, self.valid = random_split(data, [52000, 8000]) self.test = datasets.FashionMNIST(self.download_dir, train = False, transform = self.transform) def train_dataloader(self): return DataLoader(self.train, batch_size = self.batch_size) def val_dataloader(self): return DataLoader(self.valid, batch_size = self.batch_size) def test_dataloader(self): return DataLoader(self.test_data, batch_size = self.batch_size) data = DataModuleFashionMNIST() Perfect that’s basically it but if you want an in-depth explanation on them you can refer this article I wrote explaining about DataModules.\nLightning Callbacks \u0026 Hooks Callbacks are basically programs that contain code that’s run when it is required. When and what a callback should do is defined by using Callback Hooks, a few of them are on_epoch_end, on_validation_epoch_end, etc. You can maybe define a logic to monitor a metric, save a model, or various other cool stuff. If I have to define callbacks as a meme it’ll be the following.\nWell necessary for training but useful for other stuff. There are many inbuilt callbacks that can be used for various important tasks. A few of them are:-\n   CallBack Description     EarlyStopping Monitor a metric and stop training when it stops improving.   LearningRateMonitor Automatically monitors and logs learning rate for learning rate schedulers during training.   ModelCheckpoint Save the model periodically by monitoring a quantity.   Callback Base Class to define custom callbacks.   LambdaCallback Basically Lambda Function of Callbacks.    We’ll be using the first two for this article but you can refer to docs to learn more about them. Lightning’s video tutorials on their website are pretty good.\nTPU - Hulkified GPU? TPUs are accelerators used to speed up Machine Learning Tasks. The catch is that they are platform dependant i.e. TensorFlow. TPUs are optimized for Tensorflow mainly which I think is quite selfish given PyTorch is so awesome.\nBut we can actually use them in PyTorch by making and passing a TPU Sampler in the DataLoader. It’s one hell of a messy task you have to replace the device type with xm.xla_device() and add 2 extra steps for optimizer and not just that you’ll have to install PyTorch/XLA to do all that. It goes something like this:-\nimport torch_xla.core.xla_model as xm dev = xm.xla_device() # TPU sampler data_sampler = torch.utils.data.distributed.DistributedSampler( dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()) dataloader = DataLoader(dataset, batch_size=32, sampler = data_sampler) # Training loop for batch in dataloader: ... xm.optimizer_step(optimizer) xm.mark_step() ... Man the above code is a mess but in Lightning all that is reduced to a simple single line. All you need to do is pass the number of tpu_cores to be you and you are done for the day. Really it’s that simple.\ntrainer = pl.Trainer(tpu_cores = 1) trainer.fit(model) I mean it literally couldn’t get any simpler than this but still, there is one more thing that’s left to talk about and that is Loggers. Let’s take a look at that.\nLoggers Loggers are a kind of utility that you can use to monitor metrics and hyperparameters and much more cool stuff. In fact you probably already have tried one, Tensorboard. Ever heard of it? If you are here chances are you might have. Along with Tensorboard, PyTorch Lightning supports various 3rd party loggers from Weights and Biases, Comet.ml, MlFlow, etc.\nIn fact, in Lightning, you can use multiple loggers together. To use a logger you can create its instance and pass it in Trainer Class under logger parameter individually or as a list of loggers.\nfrom pytorch_lightning.loggers import WandbLogger # Single Logger wandb_logger = WandbLogger(project='Fashion MNIST', log_model='all') trainer = Trainer(logger=wandb_logger) # Multiple Loggers from pytorch_lightning.loggers import TensorBoardLogger tb_logger = TensorBoardLogger('tb_logs', name='my_model') trainer = Trainer(logger=[wandb_logger, tb_logger]) The point is how to log the values? We’ll take a look at how you can do that along with applying all the stuff I talked about above.\nPutting it All Together The Ingredient to a model is the data that you feed it, we talked about DataModules in this article so let’s start by creating a DataModule for out Fashion MNIST data.\nimport pytorch_lightning as pl from torchvision import datasets, transforms class DataModuleFashionMNIST(pl.LightningDataModule): def __init__(self, batch_size = 32): super().__init__() self.dir = '' self.batch_size = batch_size self.transform = transforms.Compose([ transforms.ToTensor() ]) def prepare_data(self): datasets.FashionMNIST(self.dir, train = True, download = True) datasets.FashionMNIST(self.dir, train = False, download = True) def setup(self, stage=None): data = datasets.FashionMNIST(self.dir, train = True, transform = self.transform) self.train, self.valid = random_split(data, [52000, 8000]) self.test = datasets.FashionMNIST(self.download_dir, train = False, transform = self.transform) def train_dataloader(self): return DataLoader(self.train, batch_size = self.batch_size) def val_dataloader(self): return DataLoader(self.valid, batch_size = self.batch_size) def test_dataloader(self): return DataLoader(self.test_data, batch_size = self.batch_size) data = DataModuleFashionMNIST() Now let’s create our model class and setup logs for the WandbLogger and then create a model instance for the same.\nfrom torch import nn, optim import torch.nn.functional as F class FashionMNISTModel(pl.LightningModule): def __init__(self): super().__init__() # 28 * 28 * 3 self.conv1 = nn.Conv2d(1,16, stride = 1, padding = 1, kernel_size = 3) # 14 * 14 * 16 self.conv2 = nn.Conv2d(16,32, stride = 1, padding = 1, kernel_size = 3) # 7 * 7 * 32 self.conv3 = nn.Conv2d(32,64, stride = 1, padding = 1, kernel_size = 3) # 3 * 3 * 64 self.fc1 = nn.Linear(3*3*64,128) self.fc2 = nn.Linear(128,64) self.out = nn.Linear(64,10) self.pool = nn.MaxPool2d(2,2) self.loss = nn.CrossEntropyLoss() def forward(self,x): x = F.relu(self.pool(self.conv1(x))) x = F.relu(self.pool(self.conv2(x))) x = F.relu(self.pool(self.conv3(x))) batch_size, _, _, _ = x.size() x = x.view(batch_size,-1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.out(x) def configure_optimizers(self): return optim.Adam(self.parameters()) def training_step(self, train_batch, batch_idx): x, y = train_batch logits = self.forward(x) loss = self.loss(logits,y) # Logging the loss self.log('train/loss', loss, on_epoch=True) return loss def validation_step(self, valid_batch, batch_idx): x, y = valid_batch logits = self.forward(x) loss = self.loss(logits,y) # Logging the loss self.log('valid/loss', loss, on_epoch=True) return loss As you can see in the above code we are using self.log() to log our loss values for which chart will be generated. Now let’s create our logger and fit our trainer.\nfrom pytorch_lightning.loggers import WandbLogger model = FashionMNISTModel() wandb_logger = WandbLogger(project='Fashion MNIST', log_model='all') trainer = pl.Trainer(max_epochs=10, tpu_cores = 1, logger = wandb_logger) wandb_logger.watch(model) trainer.fit(model, data) Once you run the above code the logs will be plotted in runtime. I am plotting loss values using the self.log() and logging gradients using watch().\nIf you get MisconfigurationException: No TPU devices were found. then run the following command to fix it.\n%%capture !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py  /dev/null ! pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev  /dev/null !pip install pytorch-lightning  /dev/null Since the model is extremely simple I didn’t use any callbacks but you can use if you like. In Learning Rate Monitor Callback you need to have a scheduler to make it work and pass the Model Checkpoint Callback in checkpoint_callback instead of callback.\nFrom Me to You… Wow, that seemed like an exciting ride. Honestly, I think Lightning has many cool stuff that a person can utilize the fact that we can use all these while keeping it as close to PyTorch proves how powerful it is. I hope now you are convinced that Lightning is a great tool to be learned and used.\n","wordCount":"2441","inLanguage":"en","datePublished":"2021-06-08T00:00:00Z","dateModified":"2021-06-08T00:00:00Z","author":{"@type":"Person","name":"Herumb Shandilya"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.herumbshandilya.com/posts/pytorch-lightning/"},"publisher":{"@type":"Organization","name":"Journal | Herumb Shandilya","logo":{"@type":"ImageObject","url":"https://www.herumbshandilya.com/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://www.herumbshandilya.com/ accesskey=h title="Herumb's Journal (Alt + H)">
<img src=https://www.herumbshandilya.com/apple-touch-icon.png alt aria-label=logo height=35>Herumb's Journal</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://www.herumbshandilya.com/posts/ title=Posts>
<span>Posts</span>
</a>
</li>
<li>
<a href=https://the-ir-book.herumbshandilya.com/ title="The Small Book of Information Retrieval">
<span>The Small Book of Information Retrieval</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg>
</a>
</li>
<li>
<a href=https://www.herumbshandilya.com/ title=Portfolio>
<span>Portfolio</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<h1 class="post-title entry-hint-parent">
PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers
</h1>
<div class=post-meta><span title="2021-06-08 00:00:00 +0000 UTC">June 8, 2021</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2441 words&nbsp;·&nbsp;Herumb Shandilya
</div>
</header>
<div class=post-content><blockquote>
<p><em>When I was a young man,</em></p>
<p><em>I had liberty but I didn’t see it,</em></p>
<p><em>I had time but I didn’t know it,</em></p>
<p><em>And I had PyTorch Lightning but I didn&rsquo;t use it.</em></p>
<p><strong>- Newbie PyTorch User</strong></p>
</blockquote>
<p>Another Blog another great video game quote butchered by these hands. Anyways, when I was getting started with PyTorch one of the things that made me jealous was the fact that Tensorflow has so much support for monitoring the model performance. I mean I have to write a training loop with redundant steps while Tensorflow beginners were just passing and chilling.</p>
<p>So I did what most PyTorch newbies did, learned and wrote the training loop code until it became muscle memory and that is something you shouldn&rsquo;t do. Few tips for that is just understanding why each step works and once you do that every line of code starts to make sense, trust me once I did that I never forgot it again.</p>
<p>Back to the topic, one thing that I love about PyTorch is the extent to which I can customize it and how easy it is to debug. But still, it would be better if somehow I was able to keep those features and reduce the redundancy.</p>
<p><img loading=lazy src=https://cdn.hashnode.com/res/hashnode/image/upload/v1623080889159/8CTEmcmR1.png alt="Untitled design.png">
</p>
<p>Well there you have it, the solution is <strong>PyTorch Lightning</strong> I mean anyone with such a cool name is already destined for greatness but if that reason doesn&rsquo;t convince you then I hope that by the end of this article you will be.</p>
<p><strong>P.S.</strong> it&rsquo;s actually better if you have some basic idea about Lightning. If you wanna learn its basics try this article I wrote a while back on <a href=https://www.geeksforgeeks.org/training-neural-networks-using-pytorch-lightning/><strong>Training Neural Networks using PyTorch Lighting</strong></a>.</p>
<h2 id=datasets-dataloaders-and-datamodule>Datasets, DataLoaders and DataModule<a hidden class=anchor aria-hidden=true href=#datasets-dataloaders-and-datamodule>#</a></h2>
<p>There are two things that give me immense satisfaction in life, first is watching a newbie Vim user trying to exit Vim, and the second is watching a newbie PyTorch user trying to create Dataloader for custom data. So let&rsquo;s take a look at how you can create DataLoaders in PyTorch using an awesome utility called <strong>Dataset</strong>.</p>
<h3 id=the-dataset-class>The Dataset Class<a hidden class=anchor aria-hidden=true href=#the-dataset-class>#</a></h3>
<p>One of the things that is essential if you are learning PyTorch is how to create DataLoaders there are many ways to go for it I mean for Image you have <strong>ImageFolder</strong> utility in <strong>torchvision</strong> and for Data we have <strong>BucketIterator</strong> which I won&rsquo;t lie are quite handy but still what if the data you load isn&rsquo;t in the desired format? In that case, you can use the Dataset class, and not just that the thing that I love about Dataset class is that they are customizable to an extent you can&rsquo;t imagine.</p>
<p>In Dataset class you have 3 main functions that you must define, let&rsquo;s take a look at them:-</p>
<ul>
<li>
<p><strong>__init__(self, )</strong>: The constructor function. there are infinite possibilities of things you can do here. Think of it as the <strong>staging area</strong> of our data. Usually, you pass either a path, a dataframe, or an array as the argument to this class&rsquo;s instance. Here you usually define class attributes like feature matrix and target vector. If you are working with an image you can assign transform variable, if you are working with the you can assign a tokenizer variable etc.</p>
</li>
<li>
<p><strong>__len__(self):</strong> The length function. this is where you&rsquo;ll return the length of the dataset, you basically return the length of the feature matrix or the dataset in whatever format you have.</p>
</li>
<li>
<p><strong>__getitem__(self, idx):</strong> The fetching function. This is the function where you define how your data will be returned. You can apply various preprocessing to be applied on the data of at <strong>idx</strong> index here and then return its tensor along with other variables like mask or target by packing them in a list, tuple, dict, etc. I used to pack them in a list but when I saw <a href="https://www.youtube.com/watch?v=oWq6aVv5mC8&ab_channel=AbhishekThakurAbhishekThakur"><strong>Abhishek Thakur&rsquo;s</strong></a> video on this I started using dicts and I never looked back since that day.</p>
</li>
</ul>
<p>The guy is a PyTorch madlad go watch his videos if you haven&rsquo;t already brilliant and to-the-point hands-on videos on many interesting topics. Let&rsquo;s keep all that aside for now and take an example, shall we? I&rsquo;ll be using <strong>make_classification</strong> to create the dataset to keep things simple.</p>
<pre tabindex=0><code>import torch
from sklearn.datasets import make_classification
from torch.utils.data import Dataset

class TabularData(Dataset):
    def __init__(self, X, Y, train = True):
        self.X = X
        self.Y = Y
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        features = self.X[idx]
        
        if self.Y is not None:
            target = self.Y[idx]
            return {
                'X' : torch.tensor(features, dtype = torch.float32),
                'Y' : torch.tensor(target, dtype = torch.long)
            }
        
        else:
            return {
                'X' : torch.tensor(features, dtype = torch.float32)
            }
        
X, Y = make_classification()
train_data = TabularData(X, Y)
train_data[0]
</code></pre><p>Output:-</p>
<pre tabindex=0><code>{'X': tensor([ 1.1018, -0.0042,  2.1382, -0.7926, -0.6016,  1.5499, -0.4010,  0.3327,
          0.1973, -1.3655,  0.4870,  0.7568, -0.7460, -0.8977,  0.1395,  0.0814,
         -1.4849, -0.2000,  1.2643,  0.4178]), 'Y': tensor(1)}
</code></pre><p>Subarashi! That seems to be working correctly you can try and experiment with what happens if you change the output of the classes a bit and check the results. But for this tutorial, we&rsquo;ll be working on Fashion MNIST data and thankfully in torchvision there already is a dataset for that so let&rsquo;s load that.</p>
<pre tabindex=0><code>from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor()
])

train = datasets.FashionMNIST('',train = True, download = True, transform=transform)
test = datasets.FashionMNIST('',train = False, download = True, transform=transform)
</code></pre><p>Now that we have the data it&rsquo;s time to move onto DataLoaders.</p>
<h3 id=dataloaders>DataLoaders<a hidden class=anchor aria-hidden=true href=#dataloaders>#</a></h3>
<p>DataLoaders are responsible to take input a dataset and then pack the data in them into batches and create an iterator to iterate over these batches. They really make the whole batching process easier while keeps the customizability to the fullest. I mean you can define how to batch your data by writing your own <strong>collate_fn</strong>, what more do you want?</p>
<p>We saw how we can create a dataset class, to create its DataLoader you just pass that Dataset instance to the DataLoader and you are done. Let&rsquo;s see how to do it for the MNIST dataset that we created above and check it&rsquo;s output.</p>
<pre tabindex=0><code>import matplotlib.pyplot as plt
from torch.utils.data import DataLoader

trainloader = DataLoader(train, batch_size= 32, shuffle=True)
testloader = DataLoader(test, batch_size= 32, shuffle=True)

#Plotting a Batch of DataLoader
images, labels = iter(trainloader).next()
plt.figure(figsize = (12,16))
for e,(img, lbl) in enumerate(zip(images, labels)):
    plt.subplot(8,4,e+1)
    plt.imshow(img[0])
    plt.title(f'Class: {lbl.item()}')

plt.subplots_adjust(hspace=0.6)
</code></pre><p>Output:-</p>
<p><img loading=lazy src=https://i.ibb.co/n8S0NxR/mnist-2.png alt=fashion-mnist.png>
</p>
<p>Damn, that looks beautiful and correct, well there you go seems like our DataLoaders work as expected. But is it just me or does this seem too messy? I mean it&rsquo;s great we batched the data but the variables seem to be everywhere, it doesn&rsquo;t seem much organized. Well, that&rsquo;s basically where DataModules come in handy 😎.</p>
<h3 id=datamodules>DataModules<a hidden class=anchor aria-hidden=true href=#datamodules>#</a></h3>
<p>DataModule is a reusable and shareable class that encapsulates the DataLoaders along with the steps required to process data. Creating DataLoaders can get messy that’s why it’s better to club the dataset in the form of DataModule. DataModule has few methods that must define the format of DataModule is as follows:-</p>
<pre tabindex=0><code>import pytorch-lightning as pl

class DataModuleClass(pl.LightningDataModule):
    def __init__(self):
        # Define class attributs here
    
    def prepare_data(self):
        # Define steps that should be done
        # on only one GPU, like getting data.
    
    def setup(self, stage=None):
        # Define steps that should be done on 
        # every GPU, like splitting data, applying
        # transform etc.
    
    def train_dataloader(self):
        # Stage DataLoader for Training Data
    
    def val_dataloader(self):
        # Stage DataLoader for Validation Data
    
    def test_dataloader(self):
        # Stage DataLoader for Testing Data
</code></pre><p>That seems great so let&rsquo;s go ahead and create DataModule for our Fashion MNIST Data.</p>
<pre tabindex=0><code>import pytorch_lightning as pl 

class DataModuleFashionMNIST(pl.LightningDataModule):
    def __init__(self):
        super().__init__()
        
        self.dir = ''
        self.batch_size = 32
        self.transform = transforms.Compose([
            transforms.ToTensor()
        ])
  
    def prepare_data(self):
        datasets.FashionMNIST(self.dir, train = True, download = True)
        datasets.FashionMNIST(self.dir, train = False, download = True)
  
    def setup(self, stage=None):
        data = datasets.FashionMNIST(self.dir,
                                     train = True, 
                                     transform = self.transform)
          
        self.train, self.valid = random_split(data, [52000, 8000])
  
        self.test = datasets.FashionMNIST(self.download_dir,
                                               train = False,
                                               transform = self.transform)
  
    def train_dataloader(self):
        return DataLoader(self.train, batch_size = self.batch_size)
  
    def val_dataloader(self):
        return DataLoader(self.valid, batch_size = self.batch_size)
  
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size = self.batch_size)

data = DataModuleFashionMNIST()
</code></pre><p>Perfect that&rsquo;s basically it but if you want an in-depth explanation on them you can refer <a href=https://www.geeksforgeeks.org/understanding-pytorch-lightning-datamodules/><strong>this article</strong></a> I wrote explaining about DataModules.</p>
<h2 id=lightning-callbacks--hooks>Lightning Callbacks & Hooks<a hidden class=anchor aria-hidden=true href=#lightning-callbacks--hooks>#</a></h2>
<p>Callbacks are basically programs that contain code that&rsquo;s run when it is required. When and what a callback should do is defined by using Callback <strong>Hooks</strong>, a few of them are on_epoch_end, on_validation_epoch_end, etc. You can maybe define a logic to monitor a metric, save a model, or various other cool stuff. If I have to define callbacks as a meme it&rsquo;ll be the following.</p>
<p><img loading=lazy src=https://i.ibb.co/Bst2x0r/gRqRS4U.png alt=gRqRS4U.png>
</p>
<p>Well necessary for training but useful for other stuff. There are many inbuilt callbacks that can be used for various important tasks. A few of them are:-</p>
<table>
<thead>
<tr>
<th>CallBack</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>EarlyStopping</strong></td>
<td><em>Monitor a metric and stop training when it stops improving.</em></td>
</tr>
<tr>
<td><strong>LearningRateMonitor</strong></td>
<td><em>Automatically monitors and logs learning rate for learning rate schedulers during training.</em></td>
</tr>
<tr>
<td><strong>ModelCheckpoint</strong></td>
<td><em>Save the model periodically by monitoring a quantity.</em></td>
</tr>
<tr>
<td><strong>Callback</strong></td>
<td><em>Base Class to define custom callbacks.</em></td>
</tr>
<tr>
<td><strong>LambdaCallback</strong></td>
<td><em>Basically Lambda Function of Callbacks.</em></td>
</tr>
</tbody>
</table>
<p>We&rsquo;ll be using the first two for this article but you can refer to docs to learn more about them. Lightning&rsquo;s video tutorials on their website are pretty good.</p>
<h2 id=tpu---hulkified-gpu>TPU - Hulkified GPU?<a hidden class=anchor aria-hidden=true href=#tpu---hulkified-gpu>#</a></h2>
<p>TPUs are accelerators used to speed up Machine Learning Tasks. The catch is that they are platform dependant i.e. TensorFlow. TPUs are optimized for Tensorflow mainly which I think is quite selfish given <strong>PyTorch</strong> is so awesome.</p>
<p><img loading=lazy src=https://i.ibb.co/TtgJcwk/5cifez.jpg alt=5cifez.jpg>
</p>
<p>But we can actually use them in PyTorch by making and passing a <strong>TPU Sampler</strong> in the DataLoader. It&rsquo;s one hell of a messy task you have to replace the device type with <strong>xm.xla_device()</strong> and add 2 extra steps for optimizer and not just that you&rsquo;ll have to install <strong>PyTorch/XLA</strong> to do all that. It goes something like this:-</p>
<pre tabindex=0><code>import torch_xla.core.xla_model as xm

dev = xm.xla_device()

# TPU sampler
data_sampler = torch.utils.data.distributed.DistributedSampler(
    dataset,
    num_replicas=xm.xrt_world_size(),
    rank=xm.get_ordinal())

dataloader = DataLoader(dataset, batch_size=32, sampler = data_sampler)

# Training loop
for batch in dataloader:
    ...
    xm.optimizer_step(optimizer)
    xm.mark_step()
    ...
</code></pre><p>Man the above code is a mess but in Lightning all that is reduced to a simple single line. All you need to do is pass the number of <strong>tpu_cores</strong> to be you and you are done for the day. Really it&rsquo;s that simple.</p>
<pre tabindex=0><code>trainer = pl.Trainer(tpu_cores = 1)
trainer.fit(model)
</code></pre><p>I mean it literally couldn&rsquo;t get any simpler than this but still, there is one more thing that&rsquo;s left to talk about and that is <strong>Loggers</strong>. Let&rsquo;s take a look at that.</p>
<h2 id=loggers>Loggers<a hidden class=anchor aria-hidden=true href=#loggers>#</a></h2>
<p>Loggers are a kind of utility that you can use to monitor metrics and hyperparameters and much more cool stuff. In fact you probably already have tried one, <strong>Tensorboard</strong>. Ever heard of it? If you are here chances are you might have. Along with Tensorboard, PyTorch Lightning supports various 3rd party loggers from Weights and Biases, Comet.ml, MlFlow, etc.</p>
<p>In fact, in Lightning, you can use multiple loggers together. To use a logger you can create its instance and pass it in Trainer Class under <strong>logger</strong> parameter individually or as a list of loggers.</p>
<pre tabindex=0><code>from pytorch_lightning.loggers import WandbLogger

# Single Logger
wandb_logger = WandbLogger(project='Fashion MNIST', log_model='all')
trainer = Trainer(logger=wandb_logger)

# Multiple Loggers
from pytorch_lightning.loggers import TensorBoardLogger
tb_logger = TensorBoardLogger('tb_logs', name='my_model')
trainer = Trainer(logger=[wandb_logger, tb_logger])
</code></pre><p>The point is how to log the values? We&rsquo;ll take a look at how you can do that along with applying all the stuff I talked about above.</p>
<h2 id=putting-it-all-together>Putting it All Together<a hidden class=anchor aria-hidden=true href=#putting-it-all-together>#</a></h2>
<p>The Ingredient to a model is the data that you feed it, we talked about DataModules in this article so let&rsquo;s start by creating a <strong>DataModule</strong> for out Fashion MNIST data.</p>
<pre tabindex=0><code>import pytorch_lightning as pl 
from torchvision import datasets, transforms

class DataModuleFashionMNIST(pl.LightningDataModule):
    def __init__(self, batch_size = 32):
        super().__init__()
        
        self.dir = ''
        self.batch_size = batch_size
        self.transform = transforms.Compose([
            transforms.ToTensor()
        ])
  
    def prepare_data(self):
        datasets.FashionMNIST(self.dir, train = True, download = True)
        datasets.FashionMNIST(self.dir, train = False, download = True)
  
    def setup(self, stage=None):
        data = datasets.FashionMNIST(self.dir,
                                     train = True, 
                                     transform = self.transform)
          
        self.train, self.valid = random_split(data, [52000, 8000])
  
        self.test = datasets.FashionMNIST(self.download_dir,
                                          train = False,
                                          transform = self.transform)
  
    def train_dataloader(self):
        return DataLoader(self.train, batch_size = self.batch_size)
  
    def val_dataloader(self):
        return DataLoader(self.valid, batch_size = self.batch_size)
  
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size = self.batch_size)

data = DataModuleFashionMNIST()
</code></pre><p>Now let&rsquo;s create our model class and setup logs for the <strong>WandbLogger</strong> and then create a model instance for the same.</p>
<pre tabindex=0><code>from torch import nn, optim
import torch.nn.functional as F

class FashionMNISTModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        # 28 * 28 * 3
        self.conv1 = nn.Conv2d(1,16, stride = 1, padding = 1, kernel_size = 3)
        # 14 * 14 * 16
        self.conv2 = nn.Conv2d(16,32, stride = 1, padding = 1, kernel_size = 3)
        # 7 * 7 * 32
        self.conv3 = nn.Conv2d(32,64, stride = 1, padding = 1, kernel_size = 3)
        # 3 * 3 * 64

        self.fc1 = nn.Linear(3*3*64,128)
        self.fc2 = nn.Linear(128,64)
        self.out = nn.Linear(64,10)

        self.pool = nn.MaxPool2d(2,2)
        self.loss = nn.CrossEntropyLoss()
    
    def forward(self,x):
        x = F.relu(self.pool(self.conv1(x)))
        x = F.relu(self.pool(self.conv2(x)))
        x = F.relu(self.pool(self.conv3(x)))

        batch_size, _, _, _ = x.size()
        
        x = x.view(batch_size,-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.out(x)
    
    def configure_optimizers(self):
        return optim.Adam(self.parameters())
    
    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        logits = self.forward(x)
        loss = self.loss(logits,y)
        
        # Logging the loss
        self.log('train/loss', loss, on_epoch=True)
        return loss
    
    def validation_step(self, valid_batch, batch_idx):
        x, y = valid_batch
        logits = self.forward(x)
        loss = self.loss(logits,y)
        
        # Logging the loss
        self.log('valid/loss', loss, on_epoch=True)
        return loss
</code></pre><p>As you can see in the above code we are using self.log() to log our loss values for which chart will be generated. Now let&rsquo;s create our logger and fit our trainer.</p>
<pre tabindex=0><code>from pytorch_lightning.loggers import WandbLogger

model = FashionMNISTModel()
wandb_logger = WandbLogger(project='Fashion MNIST', log_model='all')

trainer = pl.Trainer(max_epochs=10, tpu_cores = 1, logger = wandb_logger)
wandb_logger.watch(model)

trainer.fit(model, data)
</code></pre><p><img loading=lazy src=https://i.ibb.co/0jRwfD8/Screenshot-from-2021-06-08-20-58-37.png alt="Screenshot from 2021-06-08 20-58-37.png">
</p>
<p>Once you run the above code the logs will be plotted in runtime. I am plotting loss values using the <strong>self.log()</strong> and logging gradients using <strong>watch()</strong>.</p>
<p>If you get <code>MisconfigurationException: No TPU devices were found.</code> then run the following command to fix it.</p>
<pre tabindex=0><code>%%capture
!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py &gt; /dev/null
! pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev &gt; /dev/null
!pip install pytorch-lightning &gt; /dev/null
</code></pre><p>Since the model is extremely simple I didn&rsquo;t use any callbacks but you can use if you like. In Learning Rate Monitor Callback you need to have a scheduler to make it work and pass the Model Checkpoint Callback in <strong>checkpoint_callback</strong> instead of <strong>callback</strong>.</p>
<h2 id=from-me-to-you>From Me to You&mldr;<a hidden class=anchor aria-hidden=true href=#from-me-to-you>#</a></h2>
<p>Wow, that seemed like an exciting ride. Honestly, I think Lightning has many cool stuff that a person can utilize the fact that we can use all these while keeping it as close to PyTorch proves how powerful it is. I hope now you are convinced that Lightning is a great tool to be learned and used.</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
</ul>
<nav class=paginav>
<a class=prev href=https://www.herumbshandilya.com/posts/transformers/>
<span class=title>« Prev</span>
<br>
<span>Transformers: Attention is all you need</span>
</a>
<a class=next href=https://www.herumbshandilya.com/posts/class-imbalance/>
<span class=title>Next »</span>
<br>
<span>Class Imbalance comes in Like a Lion</span>
</a>
</nav>
<ul class=share-buttons>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on x" href="https://x.com/intent/tweet/?text=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers&url=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f&title=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers&summary=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers&source=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f&title=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on whatsapp" href="https://api.whatsapp.com/send?text=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers%20-%20https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on telegram" href="https://telegram.me/share/url?text=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers&url=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</li>
<li>
<a target=_blank rel="noopener noreferrer" aria-label="share PyTorch Lightning: DataModules, Callbacks, TPU, and Loggers on ycombinator" href="https://news.ycombinator.com/submitlink?t=PyTorch%20Lightning%3a%20DataModules%2c%20Callbacks%2c%20TPU%2c%20and%20Loggers&u=https%3a%2f%2fwww.herumbshandilya.com%2fposts%2fpytorch-lightning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2024 <a href=https://www.herumbshandilya.com/>Journal | Herumb Shandilya</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>